{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description and code can be found at:\n",
    "https://github.com/jannaescur/language_identification_slpdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['labels.csv', 'x_train.txt', 'y_train.txt', 'x_test.txt']\n",
      "Example:\n",
      "LANG = est\n",
      "TEXT = Klement Gottwaldi surnukeha palsameeriti ning paigutati mausoleumi. Surnukeha oli aga liiga hilja ja oskamatult palsameeritud ning hakkas ilmutama lagunemise tundemärke. 1962. aastal viidi ta surnukeha mausoleumist ära ja kremeeriti. Zlíni linn kandis aastatel 1949–1989 nime Gottwaldov. Ukrainas Harkivi oblastis kandis Zmiivi linn aastatel 1976–1990 nime Gotvald.\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "import random\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch # Deep learning framework\n",
    "import torch.nn.functional as F\n",
    "from nltk import ngrams\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "#Init random seed to get reproducible results\n",
    "seed = 1111\n",
    "random.seed(seed)\n",
    "np.random.RandomState(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "x_train_full = open(\"../input/x_train.txt\").read().splitlines()\n",
    "y_train_full = open(\"../input/y_train.txt\").read().splitlines()\n",
    "print('Example:')\n",
    "print('LANG =', y_train_full[0])\n",
    "print('TEXT =', x_train_full[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.token2idx = {}\n",
    "        self.idx2token = []\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token not in self.token2idx:\n",
    "            self.idx2token.append(token)\n",
    "            self.token2idx[token] = len(self.idx2token) - 1\n",
    "        return self.token2idx[token]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Dictionary** class is used to map tokens (characters, words, subwords) into consecutive integer indexes.  \n",
    "The index **0** is reserved for padding sequences up to a fixed lenght, and the index **1** for any 'unknown' character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len trigrams:  43352207\n",
      "len unique trigrams:  1263034\n",
      "unique counts max:  111233\n",
      "percentage:  0.0025657978612253812\n",
      "len my trigrams:  821920\n",
      "Vocabulary: 143262 different trigrams\n",
      "Labels: 235 languages\n"
     ]
    }
   ],
   "source": [
    "char_vocab = Dictionary()\n",
    "pad_token = '<pad>' # reserve index 0 for padding\n",
    "unk_token = '<unk>' # reserve index 1 for unknown token\n",
    "pad_index = char_vocab.add_token(pad_token)\n",
    "unk_index = char_vocab.add_token(unk_token)\n",
    "\n",
    "# join all the training sentences in a single string\n",
    "# and obtain the list of different characters with set\n",
    "\n",
    "data_str = ''.join(x_train_full)\n",
    "\n",
    "# Make ngrams!!!!\n",
    "n= 3\n",
    "trigrams = [data_str[i:i+n]  for i in range(len(data_str) - n)]\n",
    "\n",
    "\n",
    "unique_trigrams, unique_counts = np.unique(trigrams, return_counts=True)\n",
    "print('len trigrams: ', len(trigrams))\n",
    "print('len unique trigrams: ', len(unique_trigrams))\n",
    "print('unique counts max: ', np.max(unique_counts))\n",
    "print('percentage: ', np.max(unique_counts)/len(trigrams))\n",
    "\n",
    "my_trigrams = []\n",
    "for i, unique_count in enumerate(unique_counts):\n",
    "    if unique_count/len(trigrams)*100 < 5e-6:\n",
    "        my_trigrams.append(trigrams[i])\n",
    "        \n",
    "print('len my trigrams: ', len(my_trigrams))\n",
    "vocab = set(my_trigrams)\n",
    "\n",
    "for char in sorted(vocab):\n",
    "    char_vocab.add_token(char)\n",
    "print(\"Vocabulary:\", len(char_vocab), \"different trigrams\")\n",
    "\n",
    "\n",
    "lang_vocab = Dictionary()\n",
    "# use python set to obtain the list of languages without repetitions\n",
    "languages = set(y_train_full)\n",
    "for lang in sorted(languages):\n",
    "    lang_vocab.add_token(lang)\n",
    "print(\"Labels:\", len(lang_vocab), \"languages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a -> 27768\n",
      "cat -> 28\n",
      "est Klement Go\n",
      "117500\n",
      " #################################################################################################### [ 99.57%]xtrain idx finished!\n",
      "52 [23371 37263 31186 38384 31247 39894 45085  1043 22509 41324]\n",
      "xtrain idx len:  117500\n",
      "ytrain idc len:  117500\n"
     ]
    }
   ],
   "source": [
    "#From token or label to index\n",
    "print('a ->', char_vocab.token2idx['ava'])\n",
    "print('cat ->', lang_vocab.token2idx['cat'])\n",
    "print(y_train_full[0], x_train_full[0][:10])\n",
    "#Update trigrams\n",
    "\n",
    "def progbar(curr, total, full_progbar):\n",
    "    frac = curr/total\n",
    "    filled_progbar = round(frac*full_progbar)\n",
    "    print('\\r', '#'*filled_progbar + '-'*(full_progbar-filled_progbar), '[{:>7.2%}]'.format(frac), end='')\n",
    "\n",
    "\n",
    "print(len(x_train_full))\n",
    "\n",
    "x_train_line = []\n",
    "x_train_idx = []\n",
    "\n",
    "m = 0\n",
    "for line in x_train_full:\n",
    "    m=m+1\n",
    "    if (m%1000 == 0 ):\n",
    "        progbar(m, len(x_train_full), 100)\n",
    "\n",
    "    for i in range(len(line)-n):\n",
    "        try:\n",
    "            token = char_vocab.token2idx[line[i:i+n]]\n",
    "        except:\n",
    "            token = char_vocab.token2idx['<unk>']\n",
    "        x_train_line.append(token)\n",
    "    x_train_idx.append(np.array(x_train_line))\n",
    "    x_train_line = []\n",
    "\n",
    "#x_train_idx = np.array(x_train_idx)\n",
    "\n",
    "print('xtrain idx finished!')\n",
    "\n",
    "#x_train_idx= [np.array([char_vocab.token2idx[line[i:i+n]] if line[i:i+n] in my_trigrams else unk_index for i in range(len(line)-n)]) for line in x_train_full]\n",
    "\n",
    "#x_test_idx = [np.array([char_vocab.token2idx[c] if c in char_vocab.token2idx else unk_index for c in line]) for line in x_test_txt]\n",
    "#x_train_idx = [np.array([char_vocab.token2idx[line[i:i+n]] for i in range(len(line) - n)]) for line in x_train_full]\n",
    "#x_train_idx = [np.array([char_vocab.token2idx[c] for c in line]) for line in x_train_full]\n",
    "#x_train_idx = [np.array(np.where(([line[i:i+n] in my_trigrams for i in range(len(line)-n) for line in x_train_full]), [char_vocab.token2idx[line[i:i+n]], char_vocab.token2idx['<unk>']]))] \n",
    "y_train_idx = np.array([lang_vocab.token2idx[lang] for lang in y_train_full])\n",
    "print(y_train_idx[0], x_train_idx[0][:10])\n",
    "print('xtrain idx len: ', len(x_train_idx))\n",
    "print('ytrain idc len: ', len(y_train_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Radomly select 15% of the database for validation  \n",
    "Create lists of (input, target) tuples for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  (117500,)\n",
      "type:  <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "x_train_idx = np.array(x_train_idx)\n",
    "\n",
    "print('shape: ', x_train_idx.shape)\n",
    "print('type: ', type(x_train_idx[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99875 training samples\n",
      "17625 validation samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_idx, y_train_idx, test_size=0.15, random_state=seed)\n",
    "train_data = [(x, y) for x, y in zip(x_train, y_train)]\n",
    "val_data = [(x, y) for x, y in zip(x_val, y_val)]\n",
    "print(len(train_data), \"training samples\")\n",
    "print(len(val_data), \"validation samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(data, batch_size, token_size):\n",
    "    \"\"\"Yield elements from data in chunks with a maximum of batch_size sequences and token_size tokens.\"\"\"\n",
    "    minibatch, sequences_so_far, tokens_so_far = [], 0, 0\n",
    "    for ex in data:\n",
    "        minibatch.append(ex)\n",
    "        seq_len = len(ex[0])\n",
    "        if seq_len > token_size:\n",
    "            ex = (ex[0][:token_size], ex[1])\n",
    "            seq_len = token_size\n",
    "        sequences_so_far += 1\n",
    "        tokens_so_far += seq_len\n",
    "        if sequences_so_far == batch_size or tokens_so_far == token_size:\n",
    "            yield minibatch\n",
    "            minibatch, sequences_so_far, tokens_so_far = [], 0, 0\n",
    "        elif sequences_so_far > batch_size or tokens_so_far > token_size:\n",
    "            yield minibatch[:-1]\n",
    "            minibatch, sequences_so_far, tokens_so_far = minibatch[-1:], 1, len(minibatch[-1][0])\n",
    "    if minibatch:\n",
    "        yield minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_generator(data, batch_size, token_size, shuffle=False):\n",
    "    \"\"\"Sort within buckets, then batch, then shuffle batches.\n",
    "    Partitions data into chunks of size 100*token_size, sorts examples within\n",
    "    each chunk, then batch these examples and shuffle the batches.\n",
    "    \"\"\"\n",
    "    for p in batch_generator(data, batch_size * 100, token_size * 100):\n",
    "        p_batch = batch_generator(sorted(p, key=lambda t: len(t[0]), reverse=True), batch_size, token_size)\n",
    "        p_list = list(p_batch)\n",
    "        if shuffle:\n",
    "            for b in random.sample(p_list, len(p_list)):\n",
    "                yield b\n",
    "        else:\n",
    "            for b in p_list:\n",
    "                yield b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DNN Model**  \n",
    "Includes Python comments with the dimension of the input  matrix:  \n",
    "T = Max number of tokens in a sequence  \n",
    "B = Number of sequences (batch size)  \n",
    "E = Embedding size\n",
    "H = Hidden size  \n",
    "O = Output size (number of languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNNClassifier(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, model=\"lstm\", num_layers=1, bidirectional=False, pad_idx=0):\n",
    "        super().__init__()\n",
    "        self.model = model.lower()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed = torch.nn.Embedding(input_size, embedding_size, padding_idx=pad_idx)\n",
    "        if self.model == \"gru\":\n",
    "            self.rnn = torch.nn.GRU(embedding_size, hidden_size, num_layers, bidirectional=bidirectional)\n",
    "        elif self.model == \"lstm\":\n",
    "            self.rnn = torch.nn.LSTM(embedding_size, hidden_size, num_layers, bidirectional=bidirectional)\n",
    "        #self.h2o = torch.nn.Linear(hidden_size, output_size)\n",
    "        self.h2o = torch.nn.Sequential(torch.nn.Linear(hidden_size, 512),\n",
    "                                      torch.nn.Dropout(p=0.8),\n",
    "                                      torch.nn.Linear(512, output_size))\n",
    "        \n",
    "    def forward(self, input, input_lengths):\n",
    "        # T x B\n",
    "        encoded = self.embed(input)\n",
    "        # T x B x E\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(encoded, input_lengths)\n",
    "        # Packed T x B x E\n",
    "        output, _ = self.rnn(packed)\n",
    "        # Packed T x B x H\n",
    "        padded, _ = torch.nn.utils.rnn.pad_packed_sequence(output, padding_value=float('-inf'))\n",
    "        # T x B x H\n",
    "        padded = padded.permute(1,2,0)\n",
    "        # B x H x T\n",
    "        output = F.adaptive_max_pool1d(padded, 1).view(-1, self.hidden_size)\n",
    "        # B x H\n",
    "        output = self.h2o(output)\n",
    "        # B x O\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    print(\"WARNING: CUDA is not available. Select 'GPU On' on kernel settings\")\n",
    "device = torch.device(\"cuda\")\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **nn.CrossEntropyLoss()** criterion combines **nn.LogSoftmax()** and **nn.NLLLoss()** in one single class.  \n",
    "It is useful when training a classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, data, batch_size, token_size, log=False, L2_norm = True):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    ncorrect = 0\n",
    "    nsentences = 0\n",
    "    ntokens = 0\n",
    "    niterations = 0\n",
    "    for batch in pool_generator(data, batch_size, token_size, shuffle=True):\n",
    "        # Get input and target sequences from batch\n",
    "        X = [torch.from_numpy(d[0]) for d in batch]\n",
    "        X_lengths = [x.numel() for x in X]\n",
    "        ntokens += sum(X_lengths)\n",
    "        X_lengths = torch.tensor(X_lengths, dtype=torch.long, device=device)\n",
    "        y = torch.tensor([d[1] for d in batch], dtype=torch.long, device=device)\n",
    "        # Pad the input sequences to create a matrix\n",
    "        X = torch.nn.utils.rnn.pad_sequence(X).to(device)\n",
    "        model.zero_grad()\n",
    "        output = model(X, X_lengths)\n",
    "        loss = criterion(output, y)\n",
    "        if L2_norm:\n",
    "            all_linear1_params = torch.cat([x.view(-1) for x in model.h2o.parameters()])\n",
    "            l2_regularization = 0.01 * torch.norm(all_linear1_params, 2)\n",
    "            loss+=l2_regularization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Training statistics\n",
    "        total_loss += loss.item()\n",
    "        ncorrect += (torch.max(output, 1)[1] == y).sum().item()\n",
    "        nsentences += y.numel()\n",
    "        niterations += 1\n",
    "    \n",
    "    total_loss = total_loss / nsentences\n",
    "    accuracy = 100 * ncorrect / nsentences\n",
    "    if log:\n",
    "        print(f'Train: wpb={ntokens//niterations}, bsz={nsentences//niterations}, num_updates={niterations}')\n",
    "    return accuracy, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, data, batch_size, token_size):\n",
    "    model.eval()\n",
    "    # calculate accuracy on validation set\n",
    "    ncorrect = 0\n",
    "    nsentences = 0\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in pool_generator(data, batch_size, token_size):\n",
    "            # Get input and target sequences from batch\n",
    "            X = [torch.from_numpy(d[0]) for d in batch]\n",
    "            X_lengths = torch.tensor([x.numel() for x in X], dtype=torch.long, device=device)\n",
    "            y = torch.tensor([d[1] for d in batch], dtype=torch.long, device=device)\n",
    "            # Pad the input sequences to create a matrix\n",
    "            X = torch.nn.utils.rnn.pad_sequence(X).to(device)\n",
    "            answer = model(X, X_lengths)\n",
    "            \n",
    "            loss = criterion(answer, y)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            ncorrect += (torch.max(answer, 1)[1] == y).sum().item()\n",
    "            nsentences += y.numel()\n",
    "        dev_acc = 100 * ncorrect / nsentences\n",
    "        total_loss = total_loss / nsentences\n",
    "    return dev_acc, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 64\n",
    "embedding_size = 128\n",
    "bidirectional = False\n",
    "ntokens = len(char_vocab)\n",
    "nlabels = len(lang_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharRNNClassifier(ntokens, embedding_size, hidden_size, nlabels, bidirectional=bidirectional, pad_idx=pad_index).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training cross-validation model for 15 epochs\n",
      "Train: wpb=19565, bsz=53, num_updates=1868\n",
      "| epoch 001 | train accuracy=39.0%\n",
      "| epoch 001 | valid accuracy=80.7%\n",
      "| epoch 002 | train accuracy=79.0%\n",
      "| epoch 002 | valid accuracy=87.2%\n",
      "| epoch 003 | train accuracy=85.5%\n",
      "| epoch 003 | valid accuracy=89.3%\n",
      "| epoch 004 | train accuracy=88.1%\n",
      "| epoch 004 | valid accuracy=90.2%\n",
      "| epoch 005 | train accuracy=89.9%\n",
      "| epoch 005 | valid accuracy=90.7%\n",
      "| epoch 006 | train accuracy=91.3%\n",
      "| epoch 006 | valid accuracy=90.9%\n",
      "| epoch 007 | train accuracy=92.1%\n",
      "| epoch 007 | valid accuracy=91.4%\n",
      "| epoch 008 | train accuracy=93.1%\n",
      "| epoch 008 | valid accuracy=91.3%\n",
      "| epoch 009 | train accuracy=93.5%\n",
      "| epoch 009 | valid accuracy=91.5%\n",
      "| epoch 010 | train accuracy=95.3%\n",
      "| epoch 010 | valid accuracy=91.9%\n",
      "| epoch 011 | train accuracy=96.1%\n",
      "| epoch 011 | valid accuracy=91.9%\n",
      "| epoch 012 | train accuracy=96.7%\n",
      "| epoch 012 | valid accuracy=91.9%\n",
      "| epoch 013 | train accuracy=97.0%\n",
      "| epoch 013 | valid accuracy=91.9%\n",
      "| epoch 014 | train accuracy=97.3%\n",
      "| epoch 014 | valid accuracy=92.1%\n",
      "| epoch 015 | train accuracy=97.5%\n",
      "| epoch 015 | valid accuracy=92.0%\n"
     ]
    }
   ],
   "source": [
    "batch_size, token_size = 128, 20000\n",
    "epochs = 15\n",
    "train_accuracy = []\n",
    "valid_accuracy = []\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "scheduler_lr = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 8, gamma = 0.5)\n",
    "print(f'Training cross-validation model for {epochs} epochs')\n",
    "for epoch in range(1, epochs + 1):\n",
    "    acc, loss = train(model, optimizer, train_data, batch_size, token_size, log=epoch==1)\n",
    "    train_accuracy.append(acc)\n",
    "    train_loss.append(loss)\n",
    "    print(f'| epoch {epoch:03d} | train accuracy={acc:.1f}%')\n",
    "    acc, loss = validate(model, val_data, batch_size, token_size)\n",
    "    valid_accuracy.append(acc)\n",
    "    valid_loss.append(loss)\n",
    "    print(f'| epoch {epoch:03d} | valid accuracy={acc:.1f}%')\n",
    "    scheduler_lr.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(1, len(train_accuracy)+1), train_accuracy)\n",
    "plt.plot(range(1, len(valid_accuracy)+1), valid_accuracy)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final model**  \n",
    "Finally, we create a model using all the training data and we generate the submission with the predicted test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharRNNClassifier(ntokens, embedding_size, hidden_size, nlabels, bidirectional=bidirectional).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training final model for 15 epochs\n",
      "Train: wpb=19563, bsz=53, num_updates=2198\n",
      "| epoch 001 | train accuracy=43.169\n",
      "| epoch 002 | train accuracy=81.378\n",
      "| epoch 003 | train accuracy=86.499\n",
      "| epoch 004 | train accuracy=88.953\n",
      "| epoch 005 | train accuracy=90.569\n",
      "| epoch 006 | train accuracy=91.729\n",
      "| epoch 007 | train accuracy=92.602\n",
      "| epoch 008 | train accuracy=93.391\n",
      "| epoch 009 | train accuracy=93.992\n",
      "| epoch 010 | train accuracy=94.490\n",
      "| epoch 011 | train accuracy=94.994\n",
      "| epoch 012 | train accuracy=95.425\n",
      "| epoch 013 | train accuracy=95.679\n",
      "| epoch 014 | train accuracy=96.010\n",
      "| epoch 015 | train accuracy=96.343\n"
     ]
    }
   ],
   "source": [
    "print(f'Training final model for {epochs} epochs')\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(f'| epoch {epoch:03d} | train accuracy={train(model, optimizer, train_data + val_data, batch_size, token_size, log=epoch==1)[0]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, data, batch_size, token_size):\n",
    "    model.eval()\n",
    "    sindex = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in pool_generator(data, batch_size, token_size):\n",
    "            # Get input sequences from batch\n",
    "            X = [torch.from_numpy(d[0]) for d in batch]\n",
    "            X_lengths = torch.tensor([x.numel() for x in X], dtype=torch.long, device=device)\n",
    "            # Pad the input sequences to create a matrix\n",
    "            X = torch.nn.utils.rnn.pad_sequence(X).to(device)\n",
    "            answer = model(X, X_lengths)\n",
    "            label = torch.max(answer, 1)[1].cpu().numpy()\n",
    "            # Save labels and sentences index\n",
    "            labels.append(label)\n",
    "            sindex += [d[1] for d in batch]\n",
    "    return np.array(sindex), np.concatenate(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the test database we replace the label (language) with a sentence index.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_txt = open(\"../input/x_test.txt\").read().splitlines()\n",
    "n = 3\n",
    "x_test_idx = [np.array([char_vocab.token2idx[line[i:i+n]] if line[i:i+n] in char_vocab.token2idx else unk_index for i in range(len(line)-n)]) for line in x_test_txt]\n",
    "#x_test_idx = [np.array([char_vocab.token2idx[c] if c in char_vocab.token2idx else unk_index for c in line]) for line in x_test_txt]\n",
    "test_data = [(x, idx) for idx, x in enumerate(x_test_idx)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentence index is used to rearrange the labels in the original sentence order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "index, labels = test(model, test_data, batch_size, token_size)\n",
    "order = np.argsort(index)\n",
    "labels = labels[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,mwl\n",
      "1,nld\n",
      "2,ava\n",
      "3,tcy\n",
      "4,bjn\n",
      "5,mon\n",
      "6,glk\n",
      "7,lez\n",
      "8,bul\n",
      "9,nan\n"
     ]
    }
   ],
   "source": [
    "with open('submission.csv', 'w') as f:\n",
    "    print('Id,Language', file=f)\n",
    "    for sentence_id, lang_id in enumerate(labels):\n",
    "        language = lang_vocab.idx2token[lang_id]\n",
    "        if sentence_id < 10:\n",
    "            print(f'{sentence_id},{language}')\n",
    "        print(f'{sentence_id},{language}', file=f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
