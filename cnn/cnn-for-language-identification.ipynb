{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description and code can be found at:\n",
    "https://github.com/jannaescur/language_identification_slpdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['labels.csv', 'x_train.txt', 'y_train.txt', 'x_test.txt']\n",
      "Example:\n",
      "LANG = est\n",
      "TEXT = Klement Gottwaldi surnukeha palsameeriti ning paigutati mausoleumi. Surnukeha oli aga liiga hilja ja oskamatult palsameeritud ning hakkas ilmutama lagunemise tundemärke. 1962. aastal viidi ta surnukeha mausoleumist ära ja kremeeriti. Zlíni linn kandis aastatel 1949–1989 nime Gottwaldov. Ukrainas Harkivi oblastis kandis Zmiivi linn aastatel 1976–1990 nime Gotvald.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "import random\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch # Deep learning framework\n",
    "import torch.nn.functional as F\n",
    "from nltk import ngrams\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "#Init random seed to get reproducible results\n",
    "seed = 1111\n",
    "random.seed(seed)\n",
    "np.random.RandomState(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "x_train_full = open(\"../input/x_train.txt\").read().splitlines()\n",
    "y_train_full = open(\"../input/y_train.txt\").read().splitlines()\n",
    "print('Example:')\n",
    "print('LANG =', y_train_full[0])\n",
    "print('TEXT =', x_train_full[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max words: 1585\n",
      "Min words:1\n",
      "Labels: 235 languages\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import itertools\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.token2idx = {}\n",
    "        self.idx2token = []\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token not in self.token2idx:\n",
    "            self.idx2token.append(token)\n",
    "            self.token2idx[token] = len(self.idx2token) - 1\n",
    "        return self.token2idx[token]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2token)\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary mapping from word to index based on the sentences.\n",
    "    Returns vocabulary mapping and inverse vocabulary mapping.\n",
    "    \"\"\"\n",
    "    # Build vocabulary\n",
    "    word_counts = Counter(itertools.chain(*sentences))\n",
    "    # Mapping from index to word\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "    # Mapping from word to index\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    return [vocabulary, vocabulary_inv]\n",
    "\n",
    "def pad_sentences(sentences, labels, padding_word=\"<PAD/>\", max_len = 300):\n",
    "    \"\"\"\n",
    "    Pads all sentences to the same length. The length is defined by the longest sentence.\n",
    "    Returns padded sentences.\n",
    "    \"\"\"\n",
    "    sequence_length = max_len\n",
    "    padded_sentences = []\n",
    "    fianal_sent = []\n",
    "    padded_labels = []\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        n_trams = 0\n",
    "        new_sentence = []\n",
    "        for j,word in enumerate(sentence):\n",
    "            new_sentence.append(word)\n",
    "            if (j+1)%sequence_length == 0:\n",
    "                padded_sentences.append(new_sentence)\n",
    "                padded_labels.append(labels[i])\n",
    "                new_sentence = []\n",
    "                \n",
    "        num_padding = sequence_length - len(new_sentence)\n",
    "        if 0 < num_padding < sequence_length:\n",
    "            new_sentence = new_sentence + [padding_word] * num_padding\n",
    "            padded_sentences.append(new_sentence)\n",
    "            padded_labels.append(labels[i])\n",
    "        \n",
    "    return padded_sentences,padded_labels\n",
    "\n",
    "# Prepare X as clean word list without articles, and non interesting words:\n",
    "x_text = [clean_str(s).split(\" \") for s in x_train_full]\n",
    "lens = [len(x) for x in x_text]\n",
    "print(\"Max words: {}\\nMin words:{}\".format(max(lens),min(lens)))\n",
    "\n",
    "# Prepare padding\n",
    "x_train, y_train = pad_sentences(x_text, y_train_full)\n",
    "# Vocab dict\n",
    "vocabulary, vocabulary_inv = build_vocab(x_train)\n",
    "\n",
    "lang_vocab = Dictionary()\n",
    "# use python set to obtain the list of languages without repetitions\n",
    "languages = set(y_train_full)\n",
    "for lang in sorted(languages):\n",
    "    lang_vocab.add_token(lang)\n",
    "print(\"Labels:\", len(lang_vocab), \"languages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118079, 300)\n",
      "vocab size       = 678055\n",
      "max sentence len = 300\n",
      "num of classes   = 235\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_input_data(sentences, labels, vocabulary, languages):\n",
    "    \"\"\"\n",
    "    Maps sentencs and labels to vectors based on a vocabulary.\n",
    "    \"\"\"\n",
    "    x = np.array([[vocabulary[word] for word in sentence] for sentence in sentences])\n",
    "    print(x.shape)\n",
    "    y = np.array([languages.token2idx[label] for label in labels])\n",
    "    return x, y\n",
    "\n",
    "X_train, Y_train = build_input_data(x_train, y_train, vocabulary, lang_vocab)\n",
    "\n",
    "vocab_size = len(vocabulary)\n",
    "sentence_len = X_train.shape[1]\n",
    "num_classes = int(max(Y_train)) +1 # added int() to convert np.int64 to int\n",
    "\n",
    "print('vocab size       = {}'.format(vocab_size))\n",
    "print('max sentence len = {}'.format(sentence_len))\n",
    "print('num of classes   = {}'.format(num_classes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, kernel_sizes=[3,4,5], num_filters=512, embedding_dim=256, pretrained_embeddings=None):\n",
    "        super(CNN, self).__init__()\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        ConvMethod = \"in_channel__is_embedding_dim\"\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        #self.embedding.weight.data.copy_(torch.from_numpy(pretrained_embeddings))\n",
    "        #self.embedding.weight.requires_grad = mode==\"nonstatic\"\n",
    "\n",
    "        if use_cuda:\n",
    "            self.embedding = self.embedding.cuda()\n",
    "\n",
    "        conv_blocks = []\n",
    "        for kernel_size in kernel_sizes:\n",
    "            # maxpool kernel_size must <= sentence_len - kernel_size+1, otherwise, it could output empty\n",
    "            maxpool_kernel_size = sentence_len - kernel_size +1\n",
    "\n",
    "            \n",
    "            conv1d = nn.Conv1d(in_channels = 1, out_channels = num_filters, kernel_size = kernel_size*embedding_dim, stride = embedding_dim)\n",
    "\n",
    "            component = nn.Sequential(\n",
    "                conv1d,\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(kernel_size = maxpool_kernel_size)\n",
    "            )\n",
    "            if use_cuda:\n",
    "                component = component.cuda()\n",
    "\n",
    "            conv_blocks.append(component)\n",
    "\n",
    "            if 0:\n",
    "                conv_blocks.append(\n",
    "                nn.Sequential(\n",
    "                    conv1d,\n",
    "                    nn.ReLU(),\n",
    "                    nn.MaxPool1d(kernel_size = maxpool_kernel_size)\n",
    "                ).cuda()\n",
    "                )\n",
    "\n",
    "        self.conv_blocks = nn.ModuleList(conv_blocks)   # ModuleList is needed for registering parameters in conv_blocks\n",
    "        self.fc = nn.Linear(num_filters*len(kernel_sizes), num_classes)\n",
    "\n",
    "    def forward(self, x):       # x: (batch, sentence_len)\n",
    "        x = self.embedding(x)   # embedded x: (batch, sentence_len, embedding_dim)\n",
    "        #    input:  (batch, in_channel=1, in_length=sentence_len*embedding_dim),\n",
    "        #    output: (batch, out_channel=num_filters, out_length=sentence_len-...)\n",
    "        # needs to convert x to (batch, embedding_dim, sentence_len)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = x.unsqueeze(1)\n",
    "        x_list= [conv_block(x) for conv_block in self.conv_blocks]\n",
    "        out = torch.cat(x_list, 2)\n",
    "        out = out.view(out.size(0), 1,-1)\n",
    "        feature_extracted = out\n",
    "        out = F.dropout(out, p=0.5, training=self.training)\n",
    "        return self.fc(out), feature_extracted\n",
    "\n",
    "\n",
    "def evaluate(model, x_test, y_test):\n",
    "    inputs = Variable(x_test)\n",
    "    preds, vector = model(inputs)\n",
    "    preds = torch.max(preds, 1)[1]\n",
    "    if use_cuda:\n",
    "        preds = preds.cuda()\n",
    "    #eval_acc = sum(preds.data == y_test) / len(y_test)          # pytorch 0.3\n",
    "    eval_acc = (preds.data == y_test).sum().item() / len(y_test) # pytorch 0.4\n",
    "    return eval_acc, vector.cpu().data.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "embedding_dim = 300\n",
    "num_filters = 100\n",
    "kernel_sizes = [3,4,5]\n",
    "batch_size = 50\n",
    "\n",
    "def load_pretrained_embeddings():\n",
    "    pretrained_fpath_saved = os.path.expanduser(\"models/googlenews_extracted-python{}.pl\".format(sys.version_info.major))\n",
    "    if os.path.exists(pretrained_fpath_saved):\n",
    "        with open(pretrained_fpath_saved, 'rb') as f:\n",
    "            embedding_weights = pickle.load(f)\n",
    "    else:\n",
    "        print('- Error: file not found : {}\\n'.format(pretrained_fpath_saved))\n",
    "        print('- Please run the code \"python utils.py\" to generate the file first\\n\\n')\n",
    "        sys.exit()\n",
    "\n",
    "    # embedding_weights is a dictionary {word_index:numpy_array_of_300_dim}\n",
    "    out = np.array(list(embedding_weights.values())) # added list() to convert dict_values to a list for use in python 3\n",
    "    #np.random.shuffle(out)\n",
    "\n",
    "    print('embedding_weights shape:', out.shape)\n",
    "    # pretrained embeddings is a numpy matrix of shape (num_embeddings, embedding_dim)\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_pretrained_embeddings = False\n",
    "\n",
    "if use_pretrained_embeddings:\n",
    "    pretrained_embeddings = load_pretrained_embeddings()\n",
    "else:\n",
    "    pretrained_embeddingseddings = np.random.uniform(-0.01, -0.01, size=(vocab_size, embedding_dim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (embedding): Embedding(678055, 256)\n",
      "  (conv_blocks): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv1d(1, 512, kernel_size=(768,), stride=(256,))\n",
      "      (1): ReLU()\n",
      "      (2): MaxPool1d(kernel_size=298, stride=298, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv1d(1, 512, kernel_size=(1024,), stride=(256,))\n",
      "      (1): ReLU()\n",
      "      (2): MaxPool1d(kernel_size=297, stride=297, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv1d(1, 512, kernel_size=(1280,), stride=(256,))\n",
      "      (1): ReLU()\n",
      "      (2): MaxPool1d(kernel_size=296, stride=296, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1536, out_features=235, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "use_cuda =torch.cuda.is_available\n",
    "\n",
    "model = CNN().cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.7916298828125\n",
      "Accuracy:  0.009\n",
      "Loss: 5.67406298828125\n",
      "Accuracy:  0.019\n",
      "Loss: 5.55397705078125\n",
      "Accuracy:  0.0185\n",
      "Loss: 5.38517724609375\n",
      "Accuracy:  0.0245\n",
      "Loss: 5.26264404296875\n",
      "Accuracy:  0.029\n",
      "Loss: 5.19072265625\n",
      "Accuracy:  0.0295\n",
      "Loss: 5.066814453125\n",
      "Accuracy:  0.0485\n",
      "Loss: 5.01407666015625\n",
      "Accuracy:  0.063\n",
      "Loss: 4.90265478515625\n",
      "Accuracy:  0.0665\n",
      "Loss: 4.8226259765625\n",
      "Accuracy:  0.0775\n",
      "Loss: 4.75219482421875\n",
      "Accuracy:  0.081\n",
      "Loss: 4.6866455078125\n",
      "Accuracy:  0.0875\n",
      "Loss: 4.6469111328125\n",
      "Accuracy:  0.0805\n",
      "Loss: 4.57485498046875\n",
      "Accuracy:  0.096\n",
      "Loss: 4.48790380859375\n",
      "Accuracy:  0.1\n",
      "Loss: 4.44159814453125\n",
      "Accuracy:  0.111\n",
      "Loss: 4.3303076171875\n",
      "Accuracy:  0.1165\n",
      "Loss: 4.2509013671875\n",
      "Accuracy:  0.137\n",
      "Loss: 4.26089892578125\n",
      "Accuracy:  0.127\n",
      "Loss: 4.2242333984375\n",
      "Accuracy:  0.131\n",
      "Loss: 4.15829052734375\n",
      "Accuracy:  0.147\n",
      "Loss: 4.037376708984375\n",
      "Accuracy:  0.169\n",
      "Loss: 4.039865478515625\n",
      "Accuracy:  0.1605\n",
      "Loss: 3.931399169921875\n",
      "Accuracy:  0.184\n",
      "Loss: 3.89369677734375\n",
      "Accuracy:  0.181\n",
      "Loss: 3.79148486328125\n",
      "Accuracy:  0.1905\n",
      "Loss: 3.80641162109375\n",
      "Accuracy:  0.1935\n",
      "Loss: 3.754388427734375\n",
      "Accuracy:  0.2055\n",
      "Loss: 3.679336669921875\n",
      "Accuracy:  0.214\n",
      "Loss: 3.64950830078125\n",
      "Accuracy:  0.226\n",
      "Loss: 3.516330322265625\n",
      "Accuracy:  0.2455\n",
      "Loss: 3.520997802734375\n",
      "Accuracy:  0.2355\n",
      "Loss: 3.463148193359375\n",
      "Accuracy:  0.2485\n",
      "Loss: 3.380638916015625\n",
      "Accuracy:  0.2635\n",
      "Loss: 3.368451416015625\n",
      "Accuracy:  0.2705\n",
      "Loss: 3.321398193359375\n",
      "Accuracy:  0.2755\n",
      "Loss: 3.249322998046875\n",
      "Accuracy:  0.29\n",
      "Loss: 3.191569580078125\n",
      "Accuracy:  0.2985\n",
      "Loss: 3.145269775390625\n",
      "Accuracy:  0.294\n",
      "Loss: 3.131116943359375\n",
      "Accuracy:  0.3065\n",
      "Loss: 3.064544677734375\n",
      "Accuracy:  0.31\n",
      "Loss: 3.02114111328125\n",
      "Accuracy:  0.315\n",
      "Loss: 3.044211669921875\n",
      "Accuracy:  0.313\n",
      "Loss: 2.956835693359375\n",
      "Accuracy:  0.3185\n",
      "Loss: 2.909001220703125\n",
      "Accuracy:  0.3445\n",
      "Loss: 2.88895947265625\n",
      "Accuracy:  0.3485\n",
      "Loss: 2.8780498046875\n",
      "Accuracy:  0.349\n",
      "Loss: 2.870537841796875\n",
      "Accuracy:  0.341\n",
      "Loss: 2.778115478515625\n",
      "Accuracy:  0.363\n",
      "Loss: 2.76935009765625\n",
      "Accuracy:  0.3655\n",
      "Loss: 2.703179443359375\n",
      "Accuracy:  0.388\n",
      "Loss: 2.646741455078125\n",
      "Accuracy:  0.383\n",
      "Loss: 2.68342626953125\n",
      "Accuracy:  0.3815\n",
      "Loss: 2.714892333984375\n",
      "Accuracy:  0.3825\n",
      "Loss: 2.6072919921875\n",
      "Accuracy:  0.4\n",
      "Loss: 2.57621484375\n",
      "Accuracy:  0.402\n",
      "Loss: 2.509529052734375\n",
      "Accuracy:  0.412\n",
      "Loss: 2.533358642578125\n",
      "Accuracy:  0.4015\n",
      "Loss: 2.528849927684929\n",
      "Accuracy:  0.43037974683544306\n",
      "21.32555323131124\n",
      "3.819642776927202\n",
      "Train: wpb=0, bsz=1967, num_updates=60\n",
      "Loss: 2.34118408203125\n",
      "Accuracy:  0.446\n",
      "Loss: 2.360970703125\n",
      "Accuracy:  0.4475\n",
      "Loss: 2.358506591796875\n",
      "Accuracy:  0.4575\n",
      "Loss: 2.36716357421875\n",
      "Accuracy:  0.4415\n",
      "Loss: 2.425482666015625\n",
      "Accuracy:  0.4365\n",
      "Loss: 2.2658740234375\n",
      "Accuracy:  0.469\n",
      "Loss: 2.33387646484375\n",
      "Accuracy:  0.449\n",
      "Loss: 2.343140380859375\n",
      "Accuracy:  0.462\n",
      "Loss: 2.2714951171875\n",
      "Accuracy:  0.474\n",
      "Loss: 2.324677001953125\n",
      "Accuracy:  0.463\n",
      "Loss: 2.301451416015625\n",
      "Accuracy:  0.4645\n",
      "Loss: 2.224614990234375\n",
      "Accuracy:  0.483\n",
      "Loss: 2.274272705078125\n",
      "Accuracy:  0.465\n",
      "Loss: 2.237742919921875\n",
      "Accuracy:  0.471\n",
      "Loss: 2.25423095703125\n",
      "Accuracy:  0.461\n",
      "Loss: 2.21764208984375\n",
      "Accuracy:  0.497\n",
      "Loss: 2.203038818359375\n",
      "Accuracy:  0.491\n",
      "Loss: 2.158091796875\n",
      "Accuracy:  0.4985\n",
      "Loss: 2.191368408203125\n",
      "Accuracy:  0.4905\n",
      "Loss: 2.177380126953125\n",
      "Accuracy:  0.477\n",
      "Loss: 2.1760341796875\n",
      "Accuracy:  0.4855\n",
      "Loss: 2.17723779296875\n",
      "Accuracy:  0.4995\n",
      "Loss: 2.16345703125\n",
      "Accuracy:  0.482\n",
      "Loss: 2.22972314453125\n",
      "Accuracy:  0.4715\n",
      "Loss: 2.125221435546875\n",
      "Accuracy:  0.505\n",
      "Loss: 2.128549560546875\n",
      "Accuracy:  0.4955\n",
      "Loss: 2.06140283203125\n",
      "Accuracy:  0.5155\n",
      "Loss: 2.170613525390625\n",
      "Accuracy:  0.486\n",
      "Loss: 2.106396728515625\n",
      "Accuracy:  0.512\n",
      "Loss: 2.058319580078125\n",
      "Accuracy:  0.506\n",
      "Loss: 2.13640869140625\n",
      "Accuracy:  0.491\n",
      "Loss: 2.145562744140625\n",
      "Accuracy:  0.498\n",
      "Loss: 2.088677490234375\n",
      "Accuracy:  0.5035\n",
      "Loss: 2.089311279296875\n",
      "Accuracy:  0.5045\n",
      "Loss: 2.147236083984375\n",
      "Accuracy:  0.497\n",
      "Loss: 2.0358062744140626\n",
      "Accuracy:  0.5205\n",
      "Loss: 2.0404891357421877\n",
      "Accuracy:  0.5175\n",
      "Loss: 1.969343505859375\n",
      "Accuracy:  0.538\n",
      "Loss: 2.026802490234375\n",
      "Accuracy:  0.532\n",
      "Loss: 2.023359619140625\n",
      "Accuracy:  0.5145\n",
      "Loss: 2.111951171875\n",
      "Accuracy:  0.506\n",
      "Loss: 2.119386962890625\n",
      "Accuracy:  0.5055\n",
      "Loss: 2.035718017578125\n",
      "Accuracy:  0.5275\n",
      "Loss: 2.0182843017578125\n",
      "Accuracy:  0.5295\n",
      "Loss: 2.0098892822265624\n",
      "Accuracy:  0.526\n",
      "Loss: 2.120654296875\n",
      "Accuracy:  0.503\n",
      "Loss: 2.000267333984375\n",
      "Accuracy:  0.527\n",
      "Loss: 1.9686873779296874\n",
      "Accuracy:  0.544\n",
      "Loss: 2.0346046142578125\n",
      "Accuracy:  0.5215\n",
      "Loss: 2.0031365966796875\n",
      "Accuracy:  0.539\n",
      "Loss: 2.055939697265625\n",
      "Accuracy:  0.5185\n",
      "Loss: 1.9891031494140625\n",
      "Accuracy:  0.5285\n",
      "Loss: 2.046084716796875\n",
      "Accuracy:  0.517\n",
      "Loss: 2.006200439453125\n",
      "Accuracy:  0.522\n",
      "Loss: 2.0042293701171876\n",
      "Accuracy:  0.5265\n",
      "Loss: 2.0527890625\n",
      "Accuracy:  0.5235\n",
      "Loss: 1.9204586181640626\n",
      "Accuracy:  0.552\n",
      "Loss: 2.02481591796875\n",
      "Accuracy:  0.5285\n",
      "Loss: 1.725932833514636\n",
      "Accuracy:  0.6075949367088608\n",
      "35.48344752242143\n",
      "1.073757723397443\n",
      "Train: wpb=0, bsz=1967, num_updates=120\n",
      "Loss: 1.97553173828125\n",
      "Accuracy:  0.5325\n",
      "Loss: 1.920388671875\n",
      "Accuracy:  0.5555\n",
      "Loss: 1.778142578125\n",
      "Accuracy:  0.584\n",
      "Loss: 1.8584422607421875\n",
      "Accuracy:  0.5545\n",
      "Loss: 1.809497802734375\n",
      "Accuracy:  0.5765\n",
      "Loss: 1.768411376953125\n",
      "Accuracy:  0.576\n",
      "Loss: 1.8310760498046874\n",
      "Accuracy:  0.5675\n",
      "Loss: 1.870813720703125\n",
      "Accuracy:  0.5455\n",
      "Loss: 1.850491943359375\n",
      "Accuracy:  0.561\n",
      "Loss: 1.90086669921875\n",
      "Accuracy:  0.546\n",
      "Loss: 1.760977783203125\n",
      "Accuracy:  0.58\n",
      "Loss: 1.8012393798828126\n",
      "Accuracy:  0.5785\n",
      "Loss: 1.853614990234375\n",
      "Accuracy:  0.557\n",
      "Loss: 1.934712646484375\n",
      "Accuracy:  0.538\n",
      "Loss: 1.852971923828125\n",
      "Accuracy:  0.5595\n",
      "Loss: 1.8957781982421875\n",
      "Accuracy:  0.5595\n",
      "Loss: 1.8944498291015626\n",
      "Accuracy:  0.546\n",
      "Loss: 1.767174560546875\n",
      "Accuracy:  0.5775\n",
      "Loss: 1.8552113037109375\n",
      "Accuracy:  0.558\n",
      "Loss: 1.871432861328125\n",
      "Accuracy:  0.551\n",
      "Loss: 1.793074951171875\n",
      "Accuracy:  0.5625\n",
      "Loss: 1.8627015380859375\n",
      "Accuracy:  0.5565\n",
      "Loss: 1.78582568359375\n",
      "Accuracy:  0.569\n",
      "Loss: 1.80141064453125\n",
      "Accuracy:  0.571\n",
      "Loss: 1.85481982421875\n",
      "Accuracy:  0.5535\n",
      "Loss: 1.8476441650390625\n",
      "Accuracy:  0.562\n",
      "Loss: 1.82683203125\n",
      "Accuracy:  0.5685\n",
      "Loss: 1.9048709716796874\n",
      "Accuracy:  0.554\n",
      "Loss: 1.7587374267578124\n",
      "Accuracy:  0.581\n",
      "Loss: 1.7728729248046875\n",
      "Accuracy:  0.5695\n",
      "Loss: 1.9223326416015625\n",
      "Accuracy:  0.5465\n",
      "Loss: 1.77450537109375\n",
      "Accuracy:  0.581\n",
      "Loss: 1.8604599609375\n",
      "Accuracy:  0.5595\n",
      "Loss: 1.7391044921875\n",
      "Accuracy:  0.5795\n",
      "Loss: 1.7710869140625\n",
      "Accuracy:  0.5655\n",
      "Loss: 1.857826171875\n",
      "Accuracy:  0.5625\n",
      "Loss: 1.812840087890625\n",
      "Accuracy:  0.569\n",
      "Loss: 1.8740167236328125\n",
      "Accuracy:  0.5525\n",
      "Loss: 1.780619873046875\n",
      "Accuracy:  0.5785\n",
      "Loss: 1.7360333251953124\n",
      "Accuracy:  0.581\n",
      "Loss: 1.8099093017578125\n",
      "Accuracy:  0.565\n",
      "Loss: 1.7912027587890624\n",
      "Accuracy:  0.57\n",
      "Loss: 1.7419493408203126\n",
      "Accuracy:  0.588\n",
      "Loss: 1.8398548583984375\n",
      "Accuracy:  0.5645\n",
      "Loss: 1.7453240966796875\n",
      "Accuracy:  0.577\n",
      "Loss: 1.76743310546875\n",
      "Accuracy:  0.586\n",
      "Loss: 1.8292791748046875\n",
      "Accuracy:  0.5635\n",
      "Loss: 1.7325228271484374\n",
      "Accuracy:  0.589\n",
      "Loss: 1.7947767333984375\n",
      "Accuracy:  0.5765\n",
      "Loss: 1.7503902587890625\n",
      "Accuracy:  0.585\n",
      "Loss: 1.8353743896484376\n",
      "Accuracy:  0.566\n",
      "Loss: 1.738831298828125\n",
      "Accuracy:  0.5815\n",
      "Loss: 1.867338134765625\n",
      "Accuracy:  0.5545\n",
      "Loss: 1.8424796142578126\n",
      "Accuracy:  0.5545\n",
      "Loss: 1.8008013916015626\n",
      "Accuracy:  0.578\n",
      "Loss: 1.8085391845703125\n",
      "Accuracy:  0.558\n",
      "Loss: 1.7959522705078126\n",
      "Accuracy:  0.5605\n",
      "Loss: 1.8295367431640626\n",
      "Accuracy:  0.559\n",
      "Loss: 1.6323839018616495\n",
      "Accuracy:  0.5569620253164557\n",
      "42.506852756770186\n",
      "0.6078172894147804\n",
      "Train: wpb=0, bsz=1967, num_updates=180\n",
      "Loss: 1.673560302734375\n",
      "Accuracy:  0.599\n",
      "Loss: 1.6390853271484376\n",
      "Accuracy:  0.604\n",
      "Loss: 1.675132568359375\n",
      "Accuracy:  0.5955\n",
      "Loss: 1.7452484130859376\n",
      "Accuracy:  0.5875\n",
      "Loss: 1.6933226318359376\n",
      "Accuracy:  0.608\n",
      "Loss: 1.711824462890625\n",
      "Accuracy:  0.5895\n",
      "Loss: 1.602021728515625\n",
      "Accuracy:  0.6145\n",
      "Loss: 1.652052001953125\n",
      "Accuracy:  0.5955\n",
      "Loss: 1.7109490966796874\n",
      "Accuracy:  0.5855\n",
      "Loss: 1.6586458740234375\n",
      "Accuracy:  0.608\n",
      "Loss: 1.6973673095703126\n",
      "Accuracy:  0.5855\n",
      "Loss: 1.6672723388671875\n",
      "Accuracy:  0.609\n",
      "Loss: 1.6586246337890624\n",
      "Accuracy:  0.6\n",
      "Loss: 1.7084910888671876\n",
      "Accuracy:  0.5845\n",
      "Loss: 1.5887803955078126\n",
      "Accuracy:  0.622\n",
      "Loss: 1.6538753662109376\n",
      "Accuracy:  0.615\n",
      "Loss: 1.676107421875\n",
      "Accuracy:  0.6015\n",
      "Loss: 1.653143798828125\n",
      "Accuracy:  0.594\n",
      "Loss: 1.7157288818359375\n",
      "Accuracy:  0.589\n",
      "Loss: 1.61744091796875\n",
      "Accuracy:  0.6165\n",
      "Loss: 1.6638656005859376\n",
      "Accuracy:  0.604\n",
      "Loss: 1.7092227783203124\n",
      "Accuracy:  0.594\n",
      "Loss: 1.6572811279296875\n",
      "Accuracy:  0.6015\n",
      "Loss: 1.6339769287109376\n",
      "Accuracy:  0.609\n",
      "Loss: 1.720500244140625\n",
      "Accuracy:  0.5935\n",
      "Loss: 1.6435902099609374\n",
      "Accuracy:  0.614\n",
      "Loss: 1.674645751953125\n",
      "Accuracy:  0.5985\n",
      "Loss: 1.6577952880859375\n",
      "Accuracy:  0.606\n",
      "Loss: 1.7005120849609374\n",
      "Accuracy:  0.591\n",
      "Loss: 1.669205810546875\n",
      "Accuracy:  0.6\n",
      "Loss: 1.6251043701171874\n",
      "Accuracy:  0.6095\n",
      "Loss: 1.66403369140625\n",
      "Accuracy:  0.6035\n",
      "Loss: 1.58295361328125\n",
      "Accuracy:  0.614\n",
      "Loss: 1.6685391845703126\n",
      "Accuracy:  0.5905\n",
      "Loss: 1.6909105224609375\n",
      "Accuracy:  0.595\n",
      "Loss: 1.643366455078125\n",
      "Accuracy:  0.5975\n",
      "Loss: 1.706075927734375\n",
      "Accuracy:  0.583\n",
      "Loss: 1.627617431640625\n",
      "Accuracy:  0.6165\n",
      "Loss: 1.5661668701171876\n",
      "Accuracy:  0.628\n",
      "Loss: 1.5831280517578126\n",
      "Accuracy:  0.6215\n",
      "Loss: 1.691135986328125\n",
      "Accuracy:  0.5925\n",
      "Loss: 1.7709969482421875\n",
      "Accuracy:  0.573\n",
      "Loss: 1.7204263916015625\n",
      "Accuracy:  0.5955\n",
      "Loss: 1.7345406494140625\n",
      "Accuracy:  0.5765\n",
      "Loss: 1.6236092529296875\n",
      "Accuracy:  0.6145\n",
      "Loss: 1.678354736328125\n",
      "Accuracy:  0.6005\n",
      "Loss: 1.66691015625\n",
      "Accuracy:  0.606\n",
      "Loss: 1.70160986328125\n",
      "Accuracy:  0.592\n",
      "Loss: 1.681828125\n",
      "Accuracy:  0.601\n",
      "Loss: 1.6264578857421874\n",
      "Accuracy:  0.614\n",
      "Loss: 1.67235302734375\n",
      "Accuracy:  0.6005\n",
      "Loss: 1.6454044189453125\n",
      "Accuracy:  0.604\n",
      "Loss: 1.666382568359375\n",
      "Accuracy:  0.6025\n",
      "Loss: 1.7551302490234375\n",
      "Accuracy:  0.594\n",
      "Loss: 1.669299560546875\n",
      "Accuracy:  0.6015\n",
      "Loss: 1.651244873046875\n",
      "Accuracy:  0.604\n",
      "Loss: 1.61330419921875\n",
      "Accuracy:  0.6065\n",
      "Loss: 1.661843994140625\n",
      "Accuracy:  0.596\n",
      "Loss: 1.5826822594751286\n",
      "Accuracy:  0.620253164556962\n",
      "46.90461470710287\n",
      "0.4165710756133218\n",
      "Train: wpb=0, bsz=1967, num_updates=240\n",
      "Loss: 1.624509521484375\n",
      "Accuracy:  0.6105\n",
      "Loss: 1.5736376953125\n",
      "Accuracy:  0.6125\n",
      "Loss: 1.5014014892578125\n",
      "Accuracy:  0.643\n",
      "Loss: 1.5316070556640624\n",
      "Accuracy:  0.638\n",
      "Loss: 1.52347021484375\n",
      "Accuracy:  0.632\n",
      "Loss: 1.5222105712890626\n",
      "Accuracy:  0.6295\n",
      "Loss: 1.5264354248046874\n",
      "Accuracy:  0.63\n",
      "Loss: 1.58197119140625\n",
      "Accuracy:  0.619\n",
      "Loss: 1.518316162109375\n",
      "Accuracy:  0.631\n",
      "Loss: 1.5336922607421875\n",
      "Accuracy:  0.6185\n",
      "Loss: 1.5604349365234376\n",
      "Accuracy:  0.6295\n",
      "Loss: 1.631766357421875\n",
      "Accuracy:  0.608\n",
      "Loss: 1.643061279296875\n",
      "Accuracy:  0.606\n",
      "Loss: 1.4668687744140625\n",
      "Accuracy:  0.645\n",
      "Loss: 1.545765625\n",
      "Accuracy:  0.6225\n",
      "Loss: 1.6407630615234374\n",
      "Accuracy:  0.601\n",
      "Loss: 1.5553792724609374\n",
      "Accuracy:  0.6195\n",
      "Loss: 1.5184976806640624\n",
      "Accuracy:  0.636\n",
      "Loss: 1.553230712890625\n",
      "Accuracy:  0.629\n",
      "Loss: 1.61729736328125\n",
      "Accuracy:  0.606\n",
      "Loss: 1.5579517822265625\n",
      "Accuracy:  0.618\n",
      "Loss: 1.538160400390625\n",
      "Accuracy:  0.633\n",
      "Loss: 1.5537655029296875\n",
      "Accuracy:  0.6245\n",
      "Loss: 1.4390391845703125\n",
      "Accuracy:  0.652\n",
      "Loss: 1.5790467529296874\n",
      "Accuracy:  0.62\n",
      "Loss: 1.55637158203125\n",
      "Accuracy:  0.617\n",
      "Loss: 1.5802225341796876\n",
      "Accuracy:  0.6275\n",
      "Loss: 1.5263194580078125\n",
      "Accuracy:  0.637\n",
      "Loss: 1.5113529052734376\n",
      "Accuracy:  0.6285\n",
      "Loss: 1.542415283203125\n",
      "Accuracy:  0.6205\n",
      "Loss: 1.534230712890625\n",
      "Accuracy:  0.627\n",
      "Loss: 1.53602880859375\n",
      "Accuracy:  0.636\n",
      "Loss: 1.5171512451171876\n",
      "Accuracy:  0.637\n",
      "Loss: 1.5557257080078124\n",
      "Accuracy:  0.612\n",
      "Loss: 1.5686649169921876\n",
      "Accuracy:  0.6245\n",
      "Loss: 1.616461669921875\n",
      "Accuracy:  0.604\n",
      "Loss: 1.62384619140625\n",
      "Accuracy:  0.609\n",
      "Loss: 1.5768397216796874\n",
      "Accuracy:  0.6185\n",
      "Loss: 1.566798583984375\n",
      "Accuracy:  0.6245\n",
      "Loss: 1.520704345703125\n",
      "Accuracy:  0.6325\n",
      "Loss: 1.5610587158203124\n",
      "Accuracy:  0.612\n",
      "Loss: 1.55274951171875\n",
      "Accuracy:  0.6315\n",
      "Loss: 1.5689354248046874\n",
      "Accuracy:  0.6265\n",
      "Loss: 1.56310888671875\n",
      "Accuracy:  0.6245\n",
      "Loss: 1.467954345703125\n",
      "Accuracy:  0.6475\n",
      "Loss: 1.5208389892578125\n",
      "Accuracy:  0.6415\n",
      "Loss: 1.5458177490234375\n",
      "Accuracy:  0.624\n",
      "Loss: 1.5145966796875\n",
      "Accuracy:  0.635\n",
      "Loss: 1.54280517578125\n",
      "Accuracy:  0.627\n",
      "Loss: 1.5794638671875\n",
      "Accuracy:  0.6195\n",
      "Loss: 1.5359893798828126\n",
      "Accuracy:  0.6185\n",
      "Loss: 1.5544652099609375\n",
      "Accuracy:  0.627\n",
      "Loss: 1.527306884765625\n",
      "Accuracy:  0.6315\n",
      "Loss: 1.5990517578125\n",
      "Accuracy:  0.617\n",
      "Loss: 1.518465087890625\n",
      "Accuracy:  0.628\n",
      "Loss: 1.6228843994140625\n",
      "Accuracy:  0.6125\n",
      "Loss: 1.48142041015625\n",
      "Accuracy:  0.646\n",
      "Loss: 1.5294512939453124\n",
      "Accuracy:  0.6325\n",
      "Loss: 1.6508933924421478\n",
      "Accuracy:  0.5949367088607594\n",
      "50.03125026465332\n",
      "0.3102125132139031\n",
      "Train: wpb=0, bsz=1967, num_updates=300\n",
      "Loss: 1.4577357177734376\n",
      "Accuracy:  0.651\n",
      "Loss: 1.447279296875\n",
      "Accuracy:  0.656\n",
      "Loss: 1.483924072265625\n",
      "Accuracy:  0.647\n",
      "Loss: 1.3867962646484375\n",
      "Accuracy:  0.67\n",
      "Loss: 1.4158037109375\n",
      "Accuracy:  0.664\n",
      "Loss: 1.4699390869140625\n",
      "Accuracy:  0.6425\n",
      "Loss: 1.4334173583984375\n",
      "Accuracy:  0.6505\n",
      "Loss: 1.4406678466796874\n",
      "Accuracy:  0.6525\n",
      "Loss: 1.4159149169921874\n",
      "Accuracy:  0.662\n",
      "Loss: 1.3855233154296875\n",
      "Accuracy:  0.659\n",
      "Loss: 1.4704638671875\n",
      "Accuracy:  0.641\n",
      "Loss: 1.4198760986328125\n",
      "Accuracy:  0.6585\n",
      "Loss: 1.4761124267578125\n",
      "Accuracy:  0.6605\n",
      "Loss: 1.43607470703125\n",
      "Accuracy:  0.644\n",
      "Loss: 1.4529755859375\n",
      "Accuracy:  0.648\n",
      "Loss: 1.472440185546875\n",
      "Accuracy:  0.647\n",
      "Loss: 1.440670166015625\n",
      "Accuracy:  0.6425\n",
      "Loss: 1.5007781982421875\n",
      "Accuracy:  0.636\n",
      "Loss: 1.5333531494140624\n",
      "Accuracy:  0.6325\n",
      "Loss: 1.487210693359375\n",
      "Accuracy:  0.6375\n",
      "Loss: 1.4497784423828124\n",
      "Accuracy:  0.647\n",
      "Loss: 1.3993753662109376\n",
      "Accuracy:  0.658\n",
      "Loss: 1.4468536376953125\n",
      "Accuracy:  0.6435\n",
      "Loss: 1.479144775390625\n",
      "Accuracy:  0.6395\n",
      "Loss: 1.467449462890625\n",
      "Accuracy:  0.6485\n",
      "Loss: 1.3863828125\n",
      "Accuracy:  0.6625\n",
      "Loss: 1.4135333251953126\n",
      "Accuracy:  0.6485\n",
      "Loss: 1.492500732421875\n",
      "Accuracy:  0.6395\n",
      "Loss: 1.4191429443359376\n",
      "Accuracy:  0.6565\n",
      "Loss: 1.4462178955078124\n",
      "Accuracy:  0.648\n",
      "Loss: 1.49261376953125\n",
      "Accuracy:  0.628\n",
      "Loss: 1.4772666015625\n",
      "Accuracy:  0.639\n",
      "Loss: 1.4155860595703125\n",
      "Accuracy:  0.6485\n",
      "Loss: 1.5231007080078125\n",
      "Accuracy:  0.6295\n",
      "Loss: 1.407996826171875\n",
      "Accuracy:  0.648\n",
      "Loss: 1.44314892578125\n",
      "Accuracy:  0.6495\n",
      "Loss: 1.45390966796875\n",
      "Accuracy:  0.6445\n",
      "Loss: 1.57945166015625\n",
      "Accuracy:  0.6225\n",
      "Loss: 1.4735611572265626\n",
      "Accuracy:  0.6365\n",
      "Loss: 1.47483984375\n",
      "Accuracy:  0.6445\n",
      "Loss: 1.464255859375\n",
      "Accuracy:  0.656\n",
      "Loss: 1.461955322265625\n",
      "Accuracy:  0.639\n",
      "Loss: 1.46105029296875\n",
      "Accuracy:  0.654\n",
      "Loss: 1.504438232421875\n",
      "Accuracy:  0.6315\n",
      "Loss: 1.4405501708984374\n",
      "Accuracy:  0.6465\n",
      "Loss: 1.4711351318359376\n",
      "Accuracy:  0.647\n",
      "Loss: 1.4765145263671875\n",
      "Accuracy:  0.6445\n",
      "Loss: 1.434311279296875\n",
      "Accuracy:  0.655\n",
      "Loss: 1.485774169921875\n",
      "Accuracy:  0.644\n",
      "Loss: 1.4154605712890624\n",
      "Accuracy:  0.6535\n",
      "Loss: 1.445061767578125\n",
      "Accuracy:  0.6495\n",
      "Loss: 1.4243216552734375\n",
      "Accuracy:  0.6565\n",
      "Loss: 1.4533077392578124\n",
      "Accuracy:  0.6485\n",
      "Loss: 1.4703946533203125\n",
      "Accuracy:  0.6515\n",
      "Loss: 1.4825191650390626\n",
      "Accuracy:  0.6315\n",
      "Loss: 1.4550062255859375\n",
      "Accuracy:  0.6475\n",
      "Loss: 1.3966064453125\n",
      "Accuracy:  0.6565\n",
      "Loss: 1.446085693359375\n",
      "Accuracy:  0.646\n",
      "Loss: 1.6136485232582576\n",
      "Accuracy:  0.5949367088607594\n",
      "52.4835068047663\n",
      "0.24231891604646127\n",
      "Train: wpb=0, bsz=1967, num_updates=360\n",
      "Loss: 1.3758602294921876\n",
      "Accuracy:  0.6675\n",
      "Loss: 1.3353983154296876\n",
      "Accuracy:  0.6715\n",
      "Loss: 1.2870062255859376\n",
      "Accuracy:  0.6915\n",
      "Loss: 1.294020751953125\n",
      "Accuracy:  0.6865\n",
      "Loss: 1.37633154296875\n",
      "Accuracy:  0.667\n",
      "Loss: 1.3476905517578126\n",
      "Accuracy:  0.675\n",
      "Loss: 1.3153525390625\n",
      "Accuracy:  0.686\n",
      "Loss: 1.335582275390625\n",
      "Accuracy:  0.675\n",
      "Loss: 1.394594482421875\n",
      "Accuracy:  0.663\n",
      "Loss: 1.42278466796875\n",
      "Accuracy:  0.6535\n",
      "Loss: 1.3809755859375\n",
      "Accuracy:  0.6575\n",
      "Loss: 1.326267822265625\n",
      "Accuracy:  0.676\n",
      "Loss: 1.41192724609375\n",
      "Accuracy:  0.6545\n",
      "Loss: 1.3181358642578125\n",
      "Accuracy:  0.6755\n",
      "Loss: 1.35776220703125\n",
      "Accuracy:  0.6695\n",
      "Loss: 1.3489876708984374\n",
      "Accuracy:  0.666\n",
      "Loss: 1.4035572509765626\n",
      "Accuracy:  0.658\n",
      "Loss: 1.3820274658203124\n",
      "Accuracy:  0.663\n",
      "Loss: 1.359085205078125\n",
      "Accuracy:  0.667\n",
      "Loss: 1.320383544921875\n",
      "Accuracy:  0.6805\n",
      "Loss: 1.3787139892578124\n",
      "Accuracy:  0.6645\n",
      "Loss: 1.4110323486328125\n",
      "Accuracy:  0.658\n",
      "Loss: 1.342059814453125\n",
      "Accuracy:  0.6665\n",
      "Loss: 1.3243419189453125\n",
      "Accuracy:  0.6695\n",
      "Loss: 1.3408160400390625\n",
      "Accuracy:  0.674\n",
      "Loss: 1.3472120361328126\n",
      "Accuracy:  0.6655\n",
      "Loss: 1.3590081787109376\n",
      "Accuracy:  0.6705\n",
      "Loss: 1.3771761474609374\n",
      "Accuracy:  0.6705\n",
      "Loss: 1.43158740234375\n",
      "Accuracy:  0.654\n",
      "Loss: 1.339212158203125\n",
      "Accuracy:  0.669\n",
      "Loss: 1.4646639404296875\n",
      "Accuracy:  0.6505\n",
      "Loss: 1.3536241455078124\n",
      "Accuracy:  0.674\n",
      "Loss: 1.2885809326171875\n",
      "Accuracy:  0.6875\n",
      "Loss: 1.3685552978515625\n",
      "Accuracy:  0.6625\n",
      "Loss: 1.39308056640625\n",
      "Accuracy:  0.661\n",
      "Loss: 1.417835205078125\n",
      "Accuracy:  0.653\n",
      "Loss: 1.3455858154296876\n",
      "Accuracy:  0.6815\n",
      "Loss: 1.3265784912109375\n",
      "Accuracy:  0.6765\n",
      "Loss: 1.3845460205078124\n",
      "Accuracy:  0.6625\n",
      "Loss: 1.398658935546875\n",
      "Accuracy:  0.6515\n",
      "Loss: 1.42080859375\n",
      "Accuracy:  0.651\n",
      "Loss: 1.479906494140625\n",
      "Accuracy:  0.6375\n",
      "Loss: 1.3636563720703125\n",
      "Accuracy:  0.6715\n",
      "Loss: 1.3892559814453125\n",
      "Accuracy:  0.6635\n",
      "Loss: 1.2931314697265626\n",
      "Accuracy:  0.689\n",
      "Loss: 1.343666748046875\n",
      "Accuracy:  0.679\n",
      "Loss: 1.443299072265625\n",
      "Accuracy:  0.646\n",
      "Loss: 1.3734815673828125\n",
      "Accuracy:  0.6645\n",
      "Loss: 1.3522919921875\n",
      "Accuracy:  0.669\n",
      "Loss: 1.385374755859375\n",
      "Accuracy:  0.6575\n",
      "Loss: 1.4395848388671875\n",
      "Accuracy:  0.6585\n",
      "Loss: 1.38166455078125\n",
      "Accuracy:  0.663\n",
      "Loss: 1.3437388916015625\n",
      "Accuracy:  0.676\n",
      "Loss: 1.3442242431640625\n",
      "Accuracy:  0.6735\n",
      "Loss: 1.403055908203125\n",
      "Accuracy:  0.6725\n",
      "Loss: 1.3451312255859376\n",
      "Accuracy:  0.6735\n",
      "Loss: 1.3966920166015624\n",
      "Accuracy:  0.658\n",
      "Loss: 1.3800797119140624\n",
      "Accuracy:  0.665\n",
      "Loss: 1.6768240626854232\n",
      "Accuracy:  0.6075949367088608\n",
      "54.51628631194854\n",
      "0.19550789005753724\n",
      "Train: wpb=0, bsz=1967, num_updates=420\n",
      "Loss: 1.2878284912109375\n",
      "Accuracy:  0.68\n",
      "Loss: 1.216109619140625\n",
      "Accuracy:  0.697\n",
      "Loss: 1.2913173828125\n",
      "Accuracy:  0.6815\n",
      "Loss: 1.282228759765625\n",
      "Accuracy:  0.694\n",
      "Loss: 1.3437869873046875\n",
      "Accuracy:  0.6795\n",
      "Loss: 1.2021138916015626\n",
      "Accuracy:  0.7055\n",
      "Loss: 1.29314794921875\n",
      "Accuracy:  0.686\n",
      "Loss: 1.2752296142578126\n",
      "Accuracy:  0.6905\n",
      "Loss: 1.293573974609375\n",
      "Accuracy:  0.687\n",
      "Loss: 1.3255322265625\n",
      "Accuracy:  0.6725\n",
      "Loss: 1.3140853271484374\n",
      "Accuracy:  0.68\n",
      "Loss: 1.3050660400390626\n",
      "Accuracy:  0.688\n",
      "Loss: 1.2921571044921876\n",
      "Accuracy:  0.673\n",
      "Loss: 1.3643243408203125\n",
      "Accuracy:  0.667\n",
      "Loss: 1.2531185302734376\n",
      "Accuracy:  0.702\n",
      "Loss: 1.2743843994140625\n",
      "Accuracy:  0.6845\n",
      "Loss: 1.27192431640625\n",
      "Accuracy:  0.6915\n",
      "Loss: 1.246608642578125\n",
      "Accuracy:  0.689\n",
      "Loss: 1.195924072265625\n",
      "Accuracy:  0.7095\n",
      "Loss: 1.3282713623046876\n",
      "Accuracy:  0.6745\n",
      "Loss: 1.28812939453125\n",
      "Accuracy:  0.685\n",
      "Loss: 1.198493896484375\n",
      "Accuracy:  0.7085\n",
      "Loss: 1.284887939453125\n",
      "Accuracy:  0.679\n",
      "Loss: 1.3041475830078124\n",
      "Accuracy:  0.685\n",
      "Loss: 1.25963232421875\n",
      "Accuracy:  0.6975\n",
      "Loss: 1.3491971435546875\n",
      "Accuracy:  0.6675\n",
      "Loss: 1.311514892578125\n",
      "Accuracy:  0.6805\n",
      "Loss: 1.2342724609375\n",
      "Accuracy:  0.702\n",
      "Loss: 1.31803955078125\n",
      "Accuracy:  0.675\n",
      "Loss: 1.2757557373046875\n",
      "Accuracy:  0.692\n",
      "Loss: 1.3231253662109375\n",
      "Accuracy:  0.678\n",
      "Loss: 1.307080810546875\n",
      "Accuracy:  0.6815\n",
      "Loss: 1.2675863037109374\n",
      "Accuracy:  0.6865\n",
      "Loss: 1.2753707275390624\n",
      "Accuracy:  0.6855\n",
      "Loss: 1.283208984375\n",
      "Accuracy:  0.6835\n",
      "Loss: 1.334921630859375\n",
      "Accuracy:  0.669\n",
      "Loss: 1.22296728515625\n",
      "Accuracy:  0.7015\n",
      "Loss: 1.2860711669921876\n",
      "Accuracy:  0.684\n",
      "Loss: 1.2984959716796876\n",
      "Accuracy:  0.682\n",
      "Loss: 1.354798095703125\n",
      "Accuracy:  0.6655\n",
      "Loss: 1.3942308349609376\n",
      "Accuracy:  0.6705\n",
      "Loss: 1.302700439453125\n",
      "Accuracy:  0.68\n",
      "Loss: 1.303175537109375\n",
      "Accuracy:  0.683\n",
      "Loss: 1.2343065185546875\n",
      "Accuracy:  0.7095\n",
      "Loss: 1.3109075927734375\n",
      "Accuracy:  0.6795\n",
      "Loss: 1.3245684814453125\n",
      "Accuracy:  0.6735\n",
      "Loss: 1.3222755126953125\n",
      "Accuracy:  0.673\n",
      "Loss: 1.250621826171875\n",
      "Accuracy:  0.6975\n",
      "Loss: 1.332474609375\n",
      "Accuracy:  0.678\n",
      "Loss: 1.36026708984375\n",
      "Accuracy:  0.67\n",
      "Loss: 1.323672607421875\n",
      "Accuracy:  0.6805\n",
      "Loss: 1.337709716796875\n",
      "Accuracy:  0.6855\n",
      "Loss: 1.24543408203125\n",
      "Accuracy:  0.694\n",
      "Loss: 1.297582275390625\n",
      "Accuracy:  0.68\n",
      "Loss: 1.3415572509765625\n",
      "Accuracy:  0.674\n",
      "Loss: 1.3294532470703124\n",
      "Accuracy:  0.6775\n",
      "Loss: 1.3012493896484374\n",
      "Accuracy:  0.6835\n",
      "Loss: 1.2344237060546874\n",
      "Accuracy:  0.6985\n",
      "Loss: 0.865018917035453\n",
      "Accuracy:  0.759493670886076\n",
      "56.26053320234758\n",
      "0.16156538036934445\n",
      "Train: wpb=0, bsz=1967, num_updates=480\n",
      "Loss: 1.2958819580078125\n",
      "Accuracy:  0.6775\n",
      "Loss: 1.2439010009765625\n",
      "Accuracy:  0.687\n",
      "Loss: 1.18555859375\n",
      "Accuracy:  0.706\n",
      "Loss: 1.2851033935546874\n",
      "Accuracy:  0.692\n",
      "Loss: 1.1627862548828125\n",
      "Accuracy:  0.721\n",
      "Loss: 1.21756982421875\n",
      "Accuracy:  0.7085\n",
      "Loss: 1.1953125\n",
      "Accuracy:  0.717\n",
      "Loss: 1.193384765625\n",
      "Accuracy:  0.714\n",
      "Loss: 1.1806746826171874\n",
      "Accuracy:  0.7105\n",
      "Loss: 1.16634130859375\n",
      "Accuracy:  0.72\n",
      "Loss: 1.2866741943359374\n",
      "Accuracy:  0.693\n",
      "Loss: 1.206885009765625\n",
      "Accuracy:  0.7045\n",
      "Loss: 1.1853272705078124\n",
      "Accuracy:  0.709\n",
      "Loss: 1.235896728515625\n",
      "Accuracy:  0.703\n",
      "Loss: 1.25989501953125\n",
      "Accuracy:  0.7065\n",
      "Loss: 1.2033226318359376\n",
      "Accuracy:  0.705\n",
      "Loss: 1.195963623046875\n",
      "Accuracy:  0.7105\n",
      "Loss: 1.15615966796875\n",
      "Accuracy:  0.7175\n",
      "Loss: 1.2117034912109375\n",
      "Accuracy:  0.6995\n",
      "Loss: 1.1984376220703126\n",
      "Accuracy:  0.6995\n",
      "Loss: 1.188304931640625\n",
      "Accuracy:  0.7135\n",
      "Loss: 1.2423487548828125\n",
      "Accuracy:  0.695\n",
      "Loss: 1.202361572265625\n",
      "Accuracy:  0.699\n",
      "Loss: 1.1756446533203124\n",
      "Accuracy:  0.7155\n",
      "Loss: 1.21534130859375\n",
      "Accuracy:  0.707\n",
      "Loss: 1.2306143798828124\n",
      "Accuracy:  0.7\n",
      "Loss: 1.2038609619140626\n",
      "Accuracy:  0.704\n",
      "Loss: 1.201014892578125\n",
      "Accuracy:  0.692\n",
      "Loss: 1.19482763671875\n",
      "Accuracy:  0.7085\n",
      "Loss: 1.2647711181640624\n",
      "Accuracy:  0.6955\n",
      "Loss: 1.2048055419921875\n",
      "Accuracy:  0.702\n",
      "Loss: 1.2361748046875\n",
      "Accuracy:  0.7\n",
      "Loss: 1.2502860107421876\n",
      "Accuracy:  0.6905\n",
      "Loss: 1.1592164306640624\n",
      "Accuracy:  0.7115\n",
      "Loss: 1.2854451904296875\n",
      "Accuracy:  0.6955\n",
      "Loss: 1.232885009765625\n",
      "Accuracy:  0.6995\n",
      "Loss: 1.217954345703125\n",
      "Accuracy:  0.6995\n",
      "Loss: 1.22818505859375\n",
      "Accuracy:  0.697\n",
      "Loss: 1.2363153076171876\n",
      "Accuracy:  0.6925\n",
      "Loss: 1.2768778076171876\n",
      "Accuracy:  0.685\n",
      "Loss: 1.279767578125\n",
      "Accuracy:  0.691\n",
      "Loss: 1.2653101806640625\n",
      "Accuracy:  0.697\n",
      "Loss: 1.2822022705078124\n",
      "Accuracy:  0.683\n",
      "Loss: 1.2528828125\n",
      "Accuracy:  0.691\n",
      "Loss: 1.273725341796875\n",
      "Accuracy:  0.686\n",
      "Loss: 1.216041259765625\n",
      "Accuracy:  0.7015\n",
      "Loss: 1.328669921875\n",
      "Accuracy:  0.674\n",
      "Loss: 1.2728673095703125\n",
      "Accuracy:  0.683\n",
      "Loss: 1.2642432861328126\n",
      "Accuracy:  0.692\n",
      "Loss: 1.2647288818359375\n",
      "Accuracy:  0.692\n",
      "Loss: 1.2558148193359375\n",
      "Accuracy:  0.6895\n",
      "Loss: 1.1858267822265625\n",
      "Accuracy:  0.702\n",
      "Loss: 1.25089697265625\n",
      "Accuracy:  0.697\n",
      "Loss: 1.2138507080078125\n",
      "Accuracy:  0.7055\n",
      "Loss: 1.232332763671875\n",
      "Accuracy:  0.697\n",
      "Loss: 1.256322998046875\n",
      "Accuracy:  0.701\n",
      "Loss: 1.218541748046875\n",
      "Accuracy:  0.705\n",
      "Loss: 1.2707855224609375\n",
      "Accuracy:  0.695\n",
      "Loss: 1.7803924174248418\n",
      "Accuracy:  0.569620253164557\n",
      "57.787488790461374\n",
      "0.1364974677197464\n",
      "Train: wpb=0, bsz=1967, num_updates=540\n",
      "Loss: 1.209967529296875\n",
      "Accuracy:  0.71\n",
      "Loss: 1.1698162841796875\n",
      "Accuracy:  0.7165\n",
      "Loss: 1.161617431640625\n",
      "Accuracy:  0.716\n",
      "Loss: 1.152628173828125\n",
      "Accuracy:  0.7175\n",
      "Loss: 1.1626644287109376\n",
      "Accuracy:  0.721\n",
      "Loss: 1.1265828857421876\n",
      "Accuracy:  0.7225\n",
      "Loss: 1.1104422607421875\n",
      "Accuracy:  0.7315\n",
      "Loss: 1.1232191162109375\n",
      "Accuracy:  0.7255\n",
      "Loss: 1.1077940673828126\n",
      "Accuracy:  0.725\n",
      "Loss: 1.1604981689453124\n",
      "Accuracy:  0.722\n",
      "Loss: 1.1353656005859376\n",
      "Accuracy:  0.7245\n",
      "Loss: 1.050502197265625\n",
      "Accuracy:  0.7355\n",
      "Loss: 1.19688720703125\n",
      "Accuracy:  0.7135\n",
      "Loss: 1.121242919921875\n",
      "Accuracy:  0.7175\n",
      "Loss: 1.1387825927734374\n",
      "Accuracy:  0.7155\n",
      "Loss: 1.1624984130859375\n",
      "Accuracy:  0.7135\n",
      "Loss: 1.1677965087890625\n",
      "Accuracy:  0.715\n",
      "Loss: 1.21719091796875\n",
      "Accuracy:  0.706\n",
      "Loss: 1.13129736328125\n",
      "Accuracy:  0.717\n",
      "Loss: 1.099577392578125\n",
      "Accuracy:  0.7355\n",
      "Loss: 1.1645989990234376\n",
      "Accuracy:  0.718\n",
      "Loss: 1.2117352294921875\n",
      "Accuracy:  0.7005\n",
      "Loss: 1.1398348388671875\n",
      "Accuracy:  0.724\n",
      "Loss: 1.189725830078125\n",
      "Accuracy:  0.7085\n",
      "Loss: 1.1469263916015624\n",
      "Accuracy:  0.7145\n",
      "Loss: 1.161552490234375\n",
      "Accuracy:  0.7135\n",
      "Loss: 1.1670540771484375\n",
      "Accuracy:  0.7255\n",
      "Loss: 1.1319471435546875\n",
      "Accuracy:  0.724\n",
      "Loss: 1.1206961669921875\n",
      "Accuracy:  0.732\n",
      "Loss: 1.1267459716796875\n",
      "Accuracy:  0.7225\n",
      "Loss: 1.1586802978515625\n",
      "Accuracy:  0.72\n",
      "Loss: 1.1454141845703125\n",
      "Accuracy:  0.707\n",
      "Loss: 1.1294356689453124\n",
      "Accuracy:  0.7135\n",
      "Loss: 1.203193359375\n",
      "Accuracy:  0.711\n",
      "Loss: 1.1935184326171875\n",
      "Accuracy:  0.7055\n",
      "Loss: 1.1427393798828125\n",
      "Accuracy:  0.722\n",
      "Loss: 1.1740557861328125\n",
      "Accuracy:  0.7095\n",
      "Loss: 1.2096741943359375\n",
      "Accuracy:  0.702\n",
      "Loss: 1.221989990234375\n",
      "Accuracy:  0.701\n",
      "Loss: 1.1691424560546875\n",
      "Accuracy:  0.7125\n",
      "Loss: 1.21832421875\n",
      "Accuracy:  0.7005\n",
      "Loss: 1.20325341796875\n",
      "Accuracy:  0.709\n",
      "Loss: 1.1958271484375\n",
      "Accuracy:  0.7065\n",
      "Loss: 1.1593167724609375\n",
      "Accuracy:  0.7145\n",
      "Loss: 1.1555198974609375\n",
      "Accuracy:  0.716\n",
      "Loss: 1.1682607421875\n",
      "Accuracy:  0.7095\n",
      "Loss: 1.1955213623046874\n",
      "Accuracy:  0.708\n",
      "Loss: 1.1794105224609375\n",
      "Accuracy:  0.712\n",
      "Loss: 1.1776002197265625\n",
      "Accuracy:  0.7045\n",
      "Loss: 1.264713623046875\n",
      "Accuracy:  0.688\n",
      "Loss: 1.23831298828125\n",
      "Accuracy:  0.695\n",
      "Loss: 1.209736572265625\n",
      "Accuracy:  0.709\n",
      "Loss: 1.147050048828125\n",
      "Accuracy:  0.722\n",
      "Loss: 1.2382655029296874\n",
      "Accuracy:  0.703\n",
      "Loss: 1.188221923828125\n",
      "Accuracy:  0.7095\n",
      "Loss: 1.2121229248046874\n",
      "Accuracy:  0.7065\n",
      "Loss: 1.2280347900390625\n",
      "Accuracy:  0.6995\n",
      "Loss: 1.188732666015625\n",
      "Accuracy:  0.701\n",
      "Loss: 1.5965271961839893\n",
      "Accuracy:  0.5822784810126582\n",
      "59.14591078853988\n",
      "0.1168630411742594\n",
      "Train: wpb=0, bsz=1967, num_updates=600\n",
      "Loss: 1.1409017333984375\n",
      "Accuracy:  0.727\n",
      "Loss: 1.0916993408203124\n",
      "Accuracy:  0.7375\n",
      "Loss: 1.1187620849609374\n",
      "Accuracy:  0.732\n",
      "Loss: 1.0630592041015625\n",
      "Accuracy:  0.736\n",
      "Loss: 1.0497806396484375\n",
      "Accuracy:  0.7455\n",
      "Loss: 1.0967064208984374\n",
      "Accuracy:  0.7355\n",
      "Loss: 1.0757640380859375\n",
      "Accuracy:  0.729\n",
      "Loss: 1.1113004150390624\n",
      "Accuracy:  0.732\n",
      "Loss: 1.1646285400390626\n",
      "Accuracy:  0.7205\n",
      "Loss: 1.006712158203125\n",
      "Accuracy:  0.763\n",
      "Loss: 1.06745556640625\n",
      "Accuracy:  0.7405\n",
      "Loss: 1.069357666015625\n",
      "Accuracy:  0.7465\n",
      "Loss: 1.0434686279296874\n",
      "Accuracy:  0.748\n",
      "Loss: 1.0024635620117188\n",
      "Accuracy:  0.759\n",
      "Loss: 1.1021866455078124\n",
      "Accuracy:  0.7355\n",
      "Loss: 1.073749755859375\n",
      "Accuracy:  0.7295\n",
      "Loss: 1.1282796630859375\n",
      "Accuracy:  0.7275\n",
      "Loss: 1.08047705078125\n",
      "Accuracy:  0.7335\n",
      "Loss: 1.06603173828125\n",
      "Accuracy:  0.7395\n",
      "Loss: 1.0860830078125\n",
      "Accuracy:  0.7335\n",
      "Loss: 1.1036494140625\n",
      "Accuracy:  0.725\n",
      "Loss: 1.133590087890625\n",
      "Accuracy:  0.716\n",
      "Loss: 1.1163580322265625\n",
      "Accuracy:  0.732\n",
      "Loss: 1.130369140625\n",
      "Accuracy:  0.7135\n",
      "Loss: 1.1481636962890625\n",
      "Accuracy:  0.729\n",
      "Loss: 1.1172266845703125\n",
      "Accuracy:  0.7275\n",
      "Loss: 1.0485262451171875\n",
      "Accuracy:  0.75\n",
      "Loss: 1.13974951171875\n",
      "Accuracy:  0.72\n",
      "Loss: 1.1995338134765625\n",
      "Accuracy:  0.7025\n",
      "Loss: 1.115451171875\n",
      "Accuracy:  0.721\n",
      "Loss: 1.1090045166015625\n",
      "Accuracy:  0.729\n",
      "Loss: 1.117330078125\n",
      "Accuracy:  0.726\n",
      "Loss: 1.0569580078125\n",
      "Accuracy:  0.744\n",
      "Loss: 1.145466064453125\n",
      "Accuracy:  0.7195\n",
      "Loss: 1.1367696533203124\n",
      "Accuracy:  0.724\n",
      "Loss: 1.1224564208984376\n",
      "Accuracy:  0.719\n",
      "Loss: 1.0939052734375\n",
      "Accuracy:  0.7335\n",
      "Loss: 1.09247314453125\n",
      "Accuracy:  0.7325\n",
      "Loss: 1.1474150390625\n",
      "Accuracy:  0.722\n",
      "Loss: 1.136980712890625\n",
      "Accuracy:  0.728\n",
      "Loss: 1.13456494140625\n",
      "Accuracy:  0.7205\n",
      "Loss: 1.131824462890625\n",
      "Accuracy:  0.718\n",
      "Loss: 1.13524462890625\n",
      "Accuracy:  0.724\n",
      "Loss: 1.1068984375\n",
      "Accuracy:  0.7325\n",
      "Loss: 1.0951290283203126\n",
      "Accuracy:  0.7295\n",
      "Loss: 1.056439208984375\n",
      "Accuracy:  0.734\n",
      "Loss: 1.2064329833984375\n",
      "Accuracy:  0.7125\n",
      "Loss: 1.182540771484375\n",
      "Accuracy:  0.714\n",
      "Loss: 1.1077664794921875\n",
      "Accuracy:  0.732\n",
      "Loss: 1.09871630859375\n",
      "Accuracy:  0.7255\n",
      "Loss: 1.1264892578125\n",
      "Accuracy:  0.7205\n",
      "Loss: 1.079906005859375\n",
      "Accuracy:  0.728\n",
      "Loss: 1.1438897705078126\n",
      "Accuracy:  0.7225\n",
      "Loss: 1.1156654052734376\n",
      "Accuracy:  0.726\n",
      "Loss: 1.098405029296875\n",
      "Accuracy:  0.7285\n",
      "Loss: 1.2132738037109374\n",
      "Accuracy:  0.7015\n",
      "Loss: 1.13649169921875\n",
      "Accuracy:  0.712\n",
      "Loss: 1.147710693359375\n",
      "Accuracy:  0.717\n",
      "Loss: 1.2199384472038173\n",
      "Accuracy:  0.7341772151898734\n",
      "60.39207957076503\n",
      "0.10096024014403587\n",
      "Train: wpb=0, bsz=1967, num_updates=660\n",
      "Loss: 1.06036083984375\n",
      "Accuracy:  0.7435\n",
      "Loss: 1.0096325073242187\n",
      "Accuracy:  0.7565\n",
      "Loss: 1.0066223754882813\n",
      "Accuracy:  0.7505\n",
      "Loss: 1.020133544921875\n",
      "Accuracy:  0.753\n",
      "Loss: 1.05410498046875\n",
      "Accuracy:  0.748\n",
      "Loss: 1.0240616455078124\n",
      "Accuracy:  0.7575\n",
      "Loss: 1.07376416015625\n",
      "Accuracy:  0.736\n",
      "Loss: 1.083325927734375\n",
      "Accuracy:  0.732\n",
      "Loss: 1.1350943603515624\n",
      "Accuracy:  0.7235\n",
      "Loss: 1.05824560546875\n",
      "Accuracy:  0.7495\n",
      "Loss: 1.066649658203125\n",
      "Accuracy:  0.739\n",
      "Loss: 1.0073384399414063\n",
      "Accuracy:  0.7635\n",
      "Loss: 1.04525537109375\n",
      "Accuracy:  0.752\n",
      "Loss: 1.16249755859375\n",
      "Accuracy:  0.719\n",
      "Loss: 0.992644287109375\n",
      "Accuracy:  0.759\n",
      "Loss: 1.0255989990234375\n",
      "Accuracy:  0.753\n",
      "Loss: 1.0732591552734374\n",
      "Accuracy:  0.7455\n",
      "Loss: 1.0200299072265624\n",
      "Accuracy:  0.745\n",
      "Loss: 1.0391011962890624\n",
      "Accuracy:  0.7555\n",
      "Loss: 1.0688388671875\n",
      "Accuracy:  0.742\n",
      "Loss: 1.0323275146484374\n",
      "Accuracy:  0.7495\n",
      "Loss: 0.9727325439453125\n",
      "Accuracy:  0.7595\n",
      "Loss: 1.0507789306640625\n",
      "Accuracy:  0.7375\n",
      "Loss: 1.0977540283203124\n",
      "Accuracy:  0.73\n",
      "Loss: 1.1043720703125\n",
      "Accuracy:  0.7295\n",
      "Loss: 1.0967945556640626\n",
      "Accuracy:  0.7295\n",
      "Loss: 1.0459820556640624\n",
      "Accuracy:  0.7405\n",
      "Loss: 1.0965968017578125\n",
      "Accuracy:  0.73\n",
      "Loss: 1.0808231201171874\n",
      "Accuracy:  0.7395\n",
      "Loss: 1.08872216796875\n",
      "Accuracy:  0.732\n",
      "Loss: 1.0351650390625\n",
      "Accuracy:  0.7415\n",
      "Loss: 1.0778370361328125\n",
      "Accuracy:  0.7355\n",
      "Loss: 0.9767417602539062\n",
      "Accuracy:  0.7545\n",
      "Loss: 1.0333626708984376\n",
      "Accuracy:  0.7535\n",
      "Loss: 1.068849609375\n",
      "Accuracy:  0.737\n",
      "Loss: 1.1298319091796876\n",
      "Accuracy:  0.7225\n",
      "Loss: 1.0460677490234376\n",
      "Accuracy:  0.736\n",
      "Loss: 1.10868505859375\n",
      "Accuracy:  0.7235\n",
      "Loss: 1.1198228759765625\n",
      "Accuracy:  0.718\n",
      "Loss: 1.042158203125\n",
      "Accuracy:  0.7365\n",
      "Loss: 1.02840283203125\n",
      "Accuracy:  0.755\n",
      "Loss: 1.0942977294921874\n",
      "Accuracy:  0.735\n",
      "Loss: 1.1136669921875\n",
      "Accuracy:  0.7245\n",
      "Loss: 1.0887845458984375\n",
      "Accuracy:  0.739\n",
      "Loss: 1.0603038330078125\n",
      "Accuracy:  0.7375\n",
      "Loss: 1.105816162109375\n",
      "Accuracy:  0.727\n",
      "Loss: 1.089796142578125\n",
      "Accuracy:  0.736\n",
      "Loss: 1.0306171875\n",
      "Accuracy:  0.746\n",
      "Loss: 1.0660506591796874\n",
      "Accuracy:  0.7335\n",
      "Loss: 1.1468140869140624\n",
      "Accuracy:  0.7215\n",
      "Loss: 1.1047049560546875\n",
      "Accuracy:  0.7355\n",
      "Loss: 1.07063525390625\n",
      "Accuracy:  0.732\n",
      "Loss: 1.1301787109375\n",
      "Accuracy:  0.7155\n",
      "Loss: 1.04523876953125\n",
      "Accuracy:  0.7425\n",
      "Loss: 1.054156005859375\n",
      "Accuracy:  0.7345\n",
      "Loss: 1.1358348388671875\n",
      "Accuracy:  0.7195\n",
      "Loss: 1.166748779296875\n",
      "Accuracy:  0.715\n",
      "Loss: 1.04398291015625\n",
      "Accuracy:  0.741\n",
      "Loss: 0.9931697604022448\n",
      "Accuracy:  0.7721518987341772\n",
      "61.51665410445549\n",
      "0.08891989303345513\n",
      "Train: wpb=0, bsz=1967, num_updates=720\n",
      "Loss: 1.0191170654296875\n",
      "Accuracy:  0.7455\n",
      "Loss: 1.0372369384765625\n",
      "Accuracy:  0.745\n",
      "Loss: 0.9734623413085938\n",
      "Accuracy:  0.7635\n",
      "Loss: 0.9751953125\n",
      "Accuracy:  0.7625\n",
      "Loss: 1.0252413330078125\n",
      "Accuracy:  0.7525\n",
      "Loss: 1.0345579833984375\n",
      "Accuracy:  0.7465\n",
      "Loss: 1.0115529174804687\n",
      "Accuracy:  0.75\n",
      "Loss: 0.9900394287109375\n",
      "Accuracy:  0.7585\n",
      "Loss: 0.9213103637695312\n",
      "Accuracy:  0.779\n",
      "Loss: 0.9603211059570312\n",
      "Accuracy:  0.768\n",
      "Loss: 0.9816595458984375\n",
      "Accuracy:  0.7655\n",
      "Loss: 0.9922078857421875\n",
      "Accuracy:  0.753\n",
      "Loss: 1.083876953125\n",
      "Accuracy:  0.736\n",
      "Loss: 0.9914766235351562\n",
      "Accuracy:  0.75\n",
      "Loss: 1.08493701171875\n",
      "Accuracy:  0.733\n",
      "Loss: 0.9975757446289063\n",
      "Accuracy:  0.7535\n",
      "Loss: 0.9999834594726562\n",
      "Accuracy:  0.755\n",
      "Loss: 1.1285518798828125\n",
      "Accuracy:  0.7235\n",
      "Loss: 1.0599595947265625\n",
      "Accuracy:  0.7415\n",
      "Loss: 1.0195320434570312\n",
      "Accuracy:  0.7525\n",
      "Loss: 1.0904661865234375\n",
      "Accuracy:  0.732\n",
      "Loss: 1.0233243408203125\n",
      "Accuracy:  0.7525\n",
      "Loss: 0.9610413208007812\n",
      "Accuracy:  0.7565\n",
      "Loss: 0.9931140747070313\n",
      "Accuracy:  0.755\n",
      "Loss: 1.0663056640625\n",
      "Accuracy:  0.7365\n",
      "Loss: 1.0195203247070312\n",
      "Accuracy:  0.7575\n",
      "Loss: 1.017122802734375\n",
      "Accuracy:  0.742\n",
      "Loss: 0.9746312866210938\n",
      "Accuracy:  0.757\n",
      "Loss: 1.05745947265625\n",
      "Accuracy:  0.744\n",
      "Loss: 1.0196922607421874\n",
      "Accuracy:  0.744\n",
      "Loss: 1.0174920654296875\n",
      "Accuracy:  0.7525\n",
      "Loss: 1.04592822265625\n",
      "Accuracy:  0.7395\n",
      "Loss: 0.97172705078125\n",
      "Accuracy:  0.768\n",
      "Loss: 1.0721240234375\n",
      "Accuracy:  0.7445\n",
      "Loss: 1.0757421875\n",
      "Accuracy:  0.7325\n",
      "Loss: 1.0024453735351562\n",
      "Accuracy:  0.7505\n",
      "Loss: 0.9594556274414062\n",
      "Accuracy:  0.762\n",
      "Loss: 1.0504866943359374\n",
      "Accuracy:  0.747\n",
      "Loss: 0.9990682373046875\n",
      "Accuracy:  0.7535\n",
      "Loss: 1.0173253173828125\n",
      "Accuracy:  0.7555\n",
      "Loss: 1.0055164794921876\n",
      "Accuracy:  0.755\n",
      "Loss: 1.0397880859375\n",
      "Accuracy:  0.7425\n",
      "Loss: 1.102435546875\n",
      "Accuracy:  0.723\n",
      "Loss: 1.07358056640625\n",
      "Accuracy:  0.7395\n",
      "Loss: 1.0133837280273437\n",
      "Accuracy:  0.752\n",
      "Loss: 1.0204517822265624\n",
      "Accuracy:  0.7595\n",
      "Loss: 1.048232177734375\n",
      "Accuracy:  0.744\n",
      "Loss: 0.9919943237304687\n",
      "Accuracy:  0.762\n",
      "Loss: 1.0489210205078126\n",
      "Accuracy:  0.7475\n",
      "Loss: 1.022421875\n",
      "Accuracy:  0.7495\n",
      "Loss: 1.0585321044921876\n",
      "Accuracy:  0.7355\n",
      "Loss: 1.0561573486328124\n",
      "Accuracy:  0.7385\n",
      "Loss: 1.0495262451171874\n",
      "Accuracy:  0.754\n",
      "Loss: 0.9912978515625\n",
      "Accuracy:  0.7545\n",
      "Loss: 1.0909935302734375\n",
      "Accuracy:  0.735\n",
      "Loss: 1.0100526123046876\n",
      "Accuracy:  0.749\n",
      "Loss: 1.10989794921875\n",
      "Accuracy:  0.7265\n",
      "Loss: 1.06192626953125\n",
      "Accuracy:  0.7435\n",
      "Loss: 0.9190148462223101\n",
      "Accuracy:  0.7341772151898734\n",
      "62.54632654669918\n",
      "0.07886299958949881\n",
      "Train: wpb=0, bsz=1967, num_updates=780\n",
      "Loss: 0.9664561157226562\n",
      "Accuracy:  0.7665\n",
      "Loss: 0.9707857666015625\n",
      "Accuracy:  0.767\n",
      "Loss: 0.97911865234375\n",
      "Accuracy:  0.7625\n",
      "Loss: 0.950597412109375\n",
      "Accuracy:  0.762\n",
      "Loss: 0.953671142578125\n",
      "Accuracy:  0.771\n",
      "Loss: 0.9518207397460937\n",
      "Accuracy:  0.7705\n",
      "Loss: 1.0116270141601562\n",
      "Accuracy:  0.75\n",
      "Loss: 1.0132890014648437\n",
      "Accuracy:  0.7525\n",
      "Loss: 1.0078699340820312\n",
      "Accuracy:  0.757\n",
      "Loss: 0.9519569702148437\n",
      "Accuracy:  0.772\n",
      "Loss: 1.0061345825195311\n",
      "Accuracy:  0.7575\n",
      "Loss: 0.9375305786132813\n",
      "Accuracy:  0.7645\n",
      "Loss: 0.9678740234375\n",
      "Accuracy:  0.767\n",
      "Loss: 1.0074973754882812\n",
      "Accuracy:  0.7585\n",
      "Loss: 1.03537646484375\n",
      "Accuracy:  0.751\n",
      "Loss: 1.0198320922851563\n",
      "Accuracy:  0.745\n",
      "Loss: 0.9077635498046875\n",
      "Accuracy:  0.7795\n",
      "Loss: 1.0108333740234374\n",
      "Accuracy:  0.7545\n",
      "Loss: 0.9078576049804687\n",
      "Accuracy:  0.777\n",
      "Loss: 0.9546806030273437\n",
      "Accuracy:  0.767\n",
      "Loss: 0.9379576416015625\n",
      "Accuracy:  0.773\n",
      "Loss: 0.9818684692382813\n",
      "Accuracy:  0.7615\n",
      "Loss: 0.957798583984375\n",
      "Accuracy:  0.7705\n",
      "Loss: 0.9692380981445312\n",
      "Accuracy:  0.766\n",
      "Loss: 1.0192791137695312\n",
      "Accuracy:  0.747\n",
      "Loss: 1.070619140625\n",
      "Accuracy:  0.742\n",
      "Loss: 0.960871337890625\n",
      "Accuracy:  0.765\n",
      "Loss: 1.0160764770507813\n",
      "Accuracy:  0.761\n",
      "Loss: 1.012041259765625\n",
      "Accuracy:  0.7575\n",
      "Loss: 1.01383740234375\n",
      "Accuracy:  0.761\n",
      "Loss: 0.984117919921875\n",
      "Accuracy:  0.76\n",
      "Loss: 0.997334228515625\n",
      "Accuracy:  0.753\n",
      "Loss: 0.972741455078125\n",
      "Accuracy:  0.7625\n",
      "Loss: 1.0144986572265624\n",
      "Accuracy:  0.7555\n",
      "Loss: 0.9843468017578125\n",
      "Accuracy:  0.7545\n",
      "Loss: 0.9165610961914062\n",
      "Accuracy:  0.774\n",
      "Loss: 1.041308837890625\n",
      "Accuracy:  0.741\n",
      "Loss: 0.9959661254882812\n",
      "Accuracy:  0.7575\n",
      "Loss: 0.9944659423828125\n",
      "Accuracy:  0.7515\n",
      "Loss: 0.9686473999023437\n",
      "Accuracy:  0.7565\n",
      "Loss: 1.064477783203125\n",
      "Accuracy:  0.744\n",
      "Loss: 1.0170852661132812\n",
      "Accuracy:  0.7535\n",
      "Loss: 0.952079345703125\n",
      "Accuracy:  0.7625\n",
      "Loss: 0.9753447875976563\n",
      "Accuracy:  0.7575\n",
      "Loss: 0.9937241821289062\n",
      "Accuracy:  0.75\n",
      "Loss: 1.0370711669921875\n",
      "Accuracy:  0.7395\n",
      "Loss: 1.0177122192382813\n",
      "Accuracy:  0.75\n",
      "Loss: 1.0206962890625\n",
      "Accuracy:  0.7545\n",
      "Loss: 1.0179052124023438\n",
      "Accuracy:  0.7505\n",
      "Loss: 0.9394800415039063\n",
      "Accuracy:  0.771\n",
      "Loss: 0.8830091552734375\n",
      "Accuracy:  0.7765\n",
      "Loss: 1.00563916015625\n",
      "Accuracy:  0.7485\n",
      "Loss: 0.988460693359375\n",
      "Accuracy:  0.757\n",
      "Loss: 0.9926786499023438\n",
      "Accuracy:  0.756\n",
      "Loss: 1.04475634765625\n",
      "Accuracy:  0.745\n",
      "Loss: 0.9869400024414062\n",
      "Accuracy:  0.7575\n",
      "Loss: 1.0045525512695312\n",
      "Accuracy:  0.7555\n",
      "Loss: 1.0146220703125\n",
      "Accuracy:  0.751\n",
      "Loss: 1.2353442228293117\n",
      "Accuracy:  0.6835443037974683\n",
      "63.497198606743915\n",
      "0.07055847712233723\n",
      "Train: wpb=0, bsz=1967, num_updates=840\n",
      "Loss: 0.9653845825195313\n",
      "Accuracy:  0.767\n",
      "Loss: 0.9725428466796875\n",
      "Accuracy:  0.759\n",
      "Loss: 0.9727174682617188\n",
      "Accuracy:  0.767\n",
      "Loss: 0.9240255737304688\n",
      "Accuracy:  0.778\n",
      "Loss: 0.972227783203125\n",
      "Accuracy:  0.7595\n",
      "Loss: 0.8651550903320312\n",
      "Accuracy:  0.795\n",
      "Loss: 0.9151575927734374\n",
      "Accuracy:  0.7795\n",
      "Loss: 0.9540105590820313\n",
      "Accuracy:  0.7705\n",
      "Loss: 0.93244775390625\n",
      "Accuracy:  0.7725\n",
      "Loss: 0.885629150390625\n",
      "Accuracy:  0.782\n",
      "Loss: 0.9281932983398438\n",
      "Accuracy:  0.7775\n",
      "Loss: 0.9745298461914063\n",
      "Accuracy:  0.7735\n",
      "Loss: 0.9044228515625\n",
      "Accuracy:  0.7795\n",
      "Loss: 0.9297395629882812\n",
      "Accuracy:  0.773\n",
      "Loss: 0.922814208984375\n",
      "Accuracy:  0.778\n",
      "Loss: 0.9586232299804688\n",
      "Accuracy:  0.771\n",
      "Loss: 0.9827385864257813\n",
      "Accuracy:  0.7595\n",
      "Loss: 0.9560809936523438\n",
      "Accuracy:  0.762\n",
      "Loss: 0.9967562866210937\n",
      "Accuracy:  0.7615\n",
      "Loss: 1.0278165283203125\n",
      "Accuracy:  0.746\n",
      "Loss: 0.9596932983398437\n",
      "Accuracy:  0.757\n",
      "Loss: 0.9259464111328125\n",
      "Accuracy:  0.7725\n",
      "Loss: 0.9705250854492188\n",
      "Accuracy:  0.767\n",
      "Loss: 0.9459696655273437\n",
      "Accuracy:  0.7725\n",
      "Loss: 0.9699620361328125\n",
      "Accuracy:  0.7585\n",
      "Loss: 0.8938726196289063\n",
      "Accuracy:  0.7895\n",
      "Loss: 0.9404082641601562\n",
      "Accuracy:  0.767\n",
      "Loss: 0.9792463989257812\n",
      "Accuracy:  0.754\n",
      "Loss: 0.9436630249023438\n",
      "Accuracy:  0.7665\n",
      "Loss: 0.9639027099609375\n",
      "Accuracy:  0.7615\n",
      "Loss: 0.9590048828125\n",
      "Accuracy:  0.766\n",
      "Loss: 0.9844764404296875\n",
      "Accuracy:  0.761\n",
      "Loss: 0.93692041015625\n",
      "Accuracy:  0.778\n",
      "Loss: 0.9252581176757813\n",
      "Accuracy:  0.7815\n",
      "Loss: 0.96091357421875\n",
      "Accuracy:  0.768\n",
      "Loss: 1.0126463012695313\n",
      "Accuracy:  0.7535\n",
      "Loss: 0.8839619140625\n",
      "Accuracy:  0.7775\n",
      "Loss: 0.9955814819335937\n",
      "Accuracy:  0.757\n",
      "Loss: 0.94792578125\n",
      "Accuracy:  0.7665\n",
      "Loss: 0.9286787719726562\n",
      "Accuracy:  0.7715\n",
      "Loss: 0.8962041625976562\n",
      "Accuracy:  0.786\n",
      "Loss: 0.9252247924804687\n",
      "Accuracy:  0.779\n",
      "Loss: 1.0609915771484375\n",
      "Accuracy:  0.7355\n",
      "Loss: 0.9810918579101563\n",
      "Accuracy:  0.761\n",
      "Loss: 0.9407155151367188\n",
      "Accuracy:  0.7695\n",
      "Loss: 1.0147407836914062\n",
      "Accuracy:  0.7575\n",
      "Loss: 0.997241455078125\n",
      "Accuracy:  0.756\n",
      "Loss: 0.9297308959960937\n",
      "Accuracy:  0.7745\n",
      "Loss: 0.9487025756835937\n",
      "Accuracy:  0.7665\n",
      "Loss: 0.9882057495117188\n",
      "Accuracy:  0.76\n",
      "Loss: 1.0193220825195313\n",
      "Accuracy:  0.749\n",
      "Loss: 0.9670479125976562\n",
      "Accuracy:  0.7625\n",
      "Loss: 0.9916889038085938\n",
      "Accuracy:  0.751\n",
      "Loss: 0.9478532104492188\n",
      "Accuracy:  0.7655\n",
      "Loss: 0.9158108520507813\n",
      "Accuracy:  0.7785\n",
      "Loss: 0.97168310546875\n",
      "Accuracy:  0.7585\n",
      "Loss: 1.0161250610351562\n",
      "Accuracy:  0.75\n",
      "Loss: 0.9610740966796875\n",
      "Accuracy:  0.764\n",
      "Loss: 1.0165569450281844\n",
      "Accuracy:  0.7215189873417721\n",
      "64.37729542650824\n",
      "0.06372432571488243\n",
      "Train: wpb=0, bsz=1967, num_updates=900\n",
      "Loss: 0.9399713134765625\n",
      "Accuracy:  0.768\n",
      "Loss: 0.8904351196289062\n",
      "Accuracy:  0.7925\n",
      "Loss: 0.9451103515625\n",
      "Accuracy:  0.777\n",
      "Loss: 0.9197852783203125\n",
      "Accuracy:  0.782\n",
      "Loss: 0.9149533081054687\n",
      "Accuracy:  0.78\n",
      "Loss: 0.9199086303710937\n",
      "Accuracy:  0.774\n",
      "Loss: 0.9720398559570312\n",
      "Accuracy:  0.767\n",
      "Loss: 0.9280167236328125\n",
      "Accuracy:  0.778\n",
      "Loss: 0.880911865234375\n",
      "Accuracy:  0.7805\n",
      "Loss: 0.9100701293945312\n",
      "Accuracy:  0.782\n",
      "Loss: 0.9765196533203125\n",
      "Accuracy:  0.764\n",
      "Loss: 0.8846445922851562\n",
      "Accuracy:  0.78\n",
      "Loss: 0.973318359375\n",
      "Accuracy:  0.7685\n",
      "Loss: 0.9659986572265625\n",
      "Accuracy:  0.7685\n",
      "Loss: 0.9147784423828125\n",
      "Accuracy:  0.7725\n",
      "Loss: 0.9548234252929687\n",
      "Accuracy:  0.7705\n",
      "Loss: 0.9459083251953125\n",
      "Accuracy:  0.77\n",
      "Loss: 0.9211731567382813\n",
      "Accuracy:  0.773\n",
      "Loss: 0.8775826416015625\n",
      "Accuracy:  0.7915\n",
      "Loss: 0.9597156982421875\n",
      "Accuracy:  0.7555\n",
      "Loss: 0.8726116943359375\n",
      "Accuracy:  0.794\n",
      "Loss: 0.95890771484375\n",
      "Accuracy:  0.769\n",
      "Loss: 0.8939552001953125\n",
      "Accuracy:  0.7785\n",
      "Loss: 0.8899867553710937\n",
      "Accuracy:  0.7845\n",
      "Loss: 0.9146868896484375\n",
      "Accuracy:  0.7765\n",
      "Loss: 0.9413529663085938\n",
      "Accuracy:  0.7725\n",
      "Loss: 0.9577650756835937\n",
      "Accuracy:  0.7655\n",
      "Loss: 0.9808115844726563\n",
      "Accuracy:  0.7645\n",
      "Loss: 0.9193323364257813\n",
      "Accuracy:  0.7825\n",
      "Loss: 0.8976050415039063\n",
      "Accuracy:  0.779\n",
      "Loss: 0.9503515625\n",
      "Accuracy:  0.7655\n",
      "Loss: 0.94845166015625\n",
      "Accuracy:  0.767\n",
      "Loss: 0.9149882202148437\n",
      "Accuracy:  0.7765\n",
      "Loss: 0.9454068603515625\n",
      "Accuracy:  0.771\n",
      "Loss: 0.8718111572265625\n",
      "Accuracy:  0.7915\n",
      "Loss: 0.9617235107421875\n",
      "Accuracy:  0.765\n",
      "Loss: 0.8803818359375\n",
      "Accuracy:  0.788\n",
      "Loss: 0.846619873046875\n",
      "Accuracy:  0.7965\n",
      "Loss: 0.9198395385742187\n",
      "Accuracy:  0.7735\n",
      "Loss: 0.963180908203125\n",
      "Accuracy:  0.7705\n",
      "Loss: 0.9365695190429687\n",
      "Accuracy:  0.7735\n",
      "Loss: 0.9478125\n",
      "Accuracy:  0.7765\n",
      "Loss: 0.9216179809570313\n",
      "Accuracy:  0.776\n",
      "Loss: 0.9067670288085937\n",
      "Accuracy:  0.775\n",
      "Loss: 0.9124547119140625\n",
      "Accuracy:  0.7785\n",
      "Loss: 0.8988816528320313\n",
      "Accuracy:  0.783\n",
      "Loss: 0.8827606811523437\n",
      "Accuracy:  0.794\n",
      "Loss: 0.992904541015625\n",
      "Accuracy:  0.76\n",
      "Loss: 0.8569435424804688\n",
      "Accuracy:  0.79\n",
      "Loss: 0.8546643676757812\n",
      "Accuracy:  0.793\n",
      "Loss: 0.9727557983398437\n",
      "Accuracy:  0.7585\n",
      "Loss: 1.0032098999023438\n",
      "Accuracy:  0.7535\n",
      "Loss: 0.9193751220703125\n",
      "Accuracy:  0.7765\n",
      "Loss: 0.90421044921875\n",
      "Accuracy:  0.7705\n",
      "Loss: 0.9506770629882813\n",
      "Accuracy:  0.767\n",
      "Loss: 0.90792236328125\n",
      "Accuracy:  0.7745\n",
      "Loss: 0.9214009399414063\n",
      "Accuracy:  0.776\n",
      "Loss: 1.029741455078125\n",
      "Accuracy:  0.7445\n",
      "Loss: 1.0165088508702531\n",
      "Accuracy:  0.7468354430379747\n",
      "65.19829944359284\n",
      "0.05786257734747206\n",
      "Train: wpb=0, bsz=1967, num_updates=960\n",
      "Loss: 0.8902284545898438\n",
      "Accuracy:  0.786\n",
      "Loss: 0.8483544921875\n",
      "Accuracy:  0.794\n",
      "Loss: 0.9753517456054688\n",
      "Accuracy:  0.759\n",
      "Loss: 0.9034541015625\n",
      "Accuracy:  0.779\n",
      "Loss: 0.8576996459960937\n",
      "Accuracy:  0.791\n",
      "Loss: 0.8881149291992188\n",
      "Accuracy:  0.784\n",
      "Loss: 0.8996602172851562\n",
      "Accuracy:  0.7785\n",
      "Loss: 0.90280419921875\n",
      "Accuracy:  0.781\n",
      "Loss: 0.852449951171875\n",
      "Accuracy:  0.7935\n",
      "Loss: 0.8859756469726563\n",
      "Accuracy:  0.7875\n",
      "Loss: 0.886471923828125\n",
      "Accuracy:  0.7925\n",
      "Loss: 0.9198687133789063\n",
      "Accuracy:  0.781\n",
      "Loss: 0.8687385864257813\n",
      "Accuracy:  0.785\n",
      "Loss: 0.862605224609375\n",
      "Accuracy:  0.7945\n",
      "Loss: 0.8631910400390626\n",
      "Accuracy:  0.784\n",
      "Loss: 0.93287060546875\n",
      "Accuracy:  0.775\n",
      "Loss: 0.9336558227539062\n",
      "Accuracy:  0.772\n",
      "Loss: 0.8917997436523437\n",
      "Accuracy:  0.782\n",
      "Loss: 0.9391849365234375\n",
      "Accuracy:  0.7765\n",
      "Loss: 0.8563844604492188\n",
      "Accuracy:  0.796\n",
      "Loss: 0.9206524658203125\n",
      "Accuracy:  0.773\n",
      "Loss: 0.846513916015625\n",
      "Accuracy:  0.791\n",
      "Loss: 0.89424560546875\n",
      "Accuracy:  0.782\n",
      "Loss: 0.872592041015625\n",
      "Accuracy:  0.79\n",
      "Loss: 0.9285360717773438\n",
      "Accuracy:  0.777\n",
      "Loss: 0.9108178100585937\n",
      "Accuracy:  0.779\n",
      "Loss: 0.882301025390625\n",
      "Accuracy:  0.7785\n",
      "Loss: 0.9255575561523437\n",
      "Accuracy:  0.786\n",
      "Loss: 0.869437744140625\n",
      "Accuracy:  0.7905\n",
      "Loss: 0.8864506225585937\n",
      "Accuracy:  0.789\n",
      "Loss: 0.9844437866210938\n",
      "Accuracy:  0.7645\n",
      "Loss: 0.9025118408203125\n",
      "Accuracy:  0.7785\n",
      "Loss: 0.9202517700195313\n",
      "Accuracy:  0.7765\n",
      "Loss: 0.8981255493164062\n",
      "Accuracy:  0.7825\n",
      "Loss: 0.8695540771484375\n",
      "Accuracy:  0.7865\n",
      "Loss: 0.9925621337890626\n",
      "Accuracy:  0.7515\n",
      "Loss: 0.9368773193359375\n",
      "Accuracy:  0.7775\n",
      "Loss: 0.8615433959960938\n",
      "Accuracy:  0.7905\n",
      "Loss: 0.9148350219726562\n",
      "Accuracy:  0.7775\n",
      "Loss: 0.8807781982421875\n",
      "Accuracy:  0.7835\n",
      "Loss: 0.8924761962890625\n",
      "Accuracy:  0.776\n",
      "Loss: 0.9057967529296875\n",
      "Accuracy:  0.7795\n",
      "Loss: 0.9132295532226562\n",
      "Accuracy:  0.781\n",
      "Loss: 0.8482664794921875\n",
      "Accuracy:  0.7965\n",
      "Loss: 0.8251829833984375\n",
      "Accuracy:  0.798\n",
      "Loss: 0.8785432739257812\n",
      "Accuracy:  0.789\n",
      "Loss: 0.8826036376953125\n",
      "Accuracy:  0.787\n",
      "Loss: 0.8792105712890625\n",
      "Accuracy:  0.7785\n",
      "Loss: 0.9115025634765626\n",
      "Accuracy:  0.777\n",
      "Loss: 0.9720579223632813\n",
      "Accuracy:  0.7625\n",
      "Loss: 0.951802734375\n",
      "Accuracy:  0.759\n",
      "Loss: 0.9286279296875\n",
      "Accuracy:  0.777\n",
      "Loss: 0.9330266723632813\n",
      "Accuracy:  0.773\n",
      "Loss: 0.9414669189453125\n",
      "Accuracy:  0.7725\n",
      "Loss: 0.9479495849609375\n",
      "Accuracy:  0.775\n",
      "Loss: 0.8704942626953125\n",
      "Accuracy:  0.7905\n",
      "Loss: 0.968107421875\n",
      "Accuracy:  0.7695\n",
      "Loss: 0.9120689697265625\n",
      "Accuracy:  0.778\n",
      "Loss: 1.0292090886755834\n",
      "Accuracy:  0.7468354430379747\n",
      "65.9562416587499\n",
      "0.05311307938651128\n",
      "Train: wpb=0, bsz=1967, num_updates=1020\n",
      "Loss: 0.8401614990234375\n",
      "Accuracy:  0.8\n",
      "Loss: 0.9108146362304688\n",
      "Accuracy:  0.779\n",
      "Loss: 0.811197509765625\n",
      "Accuracy:  0.8095\n",
      "Loss: 0.8598994140625\n",
      "Accuracy:  0.7925\n",
      "Loss: 0.89330322265625\n",
      "Accuracy:  0.7875\n",
      "Loss: 0.8543610229492188\n",
      "Accuracy:  0.7945\n",
      "Loss: 0.8518771362304688\n",
      "Accuracy:  0.801\n",
      "Loss: 0.842455078125\n",
      "Accuracy:  0.7955\n",
      "Loss: 0.823304443359375\n",
      "Accuracy:  0.8\n",
      "Loss: 0.8846500244140625\n",
      "Accuracy:  0.7855\n",
      "Loss: 0.8738085327148437\n",
      "Accuracy:  0.787\n",
      "Loss: 0.8596947021484375\n",
      "Accuracy:  0.788\n",
      "Loss: 0.85831494140625\n",
      "Accuracy:  0.7845\n",
      "Loss: 0.83359130859375\n",
      "Accuracy:  0.799\n",
      "Loss: 0.8754134521484375\n",
      "Accuracy:  0.7835\n",
      "Loss: 0.8477844848632813\n",
      "Accuracy:  0.798\n",
      "Loss: 0.9139437255859375\n",
      "Accuracy:  0.7735\n",
      "Loss: 0.87699609375\n",
      "Accuracy:  0.7885\n",
      "Loss: 0.9176691284179688\n",
      "Accuracy:  0.777\n",
      "Loss: 0.8867008666992188\n",
      "Accuracy:  0.7835\n",
      "Loss: 0.8787247314453125\n",
      "Accuracy:  0.782\n",
      "Loss: 0.8511491088867188\n",
      "Accuracy:  0.788\n",
      "Loss: 0.8497003173828125\n",
      "Accuracy:  0.789\n",
      "Loss: 0.8890664672851563\n",
      "Accuracy:  0.777\n",
      "Loss: 0.8420090942382813\n",
      "Accuracy:  0.7985\n",
      "Loss: 0.84551806640625\n",
      "Accuracy:  0.7905\n",
      "Loss: 0.8588008422851563\n",
      "Accuracy:  0.793\n",
      "Loss: 0.895637939453125\n",
      "Accuracy:  0.779\n",
      "Loss: 0.839281005859375\n",
      "Accuracy:  0.7945\n",
      "Loss: 0.9612487182617188\n",
      "Accuracy:  0.7665\n",
      "Loss: 0.8874990844726562\n",
      "Accuracy:  0.7805\n",
      "Loss: 0.8935274047851562\n",
      "Accuracy:  0.7845\n",
      "Loss: 0.895326171875\n",
      "Accuracy:  0.7875\n",
      "Loss: 0.8713201904296874\n",
      "Accuracy:  0.7865\n",
      "Loss: 0.908486572265625\n",
      "Accuracy:  0.783\n",
      "Loss: 0.822989990234375\n",
      "Accuracy:  0.801\n",
      "Loss: 0.8032841186523437\n",
      "Accuracy:  0.8005\n",
      "Loss: 0.8882711181640625\n",
      "Accuracy:  0.7845\n",
      "Loss: 0.929989990234375\n",
      "Accuracy:  0.776\n",
      "Loss: 0.883944580078125\n",
      "Accuracy:  0.779\n",
      "Loss: 0.8512727661132813\n",
      "Accuracy:  0.792\n",
      "Loss: 0.9016232299804687\n",
      "Accuracy:  0.7815\n",
      "Loss: 0.8836116333007813\n",
      "Accuracy:  0.7845\n",
      "Loss: 0.8805905151367187\n",
      "Accuracy:  0.788\n",
      "Loss: 0.988764892578125\n",
      "Accuracy:  0.7615\n",
      "Loss: 0.9040838623046875\n",
      "Accuracy:  0.779\n",
      "Loss: 0.8775814819335938\n",
      "Accuracy:  0.7885\n",
      "Loss: 0.9003888549804687\n",
      "Accuracy:  0.775\n",
      "Loss: 0.9158648071289063\n",
      "Accuracy:  0.773\n",
      "Loss: 0.9068955688476562\n",
      "Accuracy:  0.7805\n",
      "Loss: 0.917896728515625\n",
      "Accuracy:  0.7745\n",
      "Loss: 0.88468505859375\n",
      "Accuracy:  0.784\n",
      "Loss: 0.8579935302734375\n",
      "Accuracy:  0.7865\n",
      "Loss: 0.8465443115234375\n",
      "Accuracy:  0.79\n",
      "Loss: 0.924541748046875\n",
      "Accuracy:  0.767\n",
      "Loss: 0.8939177856445313\n",
      "Accuracy:  0.781\n",
      "Loss: 0.8638618774414063\n",
      "Accuracy:  0.7895\n",
      "Loss: 0.876233642578125\n",
      "Accuracy:  0.784\n",
      "Loss: 0.8664215667338311\n",
      "Accuracy:  0.7848101265822784\n",
      "66.6573979190956\n",
      "0.048781275067565896\n",
      "Train: wpb=0, bsz=1967, num_updates=1080\n",
      "Loss: 0.8353872680664063\n",
      "Accuracy:  0.798\n",
      "Loss: 0.8527265625\n",
      "Accuracy:  0.798\n",
      "Loss: 0.8169088745117188\n",
      "Accuracy:  0.803\n",
      "Loss: 0.8517552490234375\n",
      "Accuracy:  0.795\n",
      "Loss: 0.8202105712890625\n",
      "Accuracy:  0.8015\n",
      "Loss: 0.8510714111328125\n",
      "Accuracy:  0.7955\n",
      "Loss: 0.8587909545898438\n",
      "Accuracy:  0.79\n",
      "Loss: 0.9131043701171875\n",
      "Accuracy:  0.7785\n",
      "Loss: 0.8860289916992188\n",
      "Accuracy:  0.788\n",
      "Loss: 0.7821859741210937\n",
      "Accuracy:  0.8125\n",
      "Loss: 0.8383329467773437\n",
      "Accuracy:  0.793\n",
      "Loss: 0.795892333984375\n",
      "Accuracy:  0.807\n",
      "Loss: 0.8073314208984375\n",
      "Accuracy:  0.802\n",
      "Loss: 0.902951171875\n",
      "Accuracy:  0.779\n",
      "Loss: 0.8317744140625\n",
      "Accuracy:  0.8\n",
      "Loss: 0.855784912109375\n",
      "Accuracy:  0.7885\n",
      "Loss: 0.8694054565429687\n",
      "Accuracy:  0.7815\n",
      "Loss: 0.8418898315429687\n",
      "Accuracy:  0.795\n",
      "Loss: 0.8497466430664062\n",
      "Accuracy:  0.7945\n",
      "Loss: 0.8813526000976563\n",
      "Accuracy:  0.783\n",
      "Loss: 0.8612137451171875\n",
      "Accuracy:  0.791\n",
      "Loss: 0.8650787963867187\n",
      "Accuracy:  0.7935\n",
      "Loss: 0.8933038330078125\n",
      "Accuracy:  0.7785\n",
      "Loss: 0.8061229858398438\n",
      "Accuracy:  0.802\n",
      "Loss: 0.8981312866210938\n",
      "Accuracy:  0.7825\n",
      "Loss: 0.8031762084960937\n",
      "Accuracy:  0.804\n",
      "Loss: 0.8667591552734375\n",
      "Accuracy:  0.7895\n",
      "Loss: 0.874534423828125\n",
      "Accuracy:  0.7855\n",
      "Loss: 0.8819830322265625\n",
      "Accuracy:  0.784\n",
      "Loss: 0.86497119140625\n",
      "Accuracy:  0.798\n",
      "Loss: 0.8226023559570312\n",
      "Accuracy:  0.799\n",
      "Loss: 0.8891754760742188\n",
      "Accuracy:  0.787\n",
      "Loss: 0.89530419921875\n",
      "Accuracy:  0.7745\n",
      "Loss: 0.8670180053710937\n",
      "Accuracy:  0.792\n",
      "Loss: 0.8270438842773438\n",
      "Accuracy:  0.7965\n",
      "Loss: 0.8507588500976563\n",
      "Accuracy:  0.7935\n",
      "Loss: 0.934333251953125\n",
      "Accuracy:  0.768\n",
      "Loss: 0.8255097045898437\n",
      "Accuracy:  0.7995\n",
      "Loss: 0.8928460083007812\n",
      "Accuracy:  0.784\n",
      "Loss: 0.8424193115234375\n",
      "Accuracy:  0.7895\n",
      "Loss: 0.8472093505859375\n",
      "Accuracy:  0.799\n",
      "Loss: 0.9022113647460938\n",
      "Accuracy:  0.7755\n",
      "Loss: 0.8412520751953125\n",
      "Accuracy:  0.79\n",
      "Loss: 0.867634521484375\n",
      "Accuracy:  0.7905\n",
      "Loss: 0.868107421875\n",
      "Accuracy:  0.7865\n",
      "Loss: 0.8978831787109375\n",
      "Accuracy:  0.783\n",
      "Loss: 0.8738556518554688\n",
      "Accuracy:  0.7885\n",
      "Loss: 0.9618045654296875\n",
      "Accuracy:  0.769\n",
      "Loss: 0.8827225341796875\n",
      "Accuracy:  0.783\n",
      "Loss: 0.8424962768554688\n",
      "Accuracy:  0.7945\n",
      "Loss: 0.9404300537109375\n",
      "Accuracy:  0.7705\n",
      "Loss: 0.8914359130859375\n",
      "Accuracy:  0.78\n",
      "Loss: 0.899326171875\n",
      "Accuracy:  0.7875\n",
      "Loss: 0.8415949096679688\n",
      "Accuracy:  0.7995\n",
      "Loss: 0.8244586791992188\n",
      "Accuracy:  0.8005\n",
      "Loss: 0.8629786987304687\n",
      "Accuracy:  0.791\n",
      "Loss: 0.8264404907226562\n",
      "Accuracy:  0.793\n",
      "Loss: 0.864807861328125\n",
      "Accuracy:  0.7935\n",
      "Loss: 0.8987270306937302\n",
      "Accuracy:  0.7848101265822784\n",
      "67.31028869610488\n",
      "0.04529715319717717\n",
      "Train: wpb=0, bsz=1967, num_updates=1140\n",
      "Loss: 0.8040952758789063\n",
      "Accuracy:  0.804\n",
      "Loss: 0.779523193359375\n",
      "Accuracy:  0.8135\n",
      "Loss: 0.8561552734375\n",
      "Accuracy:  0.792\n",
      "Loss: 0.7625925903320312\n",
      "Accuracy:  0.8145\n",
      "Loss: 0.782752685546875\n",
      "Accuracy:  0.814\n",
      "Loss: 0.7913533935546875\n",
      "Accuracy:  0.816\n",
      "Loss: 0.8217736206054688\n",
      "Accuracy:  0.796\n",
      "Loss: 0.8045109252929687\n",
      "Accuracy:  0.795\n",
      "Loss: 0.8711552734375\n",
      "Accuracy:  0.787\n",
      "Loss: 0.8476640625\n",
      "Accuracy:  0.7985\n",
      "Loss: 0.857059814453125\n",
      "Accuracy:  0.7975\n",
      "Loss: 0.912451904296875\n",
      "Accuracy:  0.78\n",
      "Loss: 0.8804512939453125\n",
      "Accuracy:  0.7915\n",
      "Loss: 0.77475830078125\n",
      "Accuracy:  0.811\n",
      "Loss: 0.875648681640625\n",
      "Accuracy:  0.789\n",
      "Loss: 0.828447021484375\n",
      "Accuracy:  0.7995\n",
      "Loss: 0.83523486328125\n",
      "Accuracy:  0.8025\n",
      "Loss: 0.8570429077148437\n",
      "Accuracy:  0.799\n",
      "Loss: 0.869603271484375\n",
      "Accuracy:  0.7865\n",
      "Loss: 0.7771669921875\n",
      "Accuracy:  0.8135\n",
      "Loss: 0.8265972290039062\n",
      "Accuracy:  0.809\n",
      "Loss: 0.8529150390625\n",
      "Accuracy:  0.7955\n",
      "Loss: 0.8649947509765625\n",
      "Accuracy:  0.786\n",
      "Loss: 0.8083615112304687\n",
      "Accuracy:  0.799\n",
      "Loss: 0.8615330810546875\n",
      "Accuracy:  0.788\n",
      "Loss: 0.856916015625\n",
      "Accuracy:  0.7925\n",
      "Loss: 0.8399888916015625\n",
      "Accuracy:  0.7945\n",
      "Loss: 0.8326353759765625\n",
      "Accuracy:  0.8025\n",
      "Loss: 0.919960693359375\n",
      "Accuracy:  0.7805\n",
      "Loss: 0.8809197387695312\n",
      "Accuracy:  0.784\n",
      "Loss: 0.8805507202148437\n",
      "Accuracy:  0.7825\n",
      "Loss: 0.7828142700195313\n",
      "Accuracy:  0.8115\n",
      "Loss: 0.7886819458007812\n",
      "Accuracy:  0.807\n",
      "Loss: 0.85296923828125\n",
      "Accuracy:  0.793\n",
      "Loss: 0.82092529296875\n",
      "Accuracy:  0.7985\n",
      "Loss: 0.782015869140625\n",
      "Accuracy:  0.812\n",
      "Loss: 0.7878582153320313\n",
      "Accuracy:  0.8045\n",
      "Loss: 0.89366162109375\n",
      "Accuracy:  0.782\n",
      "Loss: 0.8269456787109375\n",
      "Accuracy:  0.7945\n",
      "Loss: 0.809828369140625\n",
      "Accuracy:  0.802\n",
      "Loss: 0.8355911865234374\n",
      "Accuracy:  0.799\n",
      "Loss: 0.8560545043945312\n",
      "Accuracy:  0.7865\n",
      "Loss: 0.88175830078125\n",
      "Accuracy:  0.788\n",
      "Loss: 0.840994873046875\n",
      "Accuracy:  0.791\n",
      "Loss: 0.8923390502929688\n",
      "Accuracy:  0.782\n",
      "Loss: 0.7962459106445312\n",
      "Accuracy:  0.8095\n",
      "Loss: 0.8258134765625\n",
      "Accuracy:  0.793\n",
      "Loss: 0.8506165771484375\n",
      "Accuracy:  0.789\n",
      "Loss: 0.8192859497070313\n",
      "Accuracy:  0.7955\n",
      "Loss: 0.8417113037109375\n",
      "Accuracy:  0.7935\n",
      "Loss: 0.8781995239257813\n",
      "Accuracy:  0.784\n",
      "Loss: 0.8583911743164062\n",
      "Accuracy:  0.7895\n",
      "Loss: 0.8505654296875\n",
      "Accuracy:  0.7915\n",
      "Loss: 0.8573739624023438\n",
      "Accuracy:  0.7965\n",
      "Loss: 0.844281982421875\n",
      "Accuracy:  0.7975\n",
      "Loss: 0.8384346923828125\n",
      "Accuracy:  0.7935\n",
      "Loss: 0.8437171020507812\n",
      "Accuracy:  0.7955\n",
      "Loss: 0.8761428833007813\n",
      "Accuracy:  0.785\n",
      "Loss: 0.9909448744375494\n",
      "Accuracy:  0.7974683544303798\n",
      "67.92494855139356\n",
      "0.041985055345480846\n",
      "Train: wpb=0, bsz=1967, num_updates=1200\n",
      "Loss: 0.8490619506835938\n",
      "Accuracy:  0.7925\n",
      "Loss: 0.8182427978515625\n",
      "Accuracy:  0.8025\n",
      "Loss: 0.7940961303710937\n",
      "Accuracy:  0.806\n",
      "Loss: 0.8182303466796875\n",
      "Accuracy:  0.8075\n",
      "Loss: 0.7952291259765625\n",
      "Accuracy:  0.804\n",
      "Loss: 0.7578059692382813\n",
      "Accuracy:  0.8115\n",
      "Loss: 0.76949755859375\n",
      "Accuracy:  0.819\n",
      "Loss: 0.7815482788085938\n",
      "Accuracy:  0.8105\n",
      "Loss: 0.833964599609375\n",
      "Accuracy:  0.8005\n",
      "Loss: 0.8292009887695313\n",
      "Accuracy:  0.797\n",
      "Loss: 0.8312860107421876\n",
      "Accuracy:  0.7975\n",
      "Loss: 0.869782958984375\n",
      "Accuracy:  0.7915\n",
      "Loss: 0.8415703735351563\n",
      "Accuracy:  0.796\n",
      "Loss: 0.8538945922851563\n",
      "Accuracy:  0.786\n",
      "Loss: 0.8063883666992188\n",
      "Accuracy:  0.805\n",
      "Loss: 0.7355376586914063\n",
      "Accuracy:  0.8235\n",
      "Loss: 0.8723770141601562\n",
      "Accuracy:  0.7875\n",
      "Loss: 0.8189429321289062\n",
      "Accuracy:  0.803\n",
      "Loss: 0.84221923828125\n",
      "Accuracy:  0.795\n",
      "Loss: 0.8541554565429688\n",
      "Accuracy:  0.793\n",
      "Loss: 0.8303902587890625\n",
      "Accuracy:  0.796\n",
      "Loss: 0.8609031982421875\n",
      "Accuracy:  0.791\n",
      "Loss: 0.7441334838867187\n",
      "Accuracy:  0.8175\n",
      "Loss: 0.8347024536132812\n",
      "Accuracy:  0.7985\n",
      "Loss: 0.7274393920898438\n",
      "Accuracy:  0.8225\n",
      "Loss: 0.8462105102539063\n",
      "Accuracy:  0.7985\n",
      "Loss: 0.829596923828125\n",
      "Accuracy:  0.796\n",
      "Loss: 0.8540536499023438\n",
      "Accuracy:  0.792\n",
      "Loss: 0.7683995971679688\n",
      "Accuracy:  0.811\n",
      "Loss: 0.8266591796875\n",
      "Accuracy:  0.796\n",
      "Loss: 0.8791463623046875\n",
      "Accuracy:  0.784\n",
      "Loss: 0.7784546508789062\n",
      "Accuracy:  0.8085\n",
      "Loss: 0.8688424682617187\n",
      "Accuracy:  0.7795\n",
      "Loss: 0.8024989013671875\n",
      "Accuracy:  0.8025\n",
      "Loss: 0.7669751586914062\n",
      "Accuracy:  0.815\n",
      "Loss: 0.863076904296875\n",
      "Accuracy:  0.79\n",
      "Loss: 0.8521068725585937\n",
      "Accuracy:  0.7905\n",
      "Loss: 0.8482084350585938\n",
      "Accuracy:  0.7905\n",
      "Loss: 0.86723583984375\n",
      "Accuracy:  0.784\n",
      "Loss: 0.8646534423828125\n",
      "Accuracy:  0.792\n",
      "Loss: 0.85090625\n",
      "Accuracy:  0.797\n",
      "Loss: 0.7849818725585938\n",
      "Accuracy:  0.808\n",
      "Loss: 0.8772818603515625\n",
      "Accuracy:  0.783\n",
      "Loss: 0.8492152099609375\n",
      "Accuracy:  0.795\n",
      "Loss: 0.8551931762695313\n",
      "Accuracy:  0.7985\n",
      "Loss: 0.8369910888671875\n",
      "Accuracy:  0.792\n",
      "Loss: 0.8242947387695313\n",
      "Accuracy:  0.7995\n",
      "Loss: 0.8093563842773438\n",
      "Accuracy:  0.8075\n",
      "Loss: 0.84906396484375\n",
      "Accuracy:  0.7925\n",
      "Loss: 0.8447620239257813\n",
      "Accuracy:  0.7945\n",
      "Loss: 0.8132697143554688\n",
      "Accuracy:  0.8035\n",
      "Loss: 0.867459716796875\n",
      "Accuracy:  0.7835\n",
      "Loss: 0.8329505004882812\n",
      "Accuracy:  0.7995\n",
      "Loss: 0.8120300903320312\n",
      "Accuracy:  0.8\n",
      "Loss: 0.846195556640625\n",
      "Accuracy:  0.795\n",
      "Loss: 0.8762803344726563\n",
      "Accuracy:  0.7855\n",
      "Loss: 0.8384473266601562\n",
      "Accuracy:  0.797\n",
      "Loss: 0.8116251831054687\n",
      "Accuracy:  0.7995\n",
      "Loss: 0.9398798882206784\n",
      "Accuracy:  0.7974683544303798\n",
      "68.4934501074543\n",
      "0.039382854044717475\n",
      "Train: wpb=0, bsz=1967, num_updates=1260\n",
      "Loss: 0.7614281005859375\n",
      "Accuracy:  0.8175\n",
      "Loss: 0.853579345703125\n",
      "Accuracy:  0.793\n",
      "Loss: 0.7872240600585938\n",
      "Accuracy:  0.8055\n",
      "Loss: 0.8025498657226563\n",
      "Accuracy:  0.812\n",
      "Loss: 0.8260498046875\n",
      "Accuracy:  0.7965\n",
      "Loss: 0.776061767578125\n",
      "Accuracy:  0.807\n",
      "Loss: 0.7520467529296875\n",
      "Accuracy:  0.813\n",
      "Loss: 0.8329888916015625\n",
      "Accuracy:  0.7925\n",
      "Loss: 0.8526611938476563\n",
      "Accuracy:  0.7925\n",
      "Loss: 0.8288336181640625\n",
      "Accuracy:  0.8005\n",
      "Loss: 0.7760020751953125\n",
      "Accuracy:  0.81\n",
      "Loss: 0.8087399291992188\n",
      "Accuracy:  0.8065\n",
      "Loss: 0.7376055297851563\n",
      "Accuracy:  0.8145\n",
      "Loss: 0.8023583984375\n",
      "Accuracy:  0.802\n",
      "Loss: 0.895697265625\n",
      "Accuracy:  0.783\n",
      "Loss: 0.8203535766601563\n",
      "Accuracy:  0.794\n",
      "Loss: 0.8200975341796874\n",
      "Accuracy:  0.798\n",
      "Loss: 0.765625244140625\n",
      "Accuracy:  0.818\n",
      "Loss: 0.8326991577148437\n",
      "Accuracy:  0.794\n",
      "Loss: 0.7718712768554687\n",
      "Accuracy:  0.811\n",
      "Loss: 0.8008800048828125\n",
      "Accuracy:  0.8025\n",
      "Loss: 0.797966796875\n",
      "Accuracy:  0.809\n",
      "Loss: 0.816763427734375\n",
      "Accuracy:  0.8035\n",
      "Loss: 0.8294161987304688\n",
      "Accuracy:  0.799\n",
      "Loss: 0.7791931762695312\n",
      "Accuracy:  0.809\n",
      "Loss: 0.8145072631835938\n",
      "Accuracy:  0.801\n",
      "Loss: 0.7930822143554688\n",
      "Accuracy:  0.807\n",
      "Loss: 0.8672877807617188\n",
      "Accuracy:  0.797\n",
      "Loss: 0.8497039794921875\n",
      "Accuracy:  0.7885\n",
      "Loss: 0.8484127197265625\n",
      "Accuracy:  0.7945\n",
      "Loss: 0.9000287475585937\n",
      "Accuracy:  0.784\n",
      "Loss: 0.8314696655273438\n",
      "Accuracy:  0.7975\n",
      "Loss: 0.7839976806640625\n",
      "Accuracy:  0.807\n",
      "Loss: 0.8387913818359375\n",
      "Accuracy:  0.794\n",
      "Loss: 0.8763496704101562\n",
      "Accuracy:  0.7825\n",
      "Loss: 0.8032625732421875\n",
      "Accuracy:  0.807\n",
      "Loss: 0.7623187255859375\n",
      "Accuracy:  0.809\n",
      "Loss: 0.8250877075195312\n",
      "Accuracy:  0.8075\n",
      "Loss: 0.8285220947265625\n",
      "Accuracy:  0.805\n",
      "Loss: 0.8105318603515625\n",
      "Accuracy:  0.804\n",
      "Loss: 0.79256982421875\n",
      "Accuracy:  0.811\n",
      "Loss: 0.8055978393554688\n",
      "Accuracy:  0.8065\n",
      "Loss: 0.7629480590820312\n",
      "Accuracy:  0.815\n",
      "Loss: 0.814774658203125\n",
      "Accuracy:  0.8015\n",
      "Loss: 0.8101804809570312\n",
      "Accuracy:  0.799\n",
      "Loss: 0.7791803588867188\n",
      "Accuracy:  0.81\n",
      "Loss: 0.8322518310546875\n",
      "Accuracy:  0.7955\n",
      "Loss: 0.8003653564453125\n",
      "Accuracy:  0.805\n",
      "Loss: 0.8626353149414062\n",
      "Accuracy:  0.79\n",
      "Loss: 0.8691632690429687\n",
      "Accuracy:  0.786\n",
      "Loss: 0.74819384765625\n",
      "Accuracy:  0.819\n",
      "Loss: 0.7940067749023437\n",
      "Accuracy:  0.807\n",
      "Loss: 0.8204755859375\n",
      "Accuracy:  0.7965\n",
      "Loss: 0.8412592163085938\n",
      "Accuracy:  0.788\n",
      "Loss: 0.8323055419921875\n",
      "Accuracy:  0.803\n",
      "Loss: 0.7914405517578125\n",
      "Accuracy:  0.8085\n",
      "Loss: 0.8147744140625\n",
      "Accuracy:  0.798\n",
      "Loss: 0.7843220825195313\n",
      "Accuracy:  0.81\n",
      "Loss: 1.1693629977069324\n",
      "Accuracy:  0.6962025316455697\n",
      "69.02578320061531\n",
      "0.03693122524638241\n",
      "Train: wpb=0, bsz=1967, num_updates=1320\n",
      "Loss: 0.7961588745117187\n",
      "Accuracy:  0.8145\n",
      "Loss: 0.8071126708984375\n",
      "Accuracy:  0.8045\n",
      "Loss: 0.8444498291015625\n",
      "Accuracy:  0.795\n",
      "Loss: 0.744860107421875\n",
      "Accuracy:  0.813\n",
      "Loss: 0.7343347778320313\n",
      "Accuracy:  0.827\n",
      "Loss: 0.7841748657226563\n",
      "Accuracy:  0.8135\n",
      "Loss: 0.789583984375\n",
      "Accuracy:  0.811\n",
      "Loss: 0.7813026123046874\n",
      "Accuracy:  0.812\n",
      "Loss: 0.7848540649414063\n",
      "Accuracy:  0.803\n",
      "Loss: 0.8093048706054687\n",
      "Accuracy:  0.801\n",
      "Loss: 0.8325064697265625\n",
      "Accuracy:  0.796\n",
      "Loss: 0.8244060668945312\n",
      "Accuracy:  0.8015\n",
      "Loss: 0.8257095947265625\n",
      "Accuracy:  0.798\n",
      "Loss: 0.7813457641601562\n",
      "Accuracy:  0.81\n",
      "Loss: 0.7947020263671875\n",
      "Accuracy:  0.8095\n",
      "Loss: 0.84796337890625\n",
      "Accuracy:  0.7965\n",
      "Loss: 0.8310165405273438\n",
      "Accuracy:  0.8\n",
      "Loss: 0.8195847778320312\n",
      "Accuracy:  0.801\n",
      "Loss: 0.7326500244140625\n",
      "Accuracy:  0.8235\n",
      "Loss: 0.8055759887695313\n",
      "Accuracy:  0.807\n",
      "Loss: 0.790290283203125\n",
      "Accuracy:  0.8065\n",
      "Loss: 0.7473615112304688\n",
      "Accuracy:  0.8165\n",
      "Loss: 0.824630126953125\n",
      "Accuracy:  0.8015\n",
      "Loss: 0.8258143920898438\n",
      "Accuracy:  0.801\n",
      "Loss: 0.86647607421875\n",
      "Accuracy:  0.7865\n",
      "Loss: 0.8009220581054688\n",
      "Accuracy:  0.804\n",
      "Loss: 0.757998291015625\n",
      "Accuracy:  0.8105\n",
      "Loss: 0.794370849609375\n",
      "Accuracy:  0.807\n",
      "Loss: 0.8276758422851562\n",
      "Accuracy:  0.79\n",
      "Loss: 0.7875106811523438\n",
      "Accuracy:  0.8085\n",
      "Loss: 0.7648556518554688\n",
      "Accuracy:  0.8235\n",
      "Loss: 0.7498242797851562\n",
      "Accuracy:  0.818\n",
      "Loss: 0.800971923828125\n",
      "Accuracy:  0.799\n",
      "Loss: 0.7717908935546876\n",
      "Accuracy:  0.8095\n",
      "Loss: 0.8708726806640625\n",
      "Accuracy:  0.7795\n",
      "Loss: 0.8808004760742187\n",
      "Accuracy:  0.78\n",
      "Loss: 0.7956546020507812\n",
      "Accuracy:  0.8145\n",
      "Loss: 0.7784259033203125\n",
      "Accuracy:  0.806\n",
      "Loss: 0.7837980346679687\n",
      "Accuracy:  0.803\n",
      "Loss: 0.8535294799804688\n",
      "Accuracy:  0.7905\n",
      "Loss: 0.7863642578125\n",
      "Accuracy:  0.8125\n",
      "Loss: 0.7946255493164063\n",
      "Accuracy:  0.8135\n",
      "Loss: 0.8332444458007813\n",
      "Accuracy:  0.8015\n",
      "Loss: 0.8334660034179687\n",
      "Accuracy:  0.7985\n",
      "Loss: 0.794581298828125\n",
      "Accuracy:  0.804\n",
      "Loss: 0.8304560546875\n",
      "Accuracy:  0.801\n",
      "Loss: 0.839795166015625\n",
      "Accuracy:  0.7965\n",
      "Loss: 0.8128270263671875\n",
      "Accuracy:  0.7925\n",
      "Loss: 0.7964871215820313\n",
      "Accuracy:  0.808\n",
      "Loss: 0.8172723388671875\n",
      "Accuracy:  0.801\n",
      "Loss: 0.7819070434570312\n",
      "Accuracy:  0.811\n",
      "Loss: 0.7988792114257812\n",
      "Accuracy:  0.805\n",
      "Loss: 0.7533572998046875\n",
      "Accuracy:  0.8085\n",
      "Loss: 0.79473876953125\n",
      "Accuracy:  0.8035\n",
      "Loss: 0.7809952392578124\n",
      "Accuracy:  0.8125\n",
      "Loss: 0.773520751953125\n",
      "Accuracy:  0.8155\n",
      "Loss: 0.800188232421875\n",
      "Accuracy:  0.804\n",
      "Loss: 0.7910859985351563\n",
      "Accuracy:  0.81\n",
      "Loss: 0.7364725040484078\n",
      "Accuracy:  0.7974683544303798\n",
      "69.5250084965224\n",
      "0.034830327136545054\n",
      "Train: wpb=0, bsz=1967, num_updates=1380\n",
      "Loss: 0.7756734619140625\n",
      "Accuracy:  0.822\n",
      "Loss: 0.8497921752929688\n",
      "Accuracy:  0.789\n",
      "Loss: 0.782613037109375\n",
      "Accuracy:  0.8135\n",
      "Loss: 0.8208822631835937\n",
      "Accuracy:  0.807\n",
      "Loss: 0.8199943237304688\n",
      "Accuracy:  0.8065\n",
      "Loss: 0.7565338745117187\n",
      "Accuracy:  0.8205\n",
      "Loss: 0.8434210815429688\n",
      "Accuracy:  0.8005\n",
      "Loss: 0.8408046264648438\n",
      "Accuracy:  0.7985\n",
      "Loss: 0.7466198120117188\n",
      "Accuracy:  0.8185\n",
      "Loss: 0.762028564453125\n",
      "Accuracy:  0.817\n",
      "Loss: 0.7532293090820312\n",
      "Accuracy:  0.822\n",
      "Loss: 0.7504651489257812\n",
      "Accuracy:  0.8155\n",
      "Loss: 0.7697137451171875\n",
      "Accuracy:  0.8195\n",
      "Loss: 0.7875205688476562\n",
      "Accuracy:  0.809\n",
      "Loss: 0.7190076904296875\n",
      "Accuracy:  0.826\n",
      "Loss: 0.8007373046875\n",
      "Accuracy:  0.805\n",
      "Loss: 0.7412864990234375\n",
      "Accuracy:  0.824\n",
      "Loss: 0.7528088989257813\n",
      "Accuracy:  0.8195\n",
      "Loss: 0.7167603149414062\n",
      "Accuracy:  0.8275\n",
      "Loss: 0.83509326171875\n",
      "Accuracy:  0.792\n",
      "Loss: 0.8152612915039062\n",
      "Accuracy:  0.8025\n",
      "Loss: 0.772628173828125\n",
      "Accuracy:  0.8085\n",
      "Loss: 0.791834716796875\n",
      "Accuracy:  0.805\n",
      "Loss: 0.7981233520507812\n",
      "Accuracy:  0.8015\n",
      "Loss: 0.7370601806640625\n",
      "Accuracy:  0.8215\n",
      "Loss: 0.84382373046875\n",
      "Accuracy:  0.7975\n",
      "Loss: 0.761031005859375\n",
      "Accuracy:  0.8085\n",
      "Loss: 0.793762451171875\n",
      "Accuracy:  0.807\n",
      "Loss: 0.8474649047851562\n",
      "Accuracy:  0.7955\n",
      "Loss: 0.8147200927734375\n",
      "Accuracy:  0.8005\n",
      "Loss: 0.8543271484375\n",
      "Accuracy:  0.788\n",
      "Loss: 0.811642822265625\n",
      "Accuracy:  0.8065\n",
      "Loss: 0.7722081909179688\n",
      "Accuracy:  0.8155\n",
      "Loss: 0.8230516357421875\n",
      "Accuracy:  0.8075\n",
      "Loss: 0.7829360961914062\n",
      "Accuracy:  0.8155\n",
      "Loss: 0.7193919067382812\n",
      "Accuracy:  0.824\n",
      "Loss: 0.789169189453125\n",
      "Accuracy:  0.8115\n",
      "Loss: 0.8071367797851563\n",
      "Accuracy:  0.805\n",
      "Loss: 0.7922206420898438\n",
      "Accuracy:  0.802\n",
      "Loss: 0.7529740600585938\n",
      "Accuracy:  0.8175\n",
      "Loss: 0.7806199951171875\n",
      "Accuracy:  0.8105\n",
      "Loss: 0.7928289184570313\n",
      "Accuracy:  0.8075\n",
      "Loss: 0.764369384765625\n",
      "Accuracy:  0.8135\n",
      "Loss: 0.8140202026367187\n",
      "Accuracy:  0.7955\n",
      "Loss: 0.7010037841796875\n",
      "Accuracy:  0.831\n",
      "Loss: 0.7403370361328125\n",
      "Accuracy:  0.8235\n",
      "Loss: 0.8585252685546875\n",
      "Accuracy:  0.79\n",
      "Loss: 0.7650221557617187\n",
      "Accuracy:  0.813\n",
      "Loss: 0.7809653930664062\n",
      "Accuracy:  0.802\n",
      "Loss: 0.7934317626953125\n",
      "Accuracy:  0.805\n",
      "Loss: 0.8288132934570313\n",
      "Accuracy:  0.7995\n",
      "Loss: 0.7488282470703125\n",
      "Accuracy:  0.823\n",
      "Loss: 0.83646630859375\n",
      "Accuracy:  0.797\n",
      "Loss: 0.7735714721679687\n",
      "Accuracy:  0.8105\n",
      "Loss: 0.797101806640625\n",
      "Accuracy:  0.8015\n",
      "Loss: 0.7568895263671875\n",
      "Accuracy:  0.8205\n",
      "Loss: 0.8172659301757812\n",
      "Accuracy:  0.8005\n",
      "Loss: 0.8572401733398437\n",
      "Accuracy:  0.7845\n",
      "Loss: 0.6465259503714645\n",
      "Accuracy:  0.8607594936708861\n",
      "69.9992519132671\n",
      "0.03284767651201581\n",
      "Train: wpb=0, bsz=1967, num_updates=1440\n",
      "Loss: 0.7691739501953125\n",
      "Accuracy:  0.814\n",
      "Loss: 0.6831998901367188\n",
      "Accuracy:  0.8355\n",
      "Loss: 0.7274755249023438\n",
      "Accuracy:  0.823\n",
      "Loss: 0.778744873046875\n",
      "Accuracy:  0.814\n",
      "Loss: 0.7941527099609375\n",
      "Accuracy:  0.8075\n",
      "Loss: 0.7132322998046875\n",
      "Accuracy:  0.826\n",
      "Loss: 0.8318810424804688\n",
      "Accuracy:  0.8005\n",
      "Loss: 0.7791624755859375\n",
      "Accuracy:  0.8145\n",
      "Loss: 0.7282210083007813\n",
      "Accuracy:  0.828\n",
      "Loss: 0.7690859375\n",
      "Accuracy:  0.8135\n",
      "Loss: 0.778449951171875\n",
      "Accuracy:  0.814\n",
      "Loss: 0.756106689453125\n",
      "Accuracy:  0.817\n",
      "Loss: 0.8042044677734375\n",
      "Accuracy:  0.812\n",
      "Loss: 0.8261527709960937\n",
      "Accuracy:  0.7995\n",
      "Loss: 0.7171815795898437\n",
      "Accuracy:  0.825\n",
      "Loss: 0.8082836303710937\n",
      "Accuracy:  0.8065\n",
      "Loss: 0.74602197265625\n",
      "Accuracy:  0.8205\n",
      "Loss: 0.7580769653320313\n",
      "Accuracy:  0.8195\n",
      "Loss: 0.7588610229492188\n",
      "Accuracy:  0.819\n",
      "Loss: 0.8104127197265625\n",
      "Accuracy:  0.7985\n",
      "Loss: 0.720634033203125\n",
      "Accuracy:  0.829\n",
      "Loss: 0.7950313720703125\n",
      "Accuracy:  0.8095\n",
      "Loss: 0.7841832885742187\n",
      "Accuracy:  0.811\n",
      "Loss: 0.773441650390625\n",
      "Accuracy:  0.815\n",
      "Loss: 0.7418077392578125\n",
      "Accuracy:  0.815\n",
      "Loss: 0.8140830688476562\n",
      "Accuracy:  0.8005\n",
      "Loss: 0.8017305908203125\n",
      "Accuracy:  0.807\n",
      "Loss: 0.814958251953125\n",
      "Accuracy:  0.797\n",
      "Loss: 0.7729946899414063\n",
      "Accuracy:  0.814\n",
      "Loss: 0.7990814819335937\n",
      "Accuracy:  0.806\n",
      "Loss: 0.8042767333984375\n",
      "Accuracy:  0.8065\n",
      "Loss: 0.7771325073242188\n",
      "Accuracy:  0.814\n",
      "Loss: 0.8168717651367188\n",
      "Accuracy:  0.801\n",
      "Loss: 0.8430517578125\n",
      "Accuracy:  0.7925\n",
      "Loss: 0.8063900146484375\n",
      "Accuracy:  0.8055\n",
      "Loss: 0.7455050048828125\n",
      "Accuracy:  0.8195\n",
      "Loss: 0.7142078857421875\n",
      "Accuracy:  0.827\n",
      "Loss: 0.784335205078125\n",
      "Accuracy:  0.809\n",
      "Loss: 0.7786738891601562\n",
      "Accuracy:  0.8085\n",
      "Loss: 0.7942298583984375\n",
      "Accuracy:  0.8115\n",
      "Loss: 0.7692604370117188\n",
      "Accuracy:  0.8125\n",
      "Loss: 0.7910684814453125\n",
      "Accuracy:  0.8105\n",
      "Loss: 0.8480964965820312\n",
      "Accuracy:  0.786\n",
      "Loss: 0.7832984008789062\n",
      "Accuracy:  0.8065\n",
      "Loss: 0.7780811157226563\n",
      "Accuracy:  0.81\n",
      "Loss: 0.780923095703125\n",
      "Accuracy:  0.807\n",
      "Loss: 0.7784857177734374\n",
      "Accuracy:  0.809\n",
      "Loss: 0.8035988159179688\n",
      "Accuracy:  0.8035\n",
      "Loss: 0.7940990600585938\n",
      "Accuracy:  0.812\n",
      "Loss: 0.7604052124023437\n",
      "Accuracy:  0.811\n",
      "Loss: 0.780597412109375\n",
      "Accuracy:  0.8125\n",
      "Loss: 0.8073704223632813\n",
      "Accuracy:  0.8075\n",
      "Loss: 0.77559814453125\n",
      "Accuracy:  0.814\n",
      "Loss: 0.8088229370117187\n",
      "Accuracy:  0.805\n",
      "Loss: 0.79851513671875\n",
      "Accuracy:  0.8125\n",
      "Loss: 0.7514712524414062\n",
      "Accuracy:  0.813\n",
      "Loss: 0.7626115112304688\n",
      "Accuracy:  0.8155\n",
      "Loss: 0.79790185546875\n",
      "Accuracy:  0.802\n",
      "Loss: 0.9082933256897745\n",
      "Accuracy:  0.7721518987341772\n",
      "70.44429576808747\n",
      "0.031183502769761052\n",
      "Train: wpb=0, bsz=1967, num_updates=1500\n",
      "Loss: 0.7691026611328124\n",
      "Accuracy:  0.813\n",
      "Loss: 0.7637167358398438\n",
      "Accuracy:  0.814\n",
      "Loss: 0.7079093627929688\n",
      "Accuracy:  0.831\n",
      "Loss: 0.7879429931640625\n",
      "Accuracy:  0.811\n",
      "Loss: 0.7740560302734375\n",
      "Accuracy:  0.8175\n",
      "Loss: 0.7412435302734375\n",
      "Accuracy:  0.8235\n",
      "Loss: 0.7054712524414063\n",
      "Accuracy:  0.824\n",
      "Loss: 0.7208102416992187\n",
      "Accuracy:  0.8265\n",
      "Loss: 0.7555380249023438\n",
      "Accuracy:  0.8165\n",
      "Loss: 0.7439393310546875\n",
      "Accuracy:  0.8185\n",
      "Loss: 0.8197119140625\n",
      "Accuracy:  0.8015\n",
      "Loss: 0.7237853393554687\n",
      "Accuracy:  0.8235\n",
      "Loss: 0.7579306030273437\n",
      "Accuracy:  0.818\n",
      "Loss: 0.7393028564453125\n",
      "Accuracy:  0.824\n",
      "Loss: 0.7551851806640625\n",
      "Accuracy:  0.8205\n",
      "Loss: 0.7697254028320313\n",
      "Accuracy:  0.817\n",
      "Loss: 0.7788839111328125\n",
      "Accuracy:  0.812\n",
      "Loss: 0.8000601806640625\n",
      "Accuracy:  0.805\n",
      "Loss: 0.7489340209960937\n",
      "Accuracy:  0.821\n",
      "Loss: 0.7599462280273438\n",
      "Accuracy:  0.817\n",
      "Loss: 0.7920325927734375\n",
      "Accuracy:  0.808\n",
      "Loss: 0.7887157592773437\n",
      "Accuracy:  0.8085\n",
      "Loss: 0.7550714111328125\n",
      "Accuracy:  0.8095\n",
      "Loss: 0.7519526977539063\n",
      "Accuracy:  0.815\n",
      "Loss: 0.797385498046875\n",
      "Accuracy:  0.805\n",
      "Loss: 0.75135107421875\n",
      "Accuracy:  0.819\n",
      "Loss: 0.7580357055664062\n",
      "Accuracy:  0.8165\n",
      "Loss: 0.7442230834960938\n",
      "Accuracy:  0.82\n",
      "Loss: 0.779396728515625\n",
      "Accuracy:  0.815\n",
      "Loss: 0.71578759765625\n",
      "Accuracy:  0.8325\n",
      "Loss: 0.7300003662109374\n",
      "Accuracy:  0.822\n",
      "Loss: 0.8384461059570313\n",
      "Accuracy:  0.7945\n",
      "Loss: 0.8103255615234375\n",
      "Accuracy:  0.8065\n",
      "Loss: 0.7466942749023437\n",
      "Accuracy:  0.816\n",
      "Loss: 0.7957144165039063\n",
      "Accuracy:  0.8055\n",
      "Loss: 0.7717017211914062\n",
      "Accuracy:  0.809\n",
      "Loss: 0.8355885009765625\n",
      "Accuracy:  0.7975\n",
      "Loss: 0.7186005249023437\n",
      "Accuracy:  0.818\n",
      "Loss: 0.7966376953125\n",
      "Accuracy:  0.8115\n",
      "Loss: 0.8328125610351562\n",
      "Accuracy:  0.795\n",
      "Loss: 0.7782901611328125\n",
      "Accuracy:  0.8155\n",
      "Loss: 0.7738788452148437\n",
      "Accuracy:  0.8135\n",
      "Loss: 0.78667919921875\n",
      "Accuracy:  0.798\n",
      "Loss: 0.8252307739257813\n",
      "Accuracy:  0.8025\n",
      "Loss: 0.7680155029296875\n",
      "Accuracy:  0.8135\n",
      "Loss: 0.7154229736328125\n",
      "Accuracy:  0.8235\n",
      "Loss: 0.8013397216796875\n",
      "Accuracy:  0.8035\n",
      "Loss: 0.7631469116210937\n",
      "Accuracy:  0.8185\n",
      "Loss: 0.7628567504882813\n",
      "Accuracy:  0.811\n",
      "Loss: 0.755369873046875\n",
      "Accuracy:  0.8195\n",
      "Loss: 0.7567872924804687\n",
      "Accuracy:  0.814\n",
      "Loss: 0.8111456298828125\n",
      "Accuracy:  0.7985\n",
      "Loss: 0.794125244140625\n",
      "Accuracy:  0.8035\n",
      "Loss: 0.7535181274414062\n",
      "Accuracy:  0.814\n",
      "Loss: 0.7702330932617187\n",
      "Accuracy:  0.809\n",
      "Loss: 0.7951741333007812\n",
      "Accuracy:  0.811\n",
      "Loss: 0.7830182495117187\n",
      "Accuracy:  0.8075\n",
      "Loss: 0.753608154296875\n",
      "Accuracy:  0.817\n",
      "Loss: 0.7266119462025317\n",
      "Accuracy:  0.8227848101265823\n",
      "70.86311836860199\n",
      "0.029554361033552756\n",
      "Train: wpb=0, bsz=1967, num_updates=1560\n",
      "Loss: 0.6932860107421875\n",
      "Accuracy:  0.8355\n",
      "Loss: 0.731960205078125\n",
      "Accuracy:  0.823\n",
      "Loss: 0.7340411987304688\n",
      "Accuracy:  0.8265\n",
      "Loss: 0.7322987670898438\n",
      "Accuracy:  0.825\n",
      "Loss: 0.701876953125\n",
      "Accuracy:  0.8325\n",
      "Loss: 0.7765701293945313\n",
      "Accuracy:  0.815\n",
      "Loss: 0.7535502319335937\n",
      "Accuracy:  0.826\n",
      "Loss: 0.745090087890625\n",
      "Accuracy:  0.8205\n",
      "Loss: 0.7054339599609375\n",
      "Accuracy:  0.829\n",
      "Loss: 0.8214505004882813\n",
      "Accuracy:  0.8025\n",
      "Loss: 0.7775068969726563\n",
      "Accuracy:  0.813\n",
      "Loss: 0.814636474609375\n",
      "Accuracy:  0.8025\n",
      "Loss: 0.7432387084960937\n",
      "Accuracy:  0.8125\n",
      "Loss: 0.729344970703125\n",
      "Accuracy:  0.822\n",
      "Loss: 0.7514445190429687\n",
      "Accuracy:  0.818\n",
      "Loss: 0.7364323120117188\n",
      "Accuracy:  0.8225\n",
      "Loss: 0.7300214233398438\n",
      "Accuracy:  0.816\n",
      "Loss: 0.7624630737304687\n",
      "Accuracy:  0.814\n",
      "Loss: 0.7890181274414062\n",
      "Accuracy:  0.808\n",
      "Loss: 0.7775069580078126\n",
      "Accuracy:  0.811\n",
      "Loss: 0.7824382934570312\n",
      "Accuracy:  0.809\n",
      "Loss: 0.7775286865234375\n",
      "Accuracy:  0.8065\n",
      "Loss: 0.7289334716796875\n",
      "Accuracy:  0.823\n",
      "Loss: 0.7864302368164062\n",
      "Accuracy:  0.813\n",
      "Loss: 0.8194644775390625\n",
      "Accuracy:  0.796\n",
      "Loss: 0.7260923461914063\n",
      "Accuracy:  0.8255\n",
      "Loss: 0.7848167724609375\n",
      "Accuracy:  0.8055\n",
      "Loss: 0.794395751953125\n",
      "Accuracy:  0.8095\n",
      "Loss: 0.7489457397460938\n",
      "Accuracy:  0.8225\n",
      "Loss: 0.71204296875\n",
      "Accuracy:  0.8235\n",
      "Loss: 0.7282509155273438\n",
      "Accuracy:  0.822\n",
      "Loss: 0.8025568237304688\n",
      "Accuracy:  0.806\n",
      "Loss: 0.770252197265625\n",
      "Accuracy:  0.808\n",
      "Loss: 0.7852330322265625\n",
      "Accuracy:  0.8095\n",
      "Loss: 0.7299524536132812\n",
      "Accuracy:  0.8265\n",
      "Loss: 0.7859623413085938\n",
      "Accuracy:  0.8185\n",
      "Loss: 0.7709728393554688\n",
      "Accuracy:  0.814\n",
      "Loss: 0.7669859619140625\n",
      "Accuracy:  0.811\n",
      "Loss: 0.8228907470703125\n",
      "Accuracy:  0.8055\n",
      "Loss: 0.773720458984375\n",
      "Accuracy:  0.81\n",
      "Loss: 0.7461397705078125\n",
      "Accuracy:  0.817\n",
      "Loss: 0.7089259643554687\n",
      "Accuracy:  0.8245\n",
      "Loss: 0.7553076782226562\n",
      "Accuracy:  0.8105\n",
      "Loss: 0.7795865478515625\n",
      "Accuracy:  0.81\n",
      "Loss: 0.7229439697265625\n",
      "Accuracy:  0.822\n",
      "Loss: 0.7848512573242188\n",
      "Accuracy:  0.8115\n",
      "Loss: 0.8103311157226563\n",
      "Accuracy:  0.803\n",
      "Loss: 0.7568693237304688\n",
      "Accuracy:  0.8085\n",
      "Loss: 0.7601538696289063\n",
      "Accuracy:  0.8235\n",
      "Loss: 0.7902913208007812\n",
      "Accuracy:  0.8125\n",
      "Loss: 0.7860509033203125\n",
      "Accuracy:  0.81\n",
      "Loss: 0.775610107421875\n",
      "Accuracy:  0.81\n",
      "Loss: 0.7635280151367188\n",
      "Accuracy:  0.8155\n",
      "Loss: 0.7559656982421875\n",
      "Accuracy:  0.814\n",
      "Loss: 0.7656002807617187\n",
      "Accuracy:  0.817\n",
      "Loss: 0.7382011108398437\n",
      "Accuracy:  0.8145\n",
      "Loss: 0.75114111328125\n",
      "Accuracy:  0.8205\n",
      "Loss: 0.7848787231445312\n",
      "Accuracy:  0.813\n",
      "Loss: 0.6575879929940912\n",
      "Accuracy:  0.8227848101265823\n",
      "71.26010740455307\n",
      "0.02814539989396412\n",
      "Train: wpb=0, bsz=1967, num_updates=1620\n",
      "Loss: 0.7789969482421875\n",
      "Accuracy:  0.816\n",
      "Loss: 0.720046142578125\n",
      "Accuracy:  0.8245\n",
      "Loss: 0.7568123779296875\n",
      "Accuracy:  0.822\n",
      "Loss: 0.6957464599609375\n",
      "Accuracy:  0.826\n",
      "Loss: 0.7567210083007813\n",
      "Accuracy:  0.8215\n",
      "Loss: 0.766694580078125\n",
      "Accuracy:  0.8145\n",
      "Loss: 0.778602783203125\n",
      "Accuracy:  0.8105\n",
      "Loss: 0.7469036254882813\n",
      "Accuracy:  0.812\n",
      "Loss: 0.6766878051757812\n",
      "Accuracy:  0.8285\n",
      "Loss: 0.7984939575195312\n",
      "Accuracy:  0.8055\n",
      "Loss: 0.7438961181640625\n",
      "Accuracy:  0.8255\n",
      "Loss: 0.7724935302734375\n",
      "Accuracy:  0.817\n",
      "Loss: 0.735517333984375\n",
      "Accuracy:  0.82\n",
      "Loss: 0.7332037963867187\n",
      "Accuracy:  0.825\n",
      "Loss: 0.7103221435546875\n",
      "Accuracy:  0.83\n",
      "Loss: 0.725107177734375\n",
      "Accuracy:  0.8245\n",
      "Loss: 0.7320827026367187\n",
      "Accuracy:  0.816\n",
      "Loss: 0.75447900390625\n",
      "Accuracy:  0.8215\n",
      "Loss: 0.7522587890625\n",
      "Accuracy:  0.8175\n",
      "Loss: 0.75701513671875\n",
      "Accuracy:  0.8185\n",
      "Loss: 0.7739479370117187\n",
      "Accuracy:  0.8155\n",
      "Loss: 0.7669701538085938\n",
      "Accuracy:  0.8085\n",
      "Loss: 0.747443603515625\n",
      "Accuracy:  0.83\n",
      "Loss: 0.7159437866210937\n",
      "Accuracy:  0.8285\n",
      "Loss: 0.747787353515625\n",
      "Accuracy:  0.817\n",
      "Loss: 0.7281828002929688\n",
      "Accuracy:  0.8245\n",
      "Loss: 0.7497523803710937\n",
      "Accuracy:  0.817\n",
      "Loss: 0.7561851806640625\n",
      "Accuracy:  0.8165\n",
      "Loss: 0.7867393798828125\n",
      "Accuracy:  0.809\n",
      "Loss: 0.814500244140625\n",
      "Accuracy:  0.806\n",
      "Loss: 0.7340670776367187\n",
      "Accuracy:  0.8215\n",
      "Loss: 0.7377974853515625\n",
      "Accuracy:  0.826\n",
      "Loss: 0.7794657592773437\n",
      "Accuracy:  0.8105\n",
      "Loss: 0.8117586059570312\n",
      "Accuracy:  0.8035\n",
      "Loss: 0.7710720825195313\n",
      "Accuracy:  0.811\n",
      "Loss: 0.7450690307617187\n",
      "Accuracy:  0.8195\n",
      "Loss: 0.7845648803710937\n",
      "Accuracy:  0.8095\n",
      "Loss: 0.7425258178710937\n",
      "Accuracy:  0.8195\n",
      "Loss: 0.716088134765625\n",
      "Accuracy:  0.8225\n",
      "Loss: 0.6998196411132812\n",
      "Accuracy:  0.832\n",
      "Loss: 0.7956888427734375\n",
      "Accuracy:  0.803\n",
      "Loss: 0.74757470703125\n",
      "Accuracy:  0.82\n",
      "Loss: 0.7353707275390625\n",
      "Accuracy:  0.822\n",
      "Loss: 0.794189208984375\n",
      "Accuracy:  0.802\n",
      "Loss: 0.71579052734375\n",
      "Accuracy:  0.817\n",
      "Loss: 0.83010009765625\n",
      "Accuracy:  0.8025\n",
      "Loss: 0.7915239868164062\n",
      "Accuracy:  0.8075\n",
      "Loss: 0.75133251953125\n",
      "Accuracy:  0.8135\n",
      "Loss: 0.71869970703125\n",
      "Accuracy:  0.826\n",
      "Loss: 0.8036328735351562\n",
      "Accuracy:  0.802\n",
      "Loss: 0.7040040893554688\n",
      "Accuracy:  0.8275\n",
      "Loss: 0.7445576171875\n",
      "Accuracy:  0.8185\n",
      "Loss: 0.706923828125\n",
      "Accuracy:  0.827\n",
      "Loss: 0.8111674194335937\n",
      "Accuracy:  0.8075\n",
      "Loss: 0.7631381225585937\n",
      "Accuracy:  0.812\n",
      "Loss: 0.7171301879882812\n",
      "Accuracy:  0.821\n",
      "Loss: 0.72811669921875\n",
      "Accuracy:  0.8185\n",
      "Loss: 0.7425787353515625\n",
      "Accuracy:  0.8195\n",
      "Loss: 0.8579305334936215\n",
      "Accuracy:  0.8354430379746836\n",
      "71.63518249888392\n",
      "0.026812940376731726\n",
      "Train: wpb=0, bsz=1967, num_updates=1680\n",
      "Loss: 0.7615780029296875\n",
      "Accuracy:  0.819\n",
      "Loss: 0.74336083984375\n",
      "Accuracy:  0.8165\n",
      "Loss: 0.7624027099609375\n",
      "Accuracy:  0.815\n",
      "Loss: 0.7318338012695312\n",
      "Accuracy:  0.8275\n",
      "Loss: 0.7603801879882812\n",
      "Accuracy:  0.8175\n",
      "Loss: 0.7113577880859375\n",
      "Accuracy:  0.8305\n",
      "Loss: 0.7240970458984375\n",
      "Accuracy:  0.822\n",
      "Loss: 0.7979755249023438\n",
      "Accuracy:  0.8105\n",
      "Loss: 0.73726708984375\n",
      "Accuracy:  0.8225\n",
      "Loss: 0.7082523803710937\n",
      "Accuracy:  0.826\n",
      "Loss: 0.6791604614257812\n",
      "Accuracy:  0.833\n",
      "Loss: 0.764108642578125\n",
      "Accuracy:  0.8165\n",
      "Loss: 0.7357770385742187\n",
      "Accuracy:  0.8205\n",
      "Loss: 0.7302569580078125\n",
      "Accuracy:  0.8215\n",
      "Loss: 0.73738037109375\n",
      "Accuracy:  0.829\n",
      "Loss: 0.7156096801757813\n",
      "Accuracy:  0.827\n",
      "Loss: 0.7201884155273437\n",
      "Accuracy:  0.8245\n",
      "Loss: 0.7113380737304688\n",
      "Accuracy:  0.8305\n",
      "Loss: 0.6906994018554687\n",
      "Accuracy:  0.8335\n",
      "Loss: 0.7492537841796875\n",
      "Accuracy:  0.8205\n",
      "Loss: 0.7331461181640625\n",
      "Accuracy:  0.823\n",
      "Loss: 0.686701171875\n",
      "Accuracy:  0.835\n",
      "Loss: 0.7840081787109375\n",
      "Accuracy:  0.812\n",
      "Loss: 0.7668834228515625\n",
      "Accuracy:  0.8165\n",
      "Loss: 0.766451416015625\n",
      "Accuracy:  0.8155\n",
      "Loss: 0.7291354370117188\n",
      "Accuracy:  0.8235\n",
      "Loss: 0.7436884765625\n",
      "Accuracy:  0.819\n",
      "Loss: 0.75079541015625\n",
      "Accuracy:  0.821\n",
      "Loss: 0.7686543579101562\n",
      "Accuracy:  0.815\n",
      "Loss: 0.6917869262695312\n",
      "Accuracy:  0.834\n",
      "Loss: 0.7683789672851562\n",
      "Accuracy:  0.8135\n",
      "Loss: 0.6939718017578125\n",
      "Accuracy:  0.84\n",
      "Loss: 0.7469232177734375\n",
      "Accuracy:  0.814\n",
      "Loss: 0.7640679931640625\n",
      "Accuracy:  0.8155\n",
      "Loss: 0.76413134765625\n",
      "Accuracy:  0.8105\n",
      "Loss: 0.7776884155273438\n",
      "Accuracy:  0.811\n",
      "Loss: 0.7517967529296875\n",
      "Accuracy:  0.817\n",
      "Loss: 0.7104176635742188\n",
      "Accuracy:  0.831\n",
      "Loss: 0.7587006225585937\n",
      "Accuracy:  0.813\n",
      "Loss: 0.8078187866210937\n",
      "Accuracy:  0.8095\n",
      "Loss: 0.7194148559570313\n",
      "Accuracy:  0.8235\n",
      "Loss: 0.7753687744140625\n",
      "Accuracy:  0.807\n",
      "Loss: 0.7572215576171875\n",
      "Accuracy:  0.815\n",
      "Loss: 0.7176028442382812\n",
      "Accuracy:  0.8215\n",
      "Loss: 0.679006103515625\n",
      "Accuracy:  0.8325\n",
      "Loss: 0.7571302490234375\n",
      "Accuracy:  0.8155\n",
      "Loss: 0.753419189453125\n",
      "Accuracy:  0.8175\n",
      "Loss: 0.7349619140625\n",
      "Accuracy:  0.8215\n",
      "Loss: 0.791361572265625\n",
      "Accuracy:  0.804\n",
      "Loss: 0.7196463012695312\n",
      "Accuracy:  0.819\n",
      "Loss: 0.7559747314453125\n",
      "Accuracy:  0.818\n",
      "Loss: 0.7257921142578125\n",
      "Accuracy:  0.8275\n",
      "Loss: 0.7333305053710938\n",
      "Accuracy:  0.82\n",
      "Loss: 0.7783313598632813\n",
      "Accuracy:  0.8115\n",
      "Loss: 0.7647730102539062\n",
      "Accuracy:  0.813\n",
      "Loss: 0.7786729736328125\n",
      "Accuracy:  0.8095\n",
      "Loss: 0.6832708129882813\n",
      "Accuracy:  0.837\n",
      "Loss: 0.728456298828125\n",
      "Accuracy:  0.8185\n",
      "Loss: 0.7455294645285304\n",
      "Accuracy:  0.810126582278481\n",
      "71.9942610017665\n",
      "0.025559354950860355\n",
      "Train: wpb=0, bsz=1967, num_updates=1740\n",
      "Loss: 0.7289451904296875\n",
      "Accuracy:  0.826\n",
      "Loss: 0.74245849609375\n",
      "Accuracy:  0.8155\n",
      "Loss: 0.7245073852539062\n",
      "Accuracy:  0.824\n",
      "Loss: 0.71983837890625\n",
      "Accuracy:  0.825\n",
      "Loss: 0.7600844116210937\n",
      "Accuracy:  0.8205\n",
      "Loss: 0.7720902099609375\n",
      "Accuracy:  0.8145\n",
      "Loss: 0.7209444580078125\n",
      "Accuracy:  0.8335\n",
      "Loss: 0.6488834838867188\n",
      "Accuracy:  0.841\n",
      "Loss: 0.743996826171875\n",
      "Accuracy:  0.821\n",
      "Loss: 0.7243030395507812\n",
      "Accuracy:  0.8215\n",
      "Loss: 0.7070518798828125\n",
      "Accuracy:  0.8255\n",
      "Loss: 0.7207034912109375\n",
      "Accuracy:  0.825\n",
      "Loss: 0.730546630859375\n",
      "Accuracy:  0.825\n",
      "Loss: 0.7358798217773438\n",
      "Accuracy:  0.8225\n",
      "Loss: 0.7440205078125\n",
      "Accuracy:  0.8155\n",
      "Loss: 0.7424033203125\n",
      "Accuracy:  0.82\n",
      "Loss: 0.7358890991210938\n",
      "Accuracy:  0.8205\n",
      "Loss: 0.7612657470703125\n",
      "Accuracy:  0.8165\n",
      "Loss: 0.7363817749023438\n",
      "Accuracy:  0.827\n",
      "Loss: 0.7458004150390625\n",
      "Accuracy:  0.821\n",
      "Loss: 0.7030359497070312\n",
      "Accuracy:  0.827\n",
      "Loss: 0.67803173828125\n",
      "Accuracy:  0.841\n",
      "Loss: 0.7543978271484375\n",
      "Accuracy:  0.8185\n",
      "Loss: 0.7577890625\n",
      "Accuracy:  0.8145\n",
      "Loss: 0.638742431640625\n",
      "Accuracy:  0.8455\n",
      "Loss: 0.7414656372070313\n",
      "Accuracy:  0.8205\n",
      "Loss: 0.73405859375\n",
      "Accuracy:  0.82\n",
      "Loss: 0.7740438232421875\n",
      "Accuracy:  0.811\n",
      "Loss: 0.69849755859375\n",
      "Accuracy:  0.837\n",
      "Loss: 0.77940625\n",
      "Accuracy:  0.8065\n",
      "Loss: 0.6758035278320312\n",
      "Accuracy:  0.8285\n",
      "Loss: 0.7340751953125\n",
      "Accuracy:  0.8195\n",
      "Loss: 0.7659067993164063\n",
      "Accuracy:  0.8185\n",
      "Loss: 0.7775958862304687\n",
      "Accuracy:  0.8195\n",
      "Loss: 0.7496448974609375\n",
      "Accuracy:  0.81\n",
      "Loss: 0.7382130737304687\n",
      "Accuracy:  0.82\n",
      "Loss: 0.7788858642578125\n",
      "Accuracy:  0.814\n",
      "Loss: 0.7790521850585937\n",
      "Accuracy:  0.8145\n",
      "Loss: 0.7437376708984375\n",
      "Accuracy:  0.824\n",
      "Loss: 0.7232637939453125\n",
      "Accuracy:  0.8225\n",
      "Loss: 0.7368433227539063\n",
      "Accuracy:  0.8235\n",
      "Loss: 0.7407924194335938\n",
      "Accuracy:  0.823\n",
      "Loss: 0.707568603515625\n",
      "Accuracy:  0.825\n",
      "Loss: 0.7969827880859375\n",
      "Accuracy:  0.803\n",
      "Loss: 0.7529302368164063\n",
      "Accuracy:  0.8105\n",
      "Loss: 0.760892333984375\n",
      "Accuracy:  0.813\n",
      "Loss: 0.724969970703125\n",
      "Accuracy:  0.821\n",
      "Loss: 0.7598789672851562\n",
      "Accuracy:  0.82\n",
      "Loss: 0.7326923828125\n",
      "Accuracy:  0.822\n",
      "Loss: 0.7777234497070312\n",
      "Accuracy:  0.811\n",
      "Loss: 0.718017822265625\n",
      "Accuracy:  0.822\n",
      "Loss: 0.7673292236328125\n",
      "Accuracy:  0.816\n",
      "Loss: 0.7748565063476562\n",
      "Accuracy:  0.8065\n",
      "Loss: 0.73234423828125\n",
      "Accuracy:  0.8205\n",
      "Loss: 0.744132080078125\n",
      "Accuracy:  0.8165\n",
      "Loss: 0.7341346435546875\n",
      "Accuracy:  0.8315\n",
      "Loss: 0.7119451293945313\n",
      "Accuracy:  0.8205\n",
      "Loss: 0.7910496215820313\n",
      "Accuracy:  0.8055\n",
      "Loss: 0.8898756775674941\n",
      "Accuracy:  0.810126582278481\n",
      "72.33106648938423\n",
      "0.024591334755997125\n",
      "Train: wpb=0, bsz=1967, num_updates=1800\n",
      "Loss: 0.69362060546875\n",
      "Accuracy:  0.8355\n",
      "Loss: 0.722680908203125\n",
      "Accuracy:  0.827\n",
      "Loss: 0.7333259887695313\n",
      "Accuracy:  0.827\n",
      "Loss: 0.7579754638671875\n",
      "Accuracy:  0.815\n",
      "Loss: 0.7297592163085938\n",
      "Accuracy:  0.822\n",
      "Loss: 0.7022652587890625\n",
      "Accuracy:  0.833\n",
      "Loss: 0.7114105224609375\n",
      "Accuracy:  0.8315\n",
      "Loss: 0.7409086303710938\n",
      "Accuracy:  0.82\n",
      "Loss: 0.691670166015625\n",
      "Accuracy:  0.834\n",
      "Loss: 0.7409391479492188\n",
      "Accuracy:  0.8215\n",
      "Loss: 0.7456940307617187\n",
      "Accuracy:  0.817\n",
      "Loss: 0.7114922485351562\n",
      "Accuracy:  0.828\n",
      "Loss: 0.7379921875\n",
      "Accuracy:  0.8225\n",
      "Loss: 0.7625797119140625\n",
      "Accuracy:  0.8125\n",
      "Loss: 0.7657099609375\n",
      "Accuracy:  0.8145\n",
      "Loss: 0.6975305786132813\n",
      "Accuracy:  0.83\n",
      "Loss: 0.730631103515625\n",
      "Accuracy:  0.828\n",
      "Loss: 0.7438154296875\n",
      "Accuracy:  0.826\n",
      "Loss: 0.7821659545898437\n",
      "Accuracy:  0.812\n",
      "Loss: 0.7971383666992188\n",
      "Accuracy:  0.8085\n",
      "Loss: 0.7370572509765625\n",
      "Accuracy:  0.8205\n",
      "Loss: 0.6599801025390625\n",
      "Accuracy:  0.8365\n",
      "Loss: 0.7242260131835937\n",
      "Accuracy:  0.828\n",
      "Loss: 0.6804977416992187\n",
      "Accuracy:  0.8345\n",
      "Loss: 0.6930789794921876\n",
      "Accuracy:  0.8325\n",
      "Loss: 0.7765733642578125\n",
      "Accuracy:  0.8175\n",
      "Loss: 0.7379122314453125\n",
      "Accuracy:  0.8265\n",
      "Loss: 0.7011441040039063\n",
      "Accuracy:  0.8295\n",
      "Loss: 0.76431103515625\n",
      "Accuracy:  0.813\n",
      "Loss: 0.6929539794921875\n",
      "Accuracy:  0.8315\n",
      "Loss: 0.7314566650390625\n",
      "Accuracy:  0.816\n",
      "Loss: 0.7140797119140625\n",
      "Accuracy:  0.8215\n",
      "Loss: 0.7356484985351562\n",
      "Accuracy:  0.8255\n",
      "Loss: 0.7549900512695312\n",
      "Accuracy:  0.8125\n",
      "Loss: 0.688311767578125\n",
      "Accuracy:  0.835\n",
      "Loss: 0.7345623168945312\n",
      "Accuracy:  0.8215\n",
      "Loss: 0.6903662719726562\n",
      "Accuracy:  0.834\n",
      "Loss: 0.71114404296875\n",
      "Accuracy:  0.8275\n",
      "Loss: 0.7300181274414063\n",
      "Accuracy:  0.821\n",
      "Loss: 0.6861862182617188\n",
      "Accuracy:  0.8335\n",
      "Loss: 0.694648681640625\n",
      "Accuracy:  0.8225\n",
      "Loss: 0.7261975708007813\n",
      "Accuracy:  0.827\n",
      "Loss: 0.6595662841796875\n",
      "Accuracy:  0.8455\n",
      "Loss: 0.7828440551757813\n",
      "Accuracy:  0.806\n",
      "Loss: 0.67542919921875\n",
      "Accuracy:  0.8315\n",
      "Loss: 0.6926151733398438\n",
      "Accuracy:  0.8275\n",
      "Loss: 0.7308334350585938\n",
      "Accuracy:  0.822\n",
      "Loss: 0.798330810546875\n",
      "Accuracy:  0.806\n",
      "Loss: 0.7348809814453126\n",
      "Accuracy:  0.818\n",
      "Loss: 0.8074656982421875\n",
      "Accuracy:  0.805\n",
      "Loss: 0.7986046752929687\n",
      "Accuracy:  0.8065\n",
      "Loss: 0.75788916015625\n",
      "Accuracy:  0.8195\n",
      "Loss: 0.756124755859375\n",
      "Accuracy:  0.816\n",
      "Loss: 0.734482177734375\n",
      "Accuracy:  0.819\n",
      "Loss: 0.7562350463867188\n",
      "Accuracy:  0.8205\n",
      "Loss: 0.6631073608398438\n",
      "Accuracy:  0.8405\n",
      "Loss: 0.738386962890625\n",
      "Accuracy:  0.82\n",
      "Loss: 0.7771294555664062\n",
      "Accuracy:  0.815\n",
      "Loss: 0.5698861230777789\n",
      "Accuracy:  0.8481012658227848\n",
      "72.65379192552608\n",
      "0.023531288706973286\n",
      "Train: wpb=0, bsz=1967, num_updates=1860\n",
      "Loss: 0.7324105834960938\n",
      "Accuracy:  0.815\n",
      "Loss: 0.73520654296875\n",
      "Accuracy:  0.8235\n",
      "Loss: 0.695299072265625\n",
      "Accuracy:  0.828\n",
      "Loss: 0.746952880859375\n",
      "Accuracy:  0.816\n",
      "Loss: 0.70709033203125\n",
      "Accuracy:  0.8285\n",
      "Loss: 0.7564990234375\n",
      "Accuracy:  0.82\n",
      "Loss: 0.7441952514648438\n",
      "Accuracy:  0.819\n",
      "Loss: 0.70181494140625\n",
      "Accuracy:  0.8265\n",
      "Loss: 0.7645358276367188\n",
      "Accuracy:  0.819\n",
      "Loss: 0.687577392578125\n",
      "Accuracy:  0.8295\n",
      "Loss: 0.6791064453125\n",
      "Accuracy:  0.831\n",
      "Loss: 0.7349800415039063\n",
      "Accuracy:  0.823\n",
      "Loss: 0.7017120361328125\n",
      "Accuracy:  0.8295\n",
      "Loss: 0.7049664306640625\n",
      "Accuracy:  0.8345\n",
      "Loss: 0.6759371337890625\n",
      "Accuracy:  0.8375\n",
      "Loss: 0.70714599609375\n",
      "Accuracy:  0.8355\n",
      "Loss: 0.7612996215820312\n",
      "Accuracy:  0.815\n",
      "Loss: 0.6911770629882813\n",
      "Accuracy:  0.831\n",
      "Loss: 0.7344164428710938\n",
      "Accuracy:  0.825\n",
      "Loss: 0.7655571899414062\n",
      "Accuracy:  0.812\n",
      "Loss: 0.7201967163085937\n",
      "Accuracy:  0.822\n",
      "Loss: 0.6666273193359376\n",
      "Accuracy:  0.8465\n",
      "Loss: 0.7288026123046875\n",
      "Accuracy:  0.8205\n",
      "Loss: 0.7358795776367187\n",
      "Accuracy:  0.822\n",
      "Loss: 0.68745263671875\n",
      "Accuracy:  0.834\n",
      "Loss: 0.704666259765625\n",
      "Accuracy:  0.83\n",
      "Loss: 0.705578125\n",
      "Accuracy:  0.8275\n",
      "Loss: 0.7518173217773437\n",
      "Accuracy:  0.8215\n",
      "Loss: 0.6782387084960938\n",
      "Accuracy:  0.8405\n",
      "Loss: 0.7623834838867187\n",
      "Accuracy:  0.817\n",
      "Loss: 0.7277744140625\n",
      "Accuracy:  0.8175\n",
      "Loss: 0.7016581420898438\n",
      "Accuracy:  0.8295\n",
      "Loss: 0.6961771240234375\n",
      "Accuracy:  0.8355\n",
      "Loss: 0.718385009765625\n",
      "Accuracy:  0.8215\n",
      "Loss: 0.752453125\n",
      "Accuracy:  0.8195\n",
      "Loss: 0.7054647216796875\n",
      "Accuracy:  0.832\n",
      "Loss: 0.7234046020507813\n",
      "Accuracy:  0.8215\n",
      "Loss: 0.705345703125\n",
      "Accuracy:  0.8295\n",
      "Loss: 0.7342544555664062\n",
      "Accuracy:  0.8215\n",
      "Loss: 0.72302783203125\n",
      "Accuracy:  0.8245\n",
      "Loss: 0.6830989379882813\n",
      "Accuracy:  0.8305\n",
      "Loss: 0.7774517211914063\n",
      "Accuracy:  0.811\n",
      "Loss: 0.712010986328125\n",
      "Accuracy:  0.825\n",
      "Loss: 0.7252510375976563\n",
      "Accuracy:  0.828\n",
      "Loss: 0.6709672241210938\n",
      "Accuracy:  0.837\n",
      "Loss: 0.6890004272460938\n",
      "Accuracy:  0.829\n",
      "Loss: 0.727500244140625\n",
      "Accuracy:  0.824\n",
      "Loss: 0.7641654052734375\n",
      "Accuracy:  0.8105\n",
      "Loss: 0.7289544067382813\n",
      "Accuracy:  0.82\n",
      "Loss: 0.7474532470703125\n",
      "Accuracy:  0.8245\n",
      "Loss: 0.798241943359375\n",
      "Accuracy:  0.8055\n",
      "Loss: 0.7342249145507812\n",
      "Accuracy:  0.824\n",
      "Loss: 0.804930908203125\n",
      "Accuracy:  0.803\n",
      "Loss: 0.71869775390625\n",
      "Accuracy:  0.8225\n",
      "Loss: 0.6953261108398437\n",
      "Accuracy:  0.8325\n",
      "Loss: 0.7198785400390625\n",
      "Accuracy:  0.8275\n",
      "Loss: 0.6799148559570313\n",
      "Accuracy:  0.8285\n",
      "Loss: 0.7506431884765625\n",
      "Accuracy:  0.8115\n",
      "Loss: 0.673716629607768\n",
      "Accuracy:  0.8481012658227848\n",
      "72.96044914845146\n",
      "0.022560065858666864\n",
      "Train: wpb=0, bsz=1967, num_updates=1920\n",
      "Loss: 0.7085042724609375\n",
      "Accuracy:  0.8265\n",
      "Loss: 0.6925741577148438\n",
      "Accuracy:  0.832\n",
      "Loss: 0.7719305419921875\n",
      "Accuracy:  0.805\n",
      "Loss: 0.6649464111328125\n",
      "Accuracy:  0.8395\n",
      "Loss: 0.783985595703125\n",
      "Accuracy:  0.8125\n",
      "Loss: 0.6815315551757812\n",
      "Accuracy:  0.8325\n",
      "Loss: 0.659272216796875\n",
      "Accuracy:  0.843\n",
      "Loss: 0.6658756713867188\n",
      "Accuracy:  0.8365\n",
      "Loss: 0.7362103881835937\n",
      "Accuracy:  0.8265\n",
      "Loss: 0.7166498413085938\n",
      "Accuracy:  0.83\n",
      "Loss: 0.7328421630859375\n",
      "Accuracy:  0.825\n",
      "Loss: 0.6963798217773437\n",
      "Accuracy:  0.835\n",
      "Loss: 0.6618745727539063\n",
      "Accuracy:  0.8375\n",
      "Loss: 0.742270751953125\n",
      "Accuracy:  0.824\n",
      "Loss: 0.6804734497070313\n",
      "Accuracy:  0.8325\n",
      "Loss: 0.6817711181640626\n",
      "Accuracy:  0.8295\n",
      "Loss: 0.7150853271484375\n",
      "Accuracy:  0.826\n",
      "Loss: 0.681957275390625\n",
      "Accuracy:  0.836\n",
      "Loss: 0.7793995361328125\n",
      "Accuracy:  0.806\n",
      "Loss: 0.7657119750976562\n",
      "Accuracy:  0.812\n",
      "Loss: 0.78175146484375\n",
      "Accuracy:  0.806\n",
      "Loss: 0.733523681640625\n",
      "Accuracy:  0.819\n",
      "Loss: 0.7396558227539063\n",
      "Accuracy:  0.818\n",
      "Loss: 0.7105477905273437\n",
      "Accuracy:  0.826\n",
      "Loss: 0.6823206787109375\n",
      "Accuracy:  0.842\n",
      "Loss: 0.6629879150390625\n",
      "Accuracy:  0.843\n",
      "Loss: 0.7183521118164062\n",
      "Accuracy:  0.826\n",
      "Loss: 0.710076171875\n",
      "Accuracy:  0.8335\n",
      "Loss: 0.727335205078125\n",
      "Accuracy:  0.8285\n",
      "Loss: 0.7150106201171875\n",
      "Accuracy:  0.824\n",
      "Loss: 0.6947037353515625\n",
      "Accuracy:  0.835\n",
      "Loss: 0.7703860473632812\n",
      "Accuracy:  0.8115\n",
      "Loss: 0.7394761962890625\n",
      "Accuracy:  0.82\n",
      "Loss: 0.675767333984375\n",
      "Accuracy:  0.8385\n",
      "Loss: 0.694799072265625\n",
      "Accuracy:  0.8295\n",
      "Loss: 0.73223095703125\n",
      "Accuracy:  0.8225\n",
      "Loss: 0.7263805541992188\n",
      "Accuracy:  0.822\n",
      "Loss: 0.6864491577148437\n",
      "Accuracy:  0.8335\n",
      "Loss: 0.6807256469726563\n",
      "Accuracy:  0.8385\n",
      "Loss: 0.7280968627929687\n",
      "Accuracy:  0.822\n",
      "Loss: 0.8510095825195313\n",
      "Accuracy:  0.7885\n",
      "Loss: 0.6881453247070313\n",
      "Accuracy:  0.8365\n",
      "Loss: 0.754197021484375\n",
      "Accuracy:  0.8195\n",
      "Loss: 0.7470996704101562\n",
      "Accuracy:  0.82\n",
      "Loss: 0.6981370239257813\n",
      "Accuracy:  0.831\n",
      "Loss: 0.7281807250976563\n",
      "Accuracy:  0.818\n",
      "Loss: 0.6605001831054688\n",
      "Accuracy:  0.8405\n",
      "Loss: 0.7254666748046875\n",
      "Accuracy:  0.82\n",
      "Loss: 0.7601951904296875\n",
      "Accuracy:  0.82\n",
      "Loss: 0.7199951171875\n",
      "Accuracy:  0.8295\n",
      "Loss: 0.7313317260742187\n",
      "Accuracy:  0.8205\n",
      "Loss: 0.7857107543945312\n",
      "Accuracy:  0.808\n",
      "Loss: 0.6928512573242187\n",
      "Accuracy:  0.831\n",
      "Loss: 0.7208741455078125\n",
      "Accuracy:  0.824\n",
      "Loss: 0.6954099731445312\n",
      "Accuracy:  0.826\n",
      "Loss: 0.6985826416015625\n",
      "Accuracy:  0.827\n",
      "Loss: 0.737901123046875\n",
      "Accuracy:  0.8235\n",
      "Loss: 0.7720427856445312\n",
      "Accuracy:  0.813\n",
      "Loss: 0.9799795995784711\n",
      "Accuracy:  0.7848101265822784\n",
      "73.25139538064784\n",
      "0.02177908196126096\n",
      "Train: wpb=0, bsz=1967, num_updates=1980\n",
      "Loss: 0.701648681640625\n",
      "Accuracy:  0.829\n",
      "Loss: 0.706919921875\n",
      "Accuracy:  0.8345\n",
      "Loss: 0.7238778686523437\n",
      "Accuracy:  0.823\n",
      "Loss: 0.7335601196289062\n",
      "Accuracy:  0.823\n",
      "Loss: 0.6994981079101562\n",
      "Accuracy:  0.8285\n",
      "Loss: 0.7562031860351562\n",
      "Accuracy:  0.8215\n",
      "Loss: 0.723961181640625\n",
      "Accuracy:  0.8195\n",
      "Loss: 0.6428564453125\n",
      "Accuracy:  0.8405\n",
      "Loss: 0.6757566528320312\n",
      "Accuracy:  0.833\n",
      "Loss: 0.7007014770507812\n",
      "Accuracy:  0.83\n",
      "Loss: 0.6635200805664062\n",
      "Accuracy:  0.837\n",
      "Loss: 0.705426025390625\n",
      "Accuracy:  0.826\n",
      "Loss: 0.7234996337890625\n",
      "Accuracy:  0.82\n",
      "Loss: 0.7067412719726562\n",
      "Accuracy:  0.822\n",
      "Loss: 0.6708203735351562\n",
      "Accuracy:  0.8405\n",
      "Loss: 0.7335136108398438\n",
      "Accuracy:  0.822\n",
      "Loss: 0.670357666015625\n",
      "Accuracy:  0.833\n",
      "Loss: 0.7020474243164062\n",
      "Accuracy:  0.8275\n",
      "Loss: 0.6935222778320312\n",
      "Accuracy:  0.8315\n",
      "Loss: 0.718533447265625\n",
      "Accuracy:  0.825\n",
      "Loss: 0.6489744873046875\n",
      "Accuracy:  0.8465\n",
      "Loss: 0.7109817504882813\n",
      "Accuracy:  0.827\n",
      "Loss: 0.6647074584960937\n",
      "Accuracy:  0.842\n",
      "Loss: 0.6882548828125\n",
      "Accuracy:  0.837\n",
      "Loss: 0.73258203125\n",
      "Accuracy:  0.819\n",
      "Loss: 0.6996007690429688\n",
      "Accuracy:  0.831\n",
      "Loss: 0.70546337890625\n",
      "Accuracy:  0.8305\n",
      "Loss: 0.7510441284179687\n",
      "Accuracy:  0.81\n",
      "Loss: 0.7011499633789062\n",
      "Accuracy:  0.8295\n",
      "Loss: 0.7029849853515625\n",
      "Accuracy:  0.8315\n",
      "Loss: 0.7044437866210937\n",
      "Accuracy:  0.8265\n",
      "Loss: 0.6928888549804687\n",
      "Accuracy:  0.8305\n",
      "Loss: 0.7261426391601562\n",
      "Accuracy:  0.8225\n",
      "Loss: 0.6700982666015625\n",
      "Accuracy:  0.8375\n",
      "Loss: 0.7201408081054688\n",
      "Accuracy:  0.8235\n",
      "Loss: 0.74095751953125\n",
      "Accuracy:  0.823\n",
      "Loss: 0.7204743041992188\n",
      "Accuracy:  0.8305\n",
      "Loss: 0.726592041015625\n",
      "Accuracy:  0.825\n",
      "Loss: 0.7273220825195312\n",
      "Accuracy:  0.822\n",
      "Loss: 0.7111666870117187\n",
      "Accuracy:  0.829\n",
      "Loss: 0.6873320922851562\n",
      "Accuracy:  0.8305\n",
      "Loss: 0.6499191284179687\n",
      "Accuracy:  0.844\n",
      "Loss: 0.7409261474609375\n",
      "Accuracy:  0.8185\n",
      "Loss: 0.6983621215820313\n",
      "Accuracy:  0.828\n",
      "Loss: 0.7410828247070312\n",
      "Accuracy:  0.816\n",
      "Loss: 0.7506636352539062\n",
      "Accuracy:  0.8155\n",
      "Loss: 0.6873643798828125\n",
      "Accuracy:  0.834\n",
      "Loss: 0.7222633056640625\n",
      "Accuracy:  0.828\n",
      "Loss: 0.703399658203125\n",
      "Accuracy:  0.827\n",
      "Loss: 0.7529044799804687\n",
      "Accuracy:  0.82\n",
      "Loss: 0.7492160034179688\n",
      "Accuracy:  0.819\n",
      "Loss: 0.7544024047851563\n",
      "Accuracy:  0.8185\n",
      "Loss: 0.6533416748046875\n",
      "Accuracy:  0.84\n",
      "Loss: 0.756844970703125\n",
      "Accuracy:  0.819\n",
      "Loss: 0.7182677612304688\n",
      "Accuracy:  0.8345\n",
      "Loss: 0.6828648681640626\n",
      "Accuracy:  0.837\n",
      "Loss: 0.7446398315429688\n",
      "Accuracy:  0.819\n",
      "Loss: 0.7087659912109375\n",
      "Accuracy:  0.8305\n",
      "Loss: 0.6729941066307358\n",
      "Accuracy:  0.810126582278481\n",
      "73.53185280243585\n",
      "0.020851748124882035\n",
      "Train: wpb=0, bsz=1967, num_updates=2040\n",
      "Loss: 0.6557186279296875\n",
      "Accuracy:  0.843\n",
      "Loss: 0.7540855712890625\n",
      "Accuracy:  0.817\n",
      "Loss: 0.7824055786132813\n",
      "Accuracy:  0.8115\n",
      "Loss: 0.6612029418945312\n",
      "Accuracy:  0.8455\n",
      "Loss: 0.731565185546875\n",
      "Accuracy:  0.825\n",
      "Loss: 0.6660294189453125\n",
      "Accuracy:  0.842\n",
      "Loss: 0.7464711303710938\n",
      "Accuracy:  0.8175\n",
      "Loss: 0.6677355346679688\n",
      "Accuracy:  0.835\n",
      "Loss: 0.7172884521484375\n",
      "Accuracy:  0.8275\n",
      "Loss: 0.7032852783203125\n",
      "Accuracy:  0.828\n",
      "Loss: 0.69767578125\n",
      "Accuracy:  0.836\n",
      "Loss: 0.7349317626953125\n",
      "Accuracy:  0.8225\n",
      "Loss: 0.7345230102539062\n",
      "Accuracy:  0.821\n",
      "Loss: 0.7373776245117187\n",
      "Accuracy:  0.815\n",
      "Loss: 0.7027328491210938\n",
      "Accuracy:  0.8315\n",
      "Loss: 0.69005810546875\n",
      "Accuracy:  0.827\n",
      "Loss: 0.6621069946289062\n",
      "Accuracy:  0.8415\n",
      "Loss: 0.7421199951171875\n",
      "Accuracy:  0.8205\n",
      "Loss: 0.7558153076171875\n",
      "Accuracy:  0.8195\n",
      "Loss: 0.676685302734375\n",
      "Accuracy:  0.8355\n",
      "Loss: 0.6950817260742187\n",
      "Accuracy:  0.8285\n",
      "Loss: 0.6659419555664062\n",
      "Accuracy:  0.841\n",
      "Loss: 0.6771727294921875\n",
      "Accuracy:  0.833\n",
      "Loss: 0.7076815185546875\n",
      "Accuracy:  0.8295\n",
      "Loss: 0.6593533935546875\n",
      "Accuracy:  0.841\n",
      "Loss: 0.6710675048828125\n",
      "Accuracy:  0.834\n",
      "Loss: 0.6940100708007813\n",
      "Accuracy:  0.832\n",
      "Loss: 0.7522383422851563\n",
      "Accuracy:  0.8235\n",
      "Loss: 0.650808837890625\n",
      "Accuracy:  0.842\n",
      "Loss: 0.7068108520507812\n",
      "Accuracy:  0.83\n",
      "Loss: 0.6738152465820313\n",
      "Accuracy:  0.833\n",
      "Loss: 0.72883056640625\n",
      "Accuracy:  0.829\n",
      "Loss: 0.6621992797851562\n",
      "Accuracy:  0.842\n",
      "Loss: 0.7017843627929687\n",
      "Accuracy:  0.826\n",
      "Loss: 0.7589662475585938\n",
      "Accuracy:  0.8185\n",
      "Loss: 0.672161376953125\n",
      "Accuracy:  0.8345\n",
      "Loss: 0.7095704956054687\n",
      "Accuracy:  0.8255\n",
      "Loss: 0.7213024291992187\n",
      "Accuracy:  0.822\n",
      "Loss: 0.7562977905273438\n",
      "Accuracy:  0.8175\n",
      "Loss: 0.712852294921875\n",
      "Accuracy:  0.831\n",
      "Loss: 0.741273193359375\n",
      "Accuracy:  0.8185\n",
      "Loss: 0.7359105834960937\n",
      "Accuracy:  0.8215\n",
      "Loss: 0.7248333740234375\n",
      "Accuracy:  0.8265\n",
      "Loss: 0.6730007934570312\n",
      "Accuracy:  0.833\n",
      "Loss: 0.6830006713867187\n",
      "Accuracy:  0.828\n",
      "Loss: 0.7015919189453125\n",
      "Accuracy:  0.831\n",
      "Loss: 0.7059822998046875\n",
      "Accuracy:  0.8325\n",
      "Loss: 0.7308795776367187\n",
      "Accuracy:  0.821\n",
      "Loss: 0.725722900390625\n",
      "Accuracy:  0.8265\n",
      "Loss: 0.7352427978515625\n",
      "Accuracy:  0.8235\n",
      "Loss: 0.7414229125976562\n",
      "Accuracy:  0.819\n",
      "Loss: 0.7758609619140625\n",
      "Accuracy:  0.807\n",
      "Loss: 0.7143524780273437\n",
      "Accuracy:  0.8265\n",
      "Loss: 0.6742437133789062\n",
      "Accuracy:  0.8345\n",
      "Loss: 0.6443178100585938\n",
      "Accuracy:  0.8395\n",
      "Loss: 0.7474459228515625\n",
      "Accuracy:  0.8185\n",
      "Loss: 0.698587890625\n",
      "Accuracy:  0.827\n",
      "Loss: 0.6775455932617187\n",
      "Accuracy:  0.8355\n",
      "Loss: 0.5310898792894581\n",
      "Accuracy:  0.8860759493670886\n",
      "73.79909092338906\n",
      "0.020169101529811786\n",
      "Train: wpb=0, bsz=1967, num_updates=2100\n",
      "Loss: 0.702005615234375\n",
      "Accuracy:  0.833\n",
      "Loss: 0.692391357421875\n",
      "Accuracy:  0.8275\n",
      "Loss: 0.6741255493164062\n",
      "Accuracy:  0.8355\n",
      "Loss: 0.6965496826171875\n",
      "Accuracy:  0.8335\n",
      "Loss: 0.6334738159179687\n",
      "Accuracy:  0.844\n",
      "Loss: 0.7300989379882813\n",
      "Accuracy:  0.817\n",
      "Loss: 0.6419473266601563\n",
      "Accuracy:  0.844\n",
      "Loss: 0.71598291015625\n",
      "Accuracy:  0.826\n",
      "Loss: 0.7312216796875\n",
      "Accuracy:  0.8275\n",
      "Loss: 0.6967847290039062\n",
      "Accuracy:  0.8305\n",
      "Loss: 0.69842724609375\n",
      "Accuracy:  0.8315\n",
      "Loss: 0.668525634765625\n",
      "Accuracy:  0.838\n",
      "Loss: 0.676938720703125\n",
      "Accuracy:  0.8355\n",
      "Loss: 0.7064282836914062\n",
      "Accuracy:  0.831\n",
      "Loss: 0.6794118041992188\n",
      "Accuracy:  0.836\n",
      "Loss: 0.7214799194335938\n",
      "Accuracy:  0.828\n",
      "Loss: 0.7538865966796875\n",
      "Accuracy:  0.816\n",
      "Loss: 0.7335944213867187\n",
      "Accuracy:  0.828\n",
      "Loss: 0.6684151000976563\n",
      "Accuracy:  0.8385\n",
      "Loss: 0.6828929443359375\n",
      "Accuracy:  0.8335\n",
      "Loss: 0.6938735961914062\n",
      "Accuracy:  0.8285\n",
      "Loss: 0.6917413330078125\n",
      "Accuracy:  0.834\n",
      "Loss: 0.6858673706054688\n",
      "Accuracy:  0.833\n",
      "Loss: 0.6927169799804688\n",
      "Accuracy:  0.829\n",
      "Loss: 0.6942281494140625\n",
      "Accuracy:  0.833\n",
      "Loss: 0.67459716796875\n",
      "Accuracy:  0.834\n",
      "Loss: 0.66625\n",
      "Accuracy:  0.8375\n",
      "Loss: 0.7034918823242188\n",
      "Accuracy:  0.8255\n",
      "Loss: 0.7012200927734376\n",
      "Accuracy:  0.8255\n",
      "Loss: 0.7387260131835938\n",
      "Accuracy:  0.8215\n",
      "Loss: 0.706298095703125\n",
      "Accuracy:  0.8335\n",
      "Loss: 0.6678235473632812\n",
      "Accuracy:  0.842\n",
      "Loss: 0.7359301147460937\n",
      "Accuracy:  0.826\n",
      "Loss: 0.7184781494140625\n",
      "Accuracy:  0.8265\n",
      "Loss: 0.683524658203125\n",
      "Accuracy:  0.832\n",
      "Loss: 0.715890869140625\n",
      "Accuracy:  0.83\n",
      "Loss: 0.71713623046875\n",
      "Accuracy:  0.833\n",
      "Loss: 0.712426025390625\n",
      "Accuracy:  0.8225\n",
      "Loss: 0.7355466918945313\n",
      "Accuracy:  0.8205\n",
      "Loss: 0.6868203735351562\n",
      "Accuracy:  0.8355\n",
      "Loss: 0.68508056640625\n",
      "Accuracy:  0.8375\n",
      "Loss: 0.737298828125\n",
      "Accuracy:  0.819\n",
      "Loss: 0.7044358520507813\n",
      "Accuracy:  0.83\n",
      "Loss: 0.6568517456054688\n",
      "Accuracy:  0.8375\n",
      "Loss: 0.7102783203125\n",
      "Accuracy:  0.8255\n",
      "Loss: 0.74545458984375\n",
      "Accuracy:  0.82\n",
      "Loss: 0.7214662475585938\n",
      "Accuracy:  0.8255\n",
      "Loss: 0.7041847534179687\n",
      "Accuracy:  0.8365\n",
      "Loss: 0.7095967407226562\n",
      "Accuracy:  0.8255\n",
      "Loss: 0.6770681762695312\n",
      "Accuracy:  0.832\n",
      "Loss: 0.73650634765625\n",
      "Accuracy:  0.8275\n",
      "Loss: 0.7176201782226562\n",
      "Accuracy:  0.822\n",
      "Loss: 0.7440946044921875\n",
      "Accuracy:  0.8215\n",
      "Loss: 0.7147666015625\n",
      "Accuracy:  0.8255\n",
      "Loss: 0.6595563354492188\n",
      "Accuracy:  0.8355\n",
      "Loss: 0.7229420166015625\n",
      "Accuracy:  0.8275\n",
      "Loss: 0.7448912353515625\n",
      "Accuracy:  0.8135\n",
      "Loss: 0.6956417846679688\n",
      "Accuracy:  0.834\n",
      "Loss: 1.0093287214448181\n",
      "Accuracy:  0.7974683544303798\n",
      "74.05456422301077\n",
      "0.01949677211216255\n",
      "Train: wpb=0, bsz=1967, num_updates=2160\n",
      "Loss: 0.6666002197265625\n",
      "Accuracy:  0.843\n",
      "Loss: 0.682485107421875\n",
      "Accuracy:  0.8355\n",
      "Loss: 0.693093505859375\n",
      "Accuracy:  0.832\n",
      "Loss: 0.6295289306640625\n",
      "Accuracy:  0.848\n",
      "Loss: 0.7276915283203125\n",
      "Accuracy:  0.823\n",
      "Loss: 0.7411239013671875\n",
      "Accuracy:  0.824\n",
      "Loss: 0.6640523071289063\n",
      "Accuracy:  0.836\n",
      "Loss: 0.7273236694335937\n",
      "Accuracy:  0.8235\n",
      "Loss: 0.7298914794921875\n",
      "Accuracy:  0.8215\n",
      "Loss: 0.6800274658203125\n",
      "Accuracy:  0.8295\n",
      "Loss: 0.69022705078125\n",
      "Accuracy:  0.8285\n",
      "Loss: 0.7191194458007812\n",
      "Accuracy:  0.825\n",
      "Loss: 0.66012548828125\n",
      "Accuracy:  0.844\n",
      "Loss: 0.6354381103515625\n",
      "Accuracy:  0.8465\n",
      "Loss: 0.6940189208984375\n",
      "Accuracy:  0.834\n",
      "Loss: 0.66996875\n",
      "Accuracy:  0.8355\n",
      "Loss: 0.7206036376953125\n",
      "Accuracy:  0.829\n",
      "Loss: 0.7366388549804688\n",
      "Accuracy:  0.8225\n",
      "Loss: 0.6980761108398438\n",
      "Accuracy:  0.8265\n",
      "Loss: 0.6763948974609375\n",
      "Accuracy:  0.8375\n",
      "Loss: 0.7317152709960938\n",
      "Accuracy:  0.8235\n",
      "Loss: 0.7278541870117188\n",
      "Accuracy:  0.8295\n",
      "Loss: 0.7164475708007813\n",
      "Accuracy:  0.8265\n",
      "Loss: 0.6556713256835938\n",
      "Accuracy:  0.8395\n",
      "Loss: 0.6806022338867187\n",
      "Accuracy:  0.833\n",
      "Loss: 0.6934170532226562\n",
      "Accuracy:  0.834\n",
      "Loss: 0.7400115356445313\n",
      "Accuracy:  0.828\n",
      "Loss: 0.7102124633789062\n",
      "Accuracy:  0.82\n",
      "Loss: 0.684364990234375\n",
      "Accuracy:  0.8305\n",
      "Loss: 0.6714000244140625\n",
      "Accuracy:  0.8415\n",
      "Loss: 0.7338055419921875\n",
      "Accuracy:  0.826\n",
      "Loss: 0.6673861694335937\n",
      "Accuracy:  0.838\n",
      "Loss: 0.7477733764648438\n",
      "Accuracy:  0.822\n",
      "Loss: 0.6849742431640625\n",
      "Accuracy:  0.8315\n",
      "Loss: 0.6665358276367187\n",
      "Accuracy:  0.835\n",
      "Loss: 0.7223397827148438\n",
      "Accuracy:  0.828\n",
      "Loss: 0.7264287719726562\n",
      "Accuracy:  0.8225\n",
      "Loss: 0.66669921875\n",
      "Accuracy:  0.8395\n",
      "Loss: 0.682270751953125\n",
      "Accuracy:  0.832\n",
      "Loss: 0.700590087890625\n",
      "Accuracy:  0.833\n",
      "Loss: 0.7377435913085938\n",
      "Accuracy:  0.818\n",
      "Loss: 0.6677232055664063\n",
      "Accuracy:  0.836\n",
      "Loss: 0.70526220703125\n",
      "Accuracy:  0.827\n",
      "Loss: 0.700585693359375\n",
      "Accuracy:  0.829\n",
      "Loss: 0.7164682006835937\n",
      "Accuracy:  0.83\n",
      "Loss: 0.6753883666992188\n",
      "Accuracy:  0.8285\n",
      "Loss: 0.6956455688476563\n",
      "Accuracy:  0.8295\n",
      "Loss: 0.7135711669921875\n",
      "Accuracy:  0.8265\n",
      "Loss: 0.6608552856445312\n",
      "Accuracy:  0.8405\n",
      "Loss: 0.6812390747070313\n",
      "Accuracy:  0.8385\n",
      "Loss: 0.7221328735351562\n",
      "Accuracy:  0.826\n",
      "Loss: 0.7547133178710937\n",
      "Accuracy:  0.8215\n",
      "Loss: 0.6950202026367187\n",
      "Accuracy:  0.8275\n",
      "Loss: 0.7174653930664062\n",
      "Accuracy:  0.822\n",
      "Loss: 0.674509765625\n",
      "Accuracy:  0.833\n",
      "Loss: 0.7228419799804687\n",
      "Accuracy:  0.8185\n",
      "Loss: 0.6391728515625\n",
      "Accuracy:  0.8425\n",
      "Loss: 0.7751702880859375\n",
      "Accuracy:  0.813\n",
      "Loss: 0.7665469012682951\n",
      "Accuracy:  0.8481012658227848\n",
      "74.29718949040759\n",
      "0.01889821465068353\n",
      "Train: wpb=0, bsz=1967, num_updates=2220\n",
      "Loss: 0.7239424438476563\n",
      "Accuracy:  0.8235\n",
      "Loss: 0.696980224609375\n",
      "Accuracy:  0.829\n",
      "Loss: 0.6908856811523437\n",
      "Accuracy:  0.8315\n",
      "Loss: 0.6627822265625\n",
      "Accuracy:  0.8405\n",
      "Loss: 0.66991015625\n",
      "Accuracy:  0.8385\n",
      "Loss: 0.6556932983398438\n",
      "Accuracy:  0.8465\n",
      "Loss: 0.7405430297851563\n",
      "Accuracy:  0.827\n",
      "Loss: 0.6975263061523438\n",
      "Accuracy:  0.827\n",
      "Loss: 0.7192466430664063\n",
      "Accuracy:  0.8285\n",
      "Loss: 0.6131796264648437\n",
      "Accuracy:  0.853\n",
      "Loss: 0.6629373168945313\n",
      "Accuracy:  0.84\n",
      "Loss: 0.6367461547851563\n",
      "Accuracy:  0.848\n",
      "Loss: 0.6724187622070312\n",
      "Accuracy:  0.837\n",
      "Loss: 0.7247479858398438\n",
      "Accuracy:  0.825\n",
      "Loss: 0.7242349853515625\n",
      "Accuracy:  0.8265\n",
      "Loss: 0.73492919921875\n",
      "Accuracy:  0.8245\n",
      "Loss: 0.6269891967773438\n",
      "Accuracy:  0.85\n",
      "Loss: 0.7070216064453125\n",
      "Accuracy:  0.8315\n",
      "Loss: 0.6200660400390625\n",
      "Accuracy:  0.846\n",
      "Loss: 0.70928662109375\n",
      "Accuracy:  0.83\n",
      "Loss: 0.7100064086914063\n",
      "Accuracy:  0.8315\n",
      "Loss: 0.677503662109375\n",
      "Accuracy:  0.836\n",
      "Loss: 0.6701398315429687\n",
      "Accuracy:  0.8385\n",
      "Loss: 0.6214237670898437\n",
      "Accuracy:  0.85\n",
      "Loss: 0.77355712890625\n",
      "Accuracy:  0.816\n",
      "Loss: 0.6627265625\n",
      "Accuracy:  0.8315\n",
      "Loss: 0.6616165771484375\n",
      "Accuracy:  0.841\n",
      "Loss: 0.6680950317382812\n",
      "Accuracy:  0.8445\n",
      "Loss: 0.6723741455078125\n",
      "Accuracy:  0.843\n",
      "Loss: 0.6746043090820313\n",
      "Accuracy:  0.8315\n",
      "Loss: 0.736784912109375\n",
      "Accuracy:  0.819\n",
      "Loss: 0.6562805786132813\n",
      "Accuracy:  0.839\n",
      "Loss: 0.668204833984375\n",
      "Accuracy:  0.8385\n",
      "Loss: 0.7075591430664062\n",
      "Accuracy:  0.827\n",
      "Loss: 0.6573984375\n",
      "Accuracy:  0.835\n",
      "Loss: 0.6738642578125\n",
      "Accuracy:  0.8345\n",
      "Loss: 0.6729304809570312\n",
      "Accuracy:  0.8395\n",
      "Loss: 0.7634898071289062\n",
      "Accuracy:  0.8095\n",
      "Loss: 0.7719132690429688\n",
      "Accuracy:  0.811\n",
      "Loss: 0.720614013671875\n",
      "Accuracy:  0.8225\n",
      "Loss: 0.7645921630859375\n",
      "Accuracy:  0.8165\n",
      "Loss: 0.7229381713867188\n",
      "Accuracy:  0.826\n",
      "Loss: 0.723763427734375\n",
      "Accuracy:  0.8225\n",
      "Loss: 0.745626220703125\n",
      "Accuracy:  0.8185\n",
      "Loss: 0.6784824829101562\n",
      "Accuracy:  0.841\n",
      "Loss: 0.6956614379882813\n",
      "Accuracy:  0.8305\n",
      "Loss: 0.7029508056640625\n",
      "Accuracy:  0.828\n",
      "Loss: 0.6982047729492188\n",
      "Accuracy:  0.8285\n",
      "Loss: 0.7480406494140625\n",
      "Accuracy:  0.825\n",
      "Loss: 0.729787353515625\n",
      "Accuracy:  0.8205\n",
      "Loss: 0.7436348266601562\n",
      "Accuracy:  0.8215\n",
      "Loss: 0.7157463989257813\n",
      "Accuracy:  0.8265\n",
      "Loss: 0.6797742919921875\n",
      "Accuracy:  0.8415\n",
      "Loss: 0.6926356811523438\n",
      "Accuracy:  0.8315\n",
      "Loss: 0.7129979858398438\n",
      "Accuracy:  0.8275\n",
      "Loss: 0.6965512084960938\n",
      "Accuracy:  0.828\n",
      "Loss: 0.69122705078125\n",
      "Accuracy:  0.833\n",
      "Loss: 0.6809984130859374\n",
      "Accuracy:  0.8345\n",
      "Loss: 0.740402656265452\n",
      "Accuracy:  0.810126582278481\n",
      "74.53110116732732\n",
      "0.018291706613950263\n",
      "Train: wpb=0, bsz=1967, num_updates=2280\n",
      "Loss: 0.6272015380859375\n",
      "Accuracy:  0.8495\n",
      "Loss: 0.7124871826171875\n",
      "Accuracy:  0.8275\n",
      "Loss: 0.6941303100585937\n",
      "Accuracy:  0.83\n",
      "Loss: 0.6221625366210938\n",
      "Accuracy:  0.8465\n",
      "Loss: 0.6492145385742187\n",
      "Accuracy:  0.8445\n",
      "Loss: 0.6933839111328125\n",
      "Accuracy:  0.832\n",
      "Loss: 0.6921320190429687\n",
      "Accuracy:  0.834\n",
      "Loss: 0.7178292846679688\n",
      "Accuracy:  0.823\n",
      "Loss: 0.6925807495117188\n",
      "Accuracy:  0.8325\n",
      "Loss: 0.6741088256835938\n",
      "Accuracy:  0.834\n",
      "Loss: 0.670703857421875\n",
      "Accuracy:  0.8385\n",
      "Loss: 0.6508600463867188\n",
      "Accuracy:  0.842\n",
      "Loss: 0.7270015869140625\n",
      "Accuracy:  0.825\n",
      "Loss: 0.636810302734375\n",
      "Accuracy:  0.8405\n",
      "Loss: 0.7439769897460937\n",
      "Accuracy:  0.818\n",
      "Loss: 0.6639408569335937\n",
      "Accuracy:  0.835\n",
      "Loss: 0.7218404541015625\n",
      "Accuracy:  0.8285\n",
      "Loss: 0.71404052734375\n",
      "Accuracy:  0.8245\n",
      "Loss: 0.6792787475585937\n",
      "Accuracy:  0.8385\n",
      "Loss: 0.6400595703125\n",
      "Accuracy:  0.843\n",
      "Loss: 0.6981068725585937\n",
      "Accuracy:  0.8315\n",
      "Loss: 0.7361063232421875\n",
      "Accuracy:  0.825\n",
      "Loss: 0.6736591796875\n",
      "Accuracy:  0.833\n",
      "Loss: 0.6501819458007813\n",
      "Accuracy:  0.841\n",
      "Loss: 0.7260675048828125\n",
      "Accuracy:  0.824\n",
      "Loss: 0.725600830078125\n",
      "Accuracy:  0.8255\n",
      "Loss: 0.6607388916015625\n",
      "Accuracy:  0.8435\n",
      "Loss: 0.6793453369140625\n",
      "Accuracy:  0.832\n",
      "Loss: 0.6995906982421874\n",
      "Accuracy:  0.826\n",
      "Loss: 0.6838300170898437\n",
      "Accuracy:  0.8315\n",
      "Loss: 0.6880809326171875\n",
      "Accuracy:  0.835\n",
      "Loss: 0.7035316162109375\n",
      "Accuracy:  0.828\n",
      "Loss: 0.696825927734375\n",
      "Accuracy:  0.833\n",
      "Loss: 0.6924105834960937\n",
      "Accuracy:  0.8265\n",
      "Loss: 0.7195484008789063\n",
      "Accuracy:  0.828\n",
      "Loss: 0.7208635864257813\n",
      "Accuracy:  0.826\n",
      "Loss: 0.7291921997070312\n",
      "Accuracy:  0.828\n",
      "Loss: 0.6892207641601562\n",
      "Accuracy:  0.8335\n",
      "Loss: 0.6594349365234375\n",
      "Accuracy:  0.8415\n",
      "Loss: 0.7490599365234375\n",
      "Accuracy:  0.8265\n",
      "Loss: 0.7235991821289063\n",
      "Accuracy:  0.826\n",
      "Loss: 0.7167297973632812\n",
      "Accuracy:  0.828\n",
      "Loss: 0.6882887573242188\n",
      "Accuracy:  0.8295\n",
      "Loss: 0.7138305053710937\n",
      "Accuracy:  0.8295\n",
      "Loss: 0.7022545776367187\n",
      "Accuracy:  0.8295\n",
      "Loss: 0.7018917846679688\n",
      "Accuracy:  0.835\n",
      "Loss: 0.624210693359375\n",
      "Accuracy:  0.851\n",
      "Loss: 0.7048818969726562\n",
      "Accuracy:  0.8285\n",
      "Loss: 0.6605216064453125\n",
      "Accuracy:  0.8385\n",
      "Loss: 0.6591051025390625\n",
      "Accuracy:  0.8395\n",
      "Loss: 0.68523583984375\n",
      "Accuracy:  0.837\n",
      "Loss: 0.7373505859375\n",
      "Accuracy:  0.821\n",
      "Loss: 0.6294326171875\n",
      "Accuracy:  0.8405\n",
      "Loss: 0.625899169921875\n",
      "Accuracy:  0.8475\n",
      "Loss: 0.7039402465820312\n",
      "Accuracy:  0.832\n",
      "Loss: 0.6926094360351562\n",
      "Accuracy:  0.8275\n",
      "Loss: 0.63057958984375\n",
      "Accuracy:  0.844\n",
      "Loss: 0.679771484375\n",
      "Accuracy:  0.832\n",
      "Loss: 0.7092795553086679\n",
      "Accuracy:  0.8227848101265823\n",
      "74.75590548787306\n",
      "0.017642756498502565\n",
      "Train: wpb=0, bsz=1967, num_updates=2340\n",
      "Loss: 0.6820598754882813\n",
      "Accuracy:  0.84\n",
      "Loss: 0.6572492065429687\n",
      "Accuracy:  0.8305\n",
      "Loss: 0.7128314819335938\n",
      "Accuracy:  0.8275\n",
      "Loss: 0.7170166625976563\n",
      "Accuracy:  0.831\n",
      "Loss: 0.6150198974609375\n",
      "Accuracy:  0.845\n",
      "Loss: 0.7060008544921875\n",
      "Accuracy:  0.8285\n",
      "Loss: 0.6918244018554688\n",
      "Accuracy:  0.833\n",
      "Loss: 0.667053955078125\n",
      "Accuracy:  0.836\n",
      "Loss: 0.6794890747070312\n",
      "Accuracy:  0.833\n",
      "Loss: 0.62665478515625\n",
      "Accuracy:  0.8485\n",
      "Loss: 0.6818089599609375\n",
      "Accuracy:  0.835\n",
      "Loss: 0.6988411865234375\n",
      "Accuracy:  0.83\n",
      "Loss: 0.7027605590820313\n",
      "Accuracy:  0.8265\n",
      "Loss: 0.679760009765625\n",
      "Accuracy:  0.8385\n",
      "Loss: 0.7541766357421875\n",
      "Accuracy:  0.8135\n",
      "Loss: 0.6571783447265624\n",
      "Accuracy:  0.8425\n",
      "Loss: 0.6275787353515625\n",
      "Accuracy:  0.8475\n",
      "Loss: 0.7295799560546875\n",
      "Accuracy:  0.8155\n",
      "Loss: 0.7192607421875\n",
      "Accuracy:  0.8255\n",
      "Loss: 0.6892649536132812\n",
      "Accuracy:  0.827\n",
      "Loss: 0.6835728759765625\n",
      "Accuracy:  0.834\n",
      "Loss: 0.6974803466796875\n",
      "Accuracy:  0.8345\n",
      "Loss: 0.6268094482421875\n",
      "Accuracy:  0.8505\n",
      "Loss: 0.7028995971679688\n",
      "Accuracy:  0.833\n",
      "Loss: 0.6823404541015625\n",
      "Accuracy:  0.836\n",
      "Loss: 0.6378336791992187\n",
      "Accuracy:  0.8405\n",
      "Loss: 0.6510418090820312\n",
      "Accuracy:  0.841\n",
      "Loss: 0.6847681274414062\n",
      "Accuracy:  0.8315\n",
      "Loss: 0.6742279052734375\n",
      "Accuracy:  0.8405\n",
      "Loss: 0.6934533081054688\n",
      "Accuracy:  0.83\n",
      "Loss: 0.6974017944335937\n",
      "Accuracy:  0.829\n",
      "Loss: 0.6975668334960937\n",
      "Accuracy:  0.8305\n",
      "Loss: 0.7017645874023437\n",
      "Accuracy:  0.829\n",
      "Loss: 0.7419635620117188\n",
      "Accuracy:  0.8205\n",
      "Loss: 0.71869482421875\n",
      "Accuracy:  0.8255\n",
      "Loss: 0.6549864501953125\n",
      "Accuracy:  0.8435\n",
      "Loss: 0.6934981689453125\n",
      "Accuracy:  0.831\n",
      "Loss: 0.7543267211914062\n",
      "Accuracy:  0.8205\n",
      "Loss: 0.6484472045898437\n",
      "Accuracy:  0.8415\n",
      "Loss: 0.74625634765625\n",
      "Accuracy:  0.8165\n",
      "Loss: 0.6575416259765625\n",
      "Accuracy:  0.8365\n",
      "Loss: 0.678163818359375\n",
      "Accuracy:  0.8315\n",
      "Loss: 0.7050340576171875\n",
      "Accuracy:  0.8315\n",
      "Loss: 0.6520387573242188\n",
      "Accuracy:  0.847\n",
      "Loss: 0.6595661010742188\n",
      "Accuracy:  0.846\n",
      "Loss: 0.6854678344726562\n",
      "Accuracy:  0.842\n",
      "Loss: 0.6575512084960937\n",
      "Accuracy:  0.839\n",
      "Loss: 0.6596834106445313\n",
      "Accuracy:  0.8405\n",
      "Loss: 0.6804512939453125\n",
      "Accuracy:  0.835\n",
      "Loss: 0.6837791137695313\n",
      "Accuracy:  0.84\n",
      "Loss: 0.7121671142578125\n",
      "Accuracy:  0.8265\n",
      "Loss: 0.703882080078125\n",
      "Accuracy:  0.8305\n",
      "Loss: 0.72039013671875\n",
      "Accuracy:  0.825\n",
      "Loss: 0.6591039428710938\n",
      "Accuracy:  0.841\n",
      "Loss: 0.7385000610351562\n",
      "Accuracy:  0.815\n",
      "Loss: 0.6800322875976562\n",
      "Accuracy:  0.8385\n",
      "Loss: 0.6725096435546875\n",
      "Accuracy:  0.834\n",
      "Loss: 0.661838623046875\n",
      "Accuracy:  0.8395\n",
      "Loss: 0.6079611959336679\n",
      "Accuracy:  0.8734177215189873\n",
      "74.97118454593958\n",
      "0.01713072019424812\n",
      "Train: wpb=0, bsz=1967, num_updates=2400\n",
      "Loss: 0.65877001953125\n",
      "Accuracy:  0.841\n",
      "Loss: 0.649161865234375\n",
      "Accuracy:  0.842\n",
      "Loss: 0.6930145874023438\n",
      "Accuracy:  0.8325\n",
      "Loss: 0.6810211791992188\n",
      "Accuracy:  0.8335\n",
      "Loss: 0.6320781860351562\n",
      "Accuracy:  0.8495\n",
      "Loss: 0.682447509765625\n",
      "Accuracy:  0.8295\n",
      "Loss: 0.682977294921875\n",
      "Accuracy:  0.8375\n",
      "Loss: 0.6588905029296875\n",
      "Accuracy:  0.8415\n",
      "Loss: 0.6869400634765624\n",
      "Accuracy:  0.834\n",
      "Loss: 0.64485791015625\n",
      "Accuracy:  0.846\n",
      "Loss: 0.655051025390625\n",
      "Accuracy:  0.8425\n",
      "Loss: 0.6529962768554688\n",
      "Accuracy:  0.8455\n",
      "Loss: 0.7001472778320312\n",
      "Accuracy:  0.836\n",
      "Loss: 0.6332933349609375\n",
      "Accuracy:  0.8475\n",
      "Loss: 0.7209236450195312\n",
      "Accuracy:  0.8255\n",
      "Loss: 0.6849954833984375\n",
      "Accuracy:  0.8325\n",
      "Loss: 0.6558857421875\n",
      "Accuracy:  0.834\n",
      "Loss: 0.6935980834960938\n",
      "Accuracy:  0.828\n",
      "Loss: 0.6485305786132812\n",
      "Accuracy:  0.842\n",
      "Loss: 0.6749247436523438\n",
      "Accuracy:  0.8375\n",
      "Loss: 0.7038052368164063\n",
      "Accuracy:  0.823\n",
      "Loss: 0.6734934692382812\n",
      "Accuracy:  0.8355\n",
      "Loss: 0.6715606689453125\n",
      "Accuracy:  0.838\n",
      "Loss: 0.7172029418945313\n",
      "Accuracy:  0.827\n",
      "Loss: 0.651984619140625\n",
      "Accuracy:  0.8465\n",
      "Loss: 0.6961915283203125\n",
      "Accuracy:  0.83\n",
      "Loss: 0.6509927368164062\n",
      "Accuracy:  0.8485\n",
      "Loss: 0.7584345092773438\n",
      "Accuracy:  0.8195\n",
      "Loss: 0.6744556884765625\n",
      "Accuracy:  0.837\n",
      "Loss: 0.687212646484375\n",
      "Accuracy:  0.8365\n",
      "Loss: 0.6900741577148437\n",
      "Accuracy:  0.8335\n",
      "Loss: 0.6964803466796875\n",
      "Accuracy:  0.824\n",
      "Loss: 0.6755196533203125\n",
      "Accuracy:  0.8435\n",
      "Loss: 0.7407367553710937\n",
      "Accuracy:  0.8265\n",
      "Loss: 0.7167118530273437\n",
      "Accuracy:  0.829\n",
      "Loss: 0.6449688720703125\n",
      "Accuracy:  0.841\n",
      "Loss: 0.6857227783203125\n",
      "Accuracy:  0.8325\n",
      "Loss: 0.7294024047851563\n",
      "Accuracy:  0.818\n",
      "Loss: 0.667521484375\n",
      "Accuracy:  0.839\n",
      "Loss: 0.700181640625\n",
      "Accuracy:  0.833\n",
      "Loss: 0.725406982421875\n",
      "Accuracy:  0.8205\n",
      "Loss: 0.6867987670898438\n",
      "Accuracy:  0.831\n",
      "Loss: 0.692578369140625\n",
      "Accuracy:  0.828\n",
      "Loss: 0.6838960571289062\n",
      "Accuracy:  0.8355\n",
      "Loss: 0.6712147827148438\n",
      "Accuracy:  0.8345\n",
      "Loss: 0.6604898071289063\n",
      "Accuracy:  0.835\n",
      "Loss: 0.6175104370117187\n",
      "Accuracy:  0.8505\n",
      "Loss: 0.626744384765625\n",
      "Accuracy:  0.8425\n",
      "Loss: 0.6565618896484375\n",
      "Accuracy:  0.844\n",
      "Loss: 0.721800537109375\n",
      "Accuracy:  0.827\n",
      "Loss: 0.7072491455078125\n",
      "Accuracy:  0.8205\n",
      "Loss: 0.6921367797851562\n",
      "Accuracy:  0.8355\n",
      "Loss: 0.7311309814453125\n",
      "Accuracy:  0.8245\n",
      "Loss: 0.7001966552734376\n",
      "Accuracy:  0.8315\n",
      "Loss: 0.6418626708984375\n",
      "Accuracy:  0.841\n",
      "Loss: 0.661377197265625\n",
      "Accuracy:  0.8415\n",
      "Loss: 0.6815650634765625\n",
      "Accuracy:  0.8405\n",
      "Loss: 0.7107105102539063\n",
      "Accuracy:  0.8225\n",
      "Loss: 0.6403705258912678\n",
      "Accuracy:  0.8227848101265823\n",
      "75.17949434018854\n",
      "0.016582744217570125\n",
      "Train: wpb=0, bsz=1967, num_updates=2460\n",
      "Loss: 0.6599359130859375\n",
      "Accuracy:  0.8425\n",
      "Loss: 0.6749666748046875\n",
      "Accuracy:  0.8325\n",
      "Loss: 0.71974951171875\n",
      "Accuracy:  0.8285\n",
      "Loss: 0.7227501220703125\n",
      "Accuracy:  0.8255\n",
      "Loss: 0.6475130004882812\n",
      "Accuracy:  0.8415\n",
      "Loss: 0.6445407104492188\n",
      "Accuracy:  0.8465\n",
      "Loss: 0.6681859741210937\n",
      "Accuracy:  0.838\n",
      "Loss: 0.67511669921875\n",
      "Accuracy:  0.841\n",
      "Loss: 0.6684896240234375\n",
      "Accuracy:  0.841\n",
      "Loss: 0.6677488403320313\n",
      "Accuracy:  0.8375\n",
      "Loss: 0.6964596557617188\n",
      "Accuracy:  0.83\n",
      "Loss: 0.7000595703125\n",
      "Accuracy:  0.8265\n",
      "Loss: 0.6402999267578126\n",
      "Accuracy:  0.8435\n",
      "Loss: 0.6275519409179687\n",
      "Accuracy:  0.842\n",
      "Loss: 0.6751796875\n",
      "Accuracy:  0.84\n",
      "Loss: 0.6879129638671875\n",
      "Accuracy:  0.8295\n",
      "Loss: 0.6742623291015625\n",
      "Accuracy:  0.833\n",
      "Loss: 0.6717391357421875\n",
      "Accuracy:  0.8425\n",
      "Loss: 0.63959765625\n",
      "Accuracy:  0.844\n",
      "Loss: 0.6229873657226562\n",
      "Accuracy:  0.8415\n",
      "Loss: 0.70300390625\n",
      "Accuracy:  0.8255\n",
      "Loss: 0.6618015747070313\n",
      "Accuracy:  0.8375\n",
      "Loss: 0.65452880859375\n",
      "Accuracy:  0.8445\n",
      "Loss: 0.7119667358398437\n",
      "Accuracy:  0.8285\n",
      "Loss: 0.6707462158203125\n",
      "Accuracy:  0.8385\n",
      "Loss: 0.7180880126953125\n",
      "Accuracy:  0.828\n",
      "Loss: 0.6679815673828124\n",
      "Accuracy:  0.841\n",
      "Loss: 0.6347606811523437\n",
      "Accuracy:  0.846\n",
      "Loss: 0.670480712890625\n",
      "Accuracy:  0.8415\n",
      "Loss: 0.6755595703125\n",
      "Accuracy:  0.836\n",
      "Loss: 0.619629638671875\n",
      "Accuracy:  0.845\n",
      "Loss: 0.7211317749023437\n",
      "Accuracy:  0.8285\n",
      "Loss: 0.6610468139648438\n",
      "Accuracy:  0.842\n",
      "Loss: 0.6752549438476563\n",
      "Accuracy:  0.8425\n",
      "Loss: 0.74099609375\n",
      "Accuracy:  0.8205\n",
      "Loss: 0.6511652221679688\n",
      "Accuracy:  0.8425\n",
      "Loss: 0.640660400390625\n",
      "Accuracy:  0.841\n",
      "Loss: 0.721799072265625\n",
      "Accuracy:  0.8265\n",
      "Loss: 0.6520671997070312\n",
      "Accuracy:  0.843\n",
      "Loss: 0.6607615356445312\n",
      "Accuracy:  0.8345\n",
      "Loss: 0.643797119140625\n",
      "Accuracy:  0.845\n",
      "Loss: 0.7298396606445312\n",
      "Accuracy:  0.8185\n",
      "Loss: 0.64983984375\n",
      "Accuracy:  0.8425\n",
      "Loss: 0.6376844482421875\n",
      "Accuracy:  0.845\n",
      "Loss: 0.6613485717773437\n",
      "Accuracy:  0.838\n",
      "Loss: 0.7368553466796876\n",
      "Accuracy:  0.819\n",
      "Loss: 0.669906494140625\n",
      "Accuracy:  0.8425\n",
      "Loss: 0.7113665771484375\n",
      "Accuracy:  0.8305\n",
      "Loss: 0.7034828491210937\n",
      "Accuracy:  0.827\n",
      "Loss: 0.6595770874023438\n",
      "Accuracy:  0.8375\n",
      "Loss: 0.6806671142578125\n",
      "Accuracy:  0.837\n",
      "Loss: 0.7319042358398438\n",
      "Accuracy:  0.823\n",
      "Loss: 0.6857330932617187\n",
      "Accuracy:  0.8275\n",
      "Loss: 0.688884765625\n",
      "Accuracy:  0.831\n",
      "Loss: 0.6945291748046875\n",
      "Accuracy:  0.836\n",
      "Loss: 0.7318859252929687\n",
      "Accuracy:  0.8205\n",
      "Loss: 0.7175226440429687\n",
      "Accuracy:  0.823\n",
      "Loss: 0.6875902099609374\n",
      "Accuracy:  0.8325\n",
      "Loss: 0.9694135641749901\n",
      "Accuracy:  0.810126582278481\n",
      "75.37881216731817\n",
      "0.016140724861619073\n",
      "Train: wpb=0, bsz=1967, num_updates=2520\n",
      "Loss: 0.66172021484375\n",
      "Accuracy:  0.842\n",
      "Loss: 0.667664794921875\n",
      "Accuracy:  0.841\n",
      "Loss: 0.7200132446289063\n",
      "Accuracy:  0.828\n",
      "Loss: 0.5933261108398438\n",
      "Accuracy:  0.8555\n",
      "Loss: 0.6652411499023437\n",
      "Accuracy:  0.836\n",
      "Loss: 0.6890576782226563\n",
      "Accuracy:  0.832\n",
      "Loss: 0.6756753540039062\n",
      "Accuracy:  0.8345\n",
      "Loss: 0.7177255249023438\n",
      "Accuracy:  0.83\n",
      "Loss: 0.6783047485351562\n",
      "Accuracy:  0.836\n",
      "Loss: 0.6178964233398437\n",
      "Accuracy:  0.8515\n",
      "Loss: 0.6676048583984375\n",
      "Accuracy:  0.8375\n",
      "Loss: 0.6622455444335937\n",
      "Accuracy:  0.84\n",
      "Loss: 0.6424734497070312\n",
      "Accuracy:  0.847\n",
      "Loss: 0.7307855834960938\n",
      "Accuracy:  0.8235\n",
      "Loss: 0.7719764404296875\n",
      "Accuracy:  0.8105\n",
      "Loss: 0.64910107421875\n",
      "Accuracy:  0.842\n",
      "Loss: 0.7049244384765625\n",
      "Accuracy:  0.8255\n",
      "Loss: 0.672498291015625\n",
      "Accuracy:  0.8395\n",
      "Loss: 0.69955712890625\n",
      "Accuracy:  0.8295\n",
      "Loss: 0.6279181518554687\n",
      "Accuracy:  0.8515\n",
      "Loss: 0.6705623779296875\n",
      "Accuracy:  0.835\n",
      "Loss: 0.6710260620117188\n",
      "Accuracy:  0.839\n",
      "Loss: 0.6244320678710937\n",
      "Accuracy:  0.843\n",
      "Loss: 0.6170657348632812\n",
      "Accuracy:  0.8415\n",
      "Loss: 0.6896292114257813\n",
      "Accuracy:  0.8305\n",
      "Loss: 0.5964900512695313\n",
      "Accuracy:  0.8565\n",
      "Loss: 0.6552684936523437\n",
      "Accuracy:  0.838\n",
      "Loss: 0.6229425659179687\n",
      "Accuracy:  0.8505\n",
      "Loss: 0.6474927368164063\n",
      "Accuracy:  0.8425\n",
      "Loss: 0.6874569091796875\n",
      "Accuracy:  0.831\n",
      "Loss: 0.6469653930664062\n",
      "Accuracy:  0.844\n",
      "Loss: 0.6787367553710938\n",
      "Accuracy:  0.8365\n",
      "Loss: 0.6911326904296875\n",
      "Accuracy:  0.8325\n",
      "Loss: 0.66618701171875\n",
      "Accuracy:  0.8415\n",
      "Loss: 0.72412158203125\n",
      "Accuracy:  0.8265\n",
      "Loss: 0.63734814453125\n",
      "Accuracy:  0.844\n",
      "Loss: 0.6790565795898438\n",
      "Accuracy:  0.836\n",
      "Loss: 0.6422637939453125\n",
      "Accuracy:  0.844\n",
      "Loss: 0.64616015625\n",
      "Accuracy:  0.8465\n",
      "Loss: 0.657191162109375\n",
      "Accuracy:  0.8445\n",
      "Loss: 0.7131165771484375\n",
      "Accuracy:  0.8265\n",
      "Loss: 0.7528885498046874\n",
      "Accuracy:  0.8185\n",
      "Loss: 0.645141845703125\n",
      "Accuracy:  0.8355\n",
      "Loss: 0.7012716674804688\n",
      "Accuracy:  0.833\n",
      "Loss: 0.6882079467773438\n",
      "Accuracy:  0.835\n",
      "Loss: 0.67235546875\n",
      "Accuracy:  0.8325\n",
      "Loss: 0.70426123046875\n",
      "Accuracy:  0.825\n",
      "Loss: 0.7051697387695313\n",
      "Accuracy:  0.827\n",
      "Loss: 0.7204758911132813\n",
      "Accuracy:  0.825\n",
      "Loss: 0.6988063354492188\n",
      "Accuracy:  0.837\n",
      "Loss: 0.6336763916015625\n",
      "Accuracy:  0.8415\n",
      "Loss: 0.6905259399414062\n",
      "Accuracy:  0.8305\n",
      "Loss: 0.6984249877929688\n",
      "Accuracy:  0.833\n",
      "Loss: 0.6229053344726563\n",
      "Accuracy:  0.851\n",
      "Loss: 0.668155029296875\n",
      "Accuracy:  0.8345\n",
      "Loss: 0.6658980712890625\n",
      "Accuracy:  0.835\n",
      "Loss: 0.6909307250976563\n",
      "Accuracy:  0.829\n",
      "Loss: 0.7244614868164062\n",
      "Accuracy:  0.8225\n",
      "Loss: 0.4930765176121193\n",
      "Accuracy:  0.8607594936708861\n",
      "75.57067134990626\n",
      "0.01566753988622405\n",
      "Train: wpb=0, bsz=1967, num_updates=2580\n",
      "Loss: 0.6656499633789063\n",
      "Accuracy:  0.8385\n",
      "Loss: 0.62678857421875\n",
      "Accuracy:  0.848\n",
      "Loss: 0.667432861328125\n",
      "Accuracy:  0.8385\n",
      "Loss: 0.6795628662109375\n",
      "Accuracy:  0.8355\n",
      "Loss: 0.6774537963867188\n",
      "Accuracy:  0.8345\n",
      "Loss: 0.76152880859375\n",
      "Accuracy:  0.814\n",
      "Loss: 0.6850982055664062\n",
      "Accuracy:  0.8345\n",
      "Loss: 0.6402556762695313\n",
      "Accuracy:  0.847\n",
      "Loss: 0.6904387817382812\n",
      "Accuracy:  0.833\n",
      "Loss: 0.6348297119140625\n",
      "Accuracy:  0.847\n",
      "Loss: 0.64758154296875\n",
      "Accuracy:  0.843\n",
      "Loss: 0.69327587890625\n",
      "Accuracy:  0.832\n",
      "Loss: 0.6711875\n",
      "Accuracy:  0.835\n",
      "Loss: 0.647244384765625\n",
      "Accuracy:  0.839\n",
      "Loss: 0.6415902709960938\n",
      "Accuracy:  0.841\n",
      "Loss: 0.6491102294921876\n",
      "Accuracy:  0.8415\n",
      "Loss: 0.6545398559570312\n",
      "Accuracy:  0.8435\n",
      "Loss: 0.66466015625\n",
      "Accuracy:  0.838\n",
      "Loss: 0.71363916015625\n",
      "Accuracy:  0.824\n",
      "Loss: 0.6502887573242188\n",
      "Accuracy:  0.84\n",
      "Loss: 0.6384313354492187\n",
      "Accuracy:  0.8515\n",
      "Loss: 0.7253760986328125\n",
      "Accuracy:  0.8205\n",
      "Loss: 0.6144057006835938\n",
      "Accuracy:  0.8525\n",
      "Loss: 0.6838344116210937\n",
      "Accuracy:  0.833\n",
      "Loss: 0.7215479125976563\n",
      "Accuracy:  0.8285\n",
      "Loss: 0.68071142578125\n",
      "Accuracy:  0.8365\n",
      "Loss: 0.6781998291015625\n",
      "Accuracy:  0.8345\n",
      "Loss: 0.6523564453125\n",
      "Accuracy:  0.8455\n",
      "Loss: 0.6599952392578124\n",
      "Accuracy:  0.8365\n",
      "Loss: 0.6677648315429687\n",
      "Accuracy:  0.84\n",
      "Loss: 0.6953577270507812\n",
      "Accuracy:  0.832\n",
      "Loss: 0.6644376220703125\n",
      "Accuracy:  0.8455\n",
      "Loss: 0.672693115234375\n",
      "Accuracy:  0.8435\n",
      "Loss: 0.6594284057617188\n",
      "Accuracy:  0.8435\n",
      "Loss: 0.633711181640625\n",
      "Accuracy:  0.841\n",
      "Loss: 0.6691967163085938\n",
      "Accuracy:  0.835\n",
      "Loss: 0.6552871704101563\n",
      "Accuracy:  0.8395\n",
      "Loss: 0.6689508056640625\n",
      "Accuracy:  0.839\n",
      "Loss: 0.6658651123046875\n",
      "Accuracy:  0.837\n",
      "Loss: 0.7151376342773438\n",
      "Accuracy:  0.831\n",
      "Loss: 0.6910150756835938\n",
      "Accuracy:  0.834\n",
      "Loss: 0.6878245849609375\n",
      "Accuracy:  0.8335\n",
      "Loss: 0.6806708984375\n",
      "Accuracy:  0.836\n",
      "Loss: 0.6102195434570312\n",
      "Accuracy:  0.852\n",
      "Loss: 0.6296220092773438\n",
      "Accuracy:  0.8475\n",
      "Loss: 0.7127274780273437\n",
      "Accuracy:  0.8265\n",
      "Loss: 0.6405625\n",
      "Accuracy:  0.8505\n",
      "Loss: 0.665900634765625\n",
      "Accuracy:  0.837\n",
      "Loss: 0.6942142944335937\n",
      "Accuracy:  0.829\n",
      "Loss: 0.6499124145507813\n",
      "Accuracy:  0.8425\n",
      "Loss: 0.6326543579101562\n",
      "Accuracy:  0.8475\n",
      "Loss: 0.7458207397460938\n",
      "Accuracy:  0.818\n",
      "Loss: 0.595586181640625\n",
      "Accuracy:  0.858\n",
      "Loss: 0.6682032470703125\n",
      "Accuracy:  0.8335\n",
      "Loss: 0.7173294677734375\n",
      "Accuracy:  0.8215\n",
      "Loss: 0.6612904663085938\n",
      "Accuracy:  0.8465\n",
      "Loss: 0.675544189453125\n",
      "Accuracy:  0.839\n",
      "Loss: 0.6298632202148438\n",
      "Accuracy:  0.8435\n",
      "Loss: 0.832928331592415\n",
      "Accuracy:  0.7721518987341772\n",
      "75.75777464855963\n",
      "0.015200695470420095\n",
      "Train: wpb=0, bsz=1967, num_updates=2640\n",
      "Loss: 0.745529541015625\n",
      "Accuracy:  0.818\n",
      "Loss: 0.7069508666992188\n",
      "Accuracy:  0.83\n",
      "Loss: 0.6166954345703125\n",
      "Accuracy:  0.8515\n",
      "Loss: 0.6353944091796875\n",
      "Accuracy:  0.847\n",
      "Loss: 0.6418840942382813\n",
      "Accuracy:  0.8415\n",
      "Loss: 0.6502198486328125\n",
      "Accuracy:  0.84\n",
      "Loss: 0.6968312377929687\n",
      "Accuracy:  0.8225\n",
      "Loss: 0.613612060546875\n",
      "Accuracy:  0.845\n",
      "Loss: 0.6377330932617188\n",
      "Accuracy:  0.844\n",
      "Loss: 0.696427734375\n",
      "Accuracy:  0.831\n",
      "Loss: 0.7079967041015625\n",
      "Accuracy:  0.828\n",
      "Loss: 0.690474365234375\n",
      "Accuracy:  0.831\n",
      "Loss: 0.6081475830078125\n",
      "Accuracy:  0.8555\n",
      "Loss: 0.6604508056640624\n",
      "Accuracy:  0.8425\n",
      "Loss: 0.6196738891601562\n",
      "Accuracy:  0.854\n",
      "Loss: 0.6421387939453125\n",
      "Accuracy:  0.8405\n",
      "Loss: 0.6320756225585937\n",
      "Accuracy:  0.848\n",
      "Loss: 0.648245361328125\n",
      "Accuracy:  0.844\n",
      "Loss: 0.5925763549804688\n",
      "Accuracy:  0.852\n",
      "Loss: 0.7070804443359375\n",
      "Accuracy:  0.827\n",
      "Loss: 0.6608572387695313\n",
      "Accuracy:  0.839\n",
      "Loss: 0.6351517944335937\n",
      "Accuracy:  0.839\n",
      "Loss: 0.6813517456054687\n",
      "Accuracy:  0.83\n",
      "Loss: 0.6843995971679687\n",
      "Accuracy:  0.831\n",
      "Loss: 0.6940861206054687\n",
      "Accuracy:  0.8315\n",
      "Loss: 0.69886328125\n",
      "Accuracy:  0.831\n",
      "Loss: 0.65677099609375\n",
      "Accuracy:  0.841\n",
      "Loss: 0.6715126953125\n",
      "Accuracy:  0.835\n",
      "Loss: 0.6222066650390625\n",
      "Accuracy:  0.85\n",
      "Loss: 0.6323802490234375\n",
      "Accuracy:  0.844\n",
      "Loss: 0.6781371459960938\n",
      "Accuracy:  0.8345\n",
      "Loss: 0.6236571044921875\n",
      "Accuracy:  0.8495\n",
      "Loss: 0.700175048828125\n",
      "Accuracy:  0.8285\n",
      "Loss: 0.6603551635742188\n",
      "Accuracy:  0.841\n",
      "Loss: 0.6627161254882813\n",
      "Accuracy:  0.8325\n",
      "Loss: 0.6482034912109375\n",
      "Accuracy:  0.843\n",
      "Loss: 0.6502117309570312\n",
      "Accuracy:  0.8445\n",
      "Loss: 0.6910385131835938\n",
      "Accuracy:  0.8315\n",
      "Loss: 0.680363525390625\n",
      "Accuracy:  0.829\n",
      "Loss: 0.7153046264648437\n",
      "Accuracy:  0.8235\n",
      "Loss: 0.6962574462890625\n",
      "Accuracy:  0.8295\n",
      "Loss: 0.6529147338867187\n",
      "Accuracy:  0.8395\n",
      "Loss: 0.7237807006835938\n",
      "Accuracy:  0.8325\n",
      "Loss: 0.7248342895507812\n",
      "Accuracy:  0.824\n",
      "Loss: 0.6493311767578125\n",
      "Accuracy:  0.8455\n",
      "Loss: 0.6282174682617188\n",
      "Accuracy:  0.8455\n",
      "Loss: 0.7037708129882813\n",
      "Accuracy:  0.8295\n",
      "Loss: 0.7159627075195313\n",
      "Accuracy:  0.825\n",
      "Loss: 0.7146315307617187\n",
      "Accuracy:  0.8235\n",
      "Loss: 0.6952150268554688\n",
      "Accuracy:  0.8275\n",
      "Loss: 0.6913194580078125\n",
      "Accuracy:  0.829\n",
      "Loss: 0.6662562866210937\n",
      "Accuracy:  0.835\n",
      "Loss: 0.707537109375\n",
      "Accuracy:  0.8355\n",
      "Loss: 0.6071192626953125\n",
      "Accuracy:  0.85\n",
      "Loss: 0.6699895629882813\n",
      "Accuracy:  0.836\n",
      "Loss: 0.6806912841796875\n",
      "Accuracy:  0.839\n",
      "Loss: 0.656012939453125\n",
      "Accuracy:  0.8405\n",
      "Loss: 0.6424445190429687\n",
      "Accuracy:  0.8435\n",
      "Loss: 0.7765712496600573\n",
      "Accuracy:  0.7974683544303798\n",
      "75.93456734709625\n",
      "0.01483401056434408\n",
      "Train: wpb=0, bsz=1967, num_updates=2700\n",
      "Loss: 0.640837158203125\n",
      "Accuracy:  0.8455\n",
      "Loss: 0.6462915649414063\n",
      "Accuracy:  0.8425\n",
      "Loss: 0.6503828125\n",
      "Accuracy:  0.8405\n",
      "Loss: 0.6321048583984376\n",
      "Accuracy:  0.839\n",
      "Loss: 0.630899658203125\n",
      "Accuracy:  0.843\n",
      "Loss: 0.68835009765625\n",
      "Accuracy:  0.8285\n",
      "Loss: 0.6216083984375\n",
      "Accuracy:  0.851\n",
      "Loss: 0.6297171020507812\n",
      "Accuracy:  0.8435\n",
      "Loss: 0.6777177124023438\n",
      "Accuracy:  0.833\n",
      "Loss: 0.6148020629882812\n",
      "Accuracy:  0.848\n",
      "Loss: 0.7117672729492187\n",
      "Accuracy:  0.8255\n",
      "Loss: 0.6131239013671875\n",
      "Accuracy:  0.8525\n",
      "Loss: 0.710480712890625\n",
      "Accuracy:  0.824\n",
      "Loss: 0.65235693359375\n",
      "Accuracy:  0.8405\n",
      "Loss: 0.6482239990234375\n",
      "Accuracy:  0.846\n",
      "Loss: 0.6370779418945313\n",
      "Accuracy:  0.846\n",
      "Loss: 0.6598402099609375\n",
      "Accuracy:  0.8455\n",
      "Loss: 0.6944517211914063\n",
      "Accuracy:  0.8315\n",
      "Loss: 0.6122124633789062\n",
      "Accuracy:  0.848\n",
      "Loss: 0.7060156860351563\n",
      "Accuracy:  0.829\n",
      "Loss: 0.6657291870117188\n",
      "Accuracy:  0.838\n",
      "Loss: 0.6090970458984375\n",
      "Accuracy:  0.8535\n",
      "Loss: 0.6291693725585937\n",
      "Accuracy:  0.8465\n",
      "Loss: 0.7094901123046875\n",
      "Accuracy:  0.83\n",
      "Loss: 0.683253173828125\n",
      "Accuracy:  0.8325\n",
      "Loss: 0.6773884887695313\n",
      "Accuracy:  0.8355\n",
      "Loss: 0.6354585571289062\n",
      "Accuracy:  0.8455\n",
      "Loss: 0.6077700805664062\n",
      "Accuracy:  0.85\n",
      "Loss: 0.62325146484375\n",
      "Accuracy:  0.851\n",
      "Loss: 0.66446826171875\n",
      "Accuracy:  0.8345\n",
      "Loss: 0.7650078735351562\n",
      "Accuracy:  0.819\n",
      "Loss: 0.6970946044921875\n",
      "Accuracy:  0.833\n",
      "Loss: 0.7395482177734375\n",
      "Accuracy:  0.815\n",
      "Loss: 0.677727294921875\n",
      "Accuracy:  0.8295\n",
      "Loss: 0.6987637329101563\n",
      "Accuracy:  0.829\n",
      "Loss: 0.7280303344726563\n",
      "Accuracy:  0.821\n",
      "Loss: 0.6755597534179687\n",
      "Accuracy:  0.8315\n",
      "Loss: 0.6501642456054687\n",
      "Accuracy:  0.842\n",
      "Loss: 0.6557036743164063\n",
      "Accuracy:  0.8425\n",
      "Loss: 0.653922607421875\n",
      "Accuracy:  0.845\n",
      "Loss: 0.6787835693359375\n",
      "Accuracy:  0.8335\n",
      "Loss: 0.6576171875\n",
      "Accuracy:  0.838\n",
      "Loss: 0.6611307983398438\n",
      "Accuracy:  0.837\n",
      "Loss: 0.6705477294921875\n",
      "Accuracy:  0.8375\n",
      "Loss: 0.6350487060546876\n",
      "Accuracy:  0.8485\n",
      "Loss: 0.6708348999023438\n",
      "Accuracy:  0.838\n",
      "Loss: 0.683182373046875\n",
      "Accuracy:  0.8345\n",
      "Loss: 0.6571793212890625\n",
      "Accuracy:  0.8395\n",
      "Loss: 0.632104736328125\n",
      "Accuracy:  0.847\n",
      "Loss: 0.691126708984375\n",
      "Accuracy:  0.835\n",
      "Loss: 0.6550074462890625\n",
      "Accuracy:  0.841\n",
      "Loss: 0.650522705078125\n",
      "Accuracy:  0.845\n",
      "Loss: 0.6739149780273438\n",
      "Accuracy:  0.839\n",
      "Loss: 0.6951978149414062\n",
      "Accuracy:  0.8325\n",
      "Loss: 0.6772964477539063\n",
      "Accuracy:  0.8365\n",
      "Loss: 0.7144057006835938\n",
      "Accuracy:  0.8245\n",
      "Loss: 0.7291922607421875\n",
      "Accuracy:  0.822\n",
      "Loss: 0.7051331176757812\n",
      "Accuracy:  0.822\n",
      "Loss: 0.868361074713212\n",
      "Accuracy:  0.8227848101265823\n",
      "76.10474122520037\n",
      "0.01448613446628246\n",
      "Train: wpb=0, bsz=1967, num_updates=2760\n",
      "Loss: 0.6297147216796875\n",
      "Accuracy:  0.8505\n",
      "Loss: 0.6856090698242188\n",
      "Accuracy:  0.834\n",
      "Loss: 0.6442766723632812\n",
      "Accuracy:  0.8405\n",
      "Loss: 0.68261279296875\n",
      "Accuracy:  0.8405\n",
      "Loss: 0.6103129272460938\n",
      "Accuracy:  0.8545\n",
      "Loss: 0.62546875\n",
      "Accuracy:  0.8485\n",
      "Loss: 0.6306286010742187\n",
      "Accuracy:  0.844\n",
      "Loss: 0.6829453125\n",
      "Accuracy:  0.8365\n",
      "Loss: 0.6156885375976563\n",
      "Accuracy:  0.851\n",
      "Loss: 0.6268739624023437\n",
      "Accuracy:  0.8465\n",
      "Loss: 0.6158907470703125\n",
      "Accuracy:  0.851\n",
      "Loss: 0.7290042114257812\n",
      "Accuracy:  0.826\n",
      "Loss: 0.6449686889648437\n",
      "Accuracy:  0.842\n",
      "Loss: 0.6119356689453125\n",
      "Accuracy:  0.8445\n",
      "Loss: 0.6526135864257813\n",
      "Accuracy:  0.8425\n",
      "Loss: 0.719545166015625\n",
      "Accuracy:  0.824\n",
      "Loss: 0.6917904663085938\n",
      "Accuracy:  0.8305\n",
      "Loss: 0.6932177734375\n",
      "Accuracy:  0.833\n",
      "Loss: 0.7379222412109375\n",
      "Accuracy:  0.8165\n",
      "Loss: 0.6925349731445313\n",
      "Accuracy:  0.8385\n",
      "Loss: 0.6295744018554688\n",
      "Accuracy:  0.846\n",
      "Loss: 0.6676893310546875\n",
      "Accuracy:  0.8415\n",
      "Loss: 0.6298863525390626\n",
      "Accuracy:  0.844\n",
      "Loss: 0.6483507080078125\n",
      "Accuracy:  0.842\n",
      "Loss: 0.6857496948242188\n",
      "Accuracy:  0.8315\n",
      "Loss: 0.6429526977539063\n",
      "Accuracy:  0.8395\n",
      "Loss: 0.6576263427734375\n",
      "Accuracy:  0.844\n",
      "Loss: 0.6351195678710938\n",
      "Accuracy:  0.8465\n",
      "Loss: 0.6953356323242188\n",
      "Accuracy:  0.834\n",
      "Loss: 0.6532976684570313\n",
      "Accuracy:  0.837\n",
      "Loss: 0.64079052734375\n",
      "Accuracy:  0.842\n",
      "Loss: 0.7101478271484375\n",
      "Accuracy:  0.8295\n",
      "Loss: 0.679545654296875\n",
      "Accuracy:  0.833\n",
      "Loss: 0.6414835205078125\n",
      "Accuracy:  0.8365\n",
      "Loss: 0.61737451171875\n",
      "Accuracy:  0.844\n",
      "Loss: 0.6654932250976563\n",
      "Accuracy:  0.845\n",
      "Loss: 0.7051626586914063\n",
      "Accuracy:  0.8235\n",
      "Loss: 0.65400830078125\n",
      "Accuracy:  0.842\n",
      "Loss: 0.6548900756835937\n",
      "Accuracy:  0.8415\n",
      "Loss: 0.6014063110351563\n",
      "Accuracy:  0.8545\n",
      "Loss: 0.6478156127929687\n",
      "Accuracy:  0.838\n",
      "Loss: 0.6762412109375\n",
      "Accuracy:  0.837\n",
      "Loss: 0.6921309204101562\n",
      "Accuracy:  0.833\n",
      "Loss: 0.666796630859375\n",
      "Accuracy:  0.836\n",
      "Loss: 0.735536376953125\n",
      "Accuracy:  0.8245\n",
      "Loss: 0.6513510131835938\n",
      "Accuracy:  0.8435\n",
      "Loss: 0.7005740356445312\n",
      "Accuracy:  0.8265\n",
      "Loss: 0.6888887329101563\n",
      "Accuracy:  0.832\n",
      "Loss: 0.6501482543945313\n",
      "Accuracy:  0.841\n",
      "Loss: 0.6274495849609375\n",
      "Accuracy:  0.8515\n",
      "Loss: 0.6331068115234375\n",
      "Accuracy:  0.8485\n",
      "Loss: 0.714415771484375\n",
      "Accuracy:  0.83\n",
      "Loss: 0.6794827880859375\n",
      "Accuracy:  0.838\n",
      "Loss: 0.674909423828125\n",
      "Accuracy:  0.838\n",
      "Loss: 0.6810941162109375\n",
      "Accuracy:  0.831\n",
      "Loss: 0.6726962280273437\n",
      "Accuracy:  0.836\n",
      "Loss: 0.7195948486328125\n",
      "Accuracy:  0.8285\n",
      "Loss: 0.6773101806640625\n",
      "Accuracy:  0.8365\n",
      "Loss: 0.8685323015044007\n",
      "Accuracy:  0.810126582278481\n",
      "76.26994404935895\n",
      "0.014130033523394621\n",
      "Train: wpb=0, bsz=1967, num_updates=2820\n",
      "Loss: 0.6581878662109375\n",
      "Accuracy:  0.847\n",
      "Loss: 0.6628073120117187\n",
      "Accuracy:  0.836\n",
      "Loss: 0.615587890625\n",
      "Accuracy:  0.854\n",
      "Loss: 0.7789911499023437\n",
      "Accuracy:  0.815\n",
      "Loss: 0.6165853881835938\n",
      "Accuracy:  0.8515\n",
      "Loss: 0.654305419921875\n",
      "Accuracy:  0.838\n",
      "Loss: 0.6575014038085938\n",
      "Accuracy:  0.835\n",
      "Loss: 0.6084102783203125\n",
      "Accuracy:  0.855\n",
      "Loss: 0.65976123046875\n",
      "Accuracy:  0.836\n",
      "Loss: 0.6063684692382812\n",
      "Accuracy:  0.852\n",
      "Loss: 0.5827401123046875\n",
      "Accuracy:  0.854\n",
      "Loss: 0.7199307861328125\n",
      "Accuracy:  0.8255\n",
      "Loss: 0.6841886596679687\n",
      "Accuracy:  0.835\n",
      "Loss: 0.7019647827148437\n",
      "Accuracy:  0.8295\n",
      "Loss: 0.7034764404296875\n",
      "Accuracy:  0.826\n",
      "Loss: 0.5957694091796875\n",
      "Accuracy:  0.851\n",
      "Loss: 0.6974454956054688\n",
      "Accuracy:  0.8345\n",
      "Loss: 0.7089887084960937\n",
      "Accuracy:  0.831\n",
      "Loss: 0.6068120727539063\n",
      "Accuracy:  0.854\n",
      "Loss: 0.6731973266601563\n",
      "Accuracy:  0.839\n",
      "Loss: 0.6869242553710937\n",
      "Accuracy:  0.832\n",
      "Loss: 0.6645562133789062\n",
      "Accuracy:  0.835\n",
      "Loss: 0.6361904907226562\n",
      "Accuracy:  0.84\n",
      "Loss: 0.6516000366210938\n",
      "Accuracy:  0.841\n",
      "Loss: 0.69491552734375\n",
      "Accuracy:  0.8355\n",
      "Loss: 0.6659955444335938\n",
      "Accuracy:  0.84\n",
      "Loss: 0.6488574829101562\n",
      "Accuracy:  0.843\n",
      "Loss: 0.7156870727539062\n",
      "Accuracy:  0.829\n",
      "Loss: 0.700381591796875\n",
      "Accuracy:  0.8345\n",
      "Loss: 0.6523370971679687\n",
      "Accuracy:  0.84\n",
      "Loss: 0.610462890625\n",
      "Accuracy:  0.851\n",
      "Loss: 0.6905745239257812\n",
      "Accuracy:  0.827\n",
      "Loss: 0.6725780639648438\n",
      "Accuracy:  0.838\n",
      "Loss: 0.7281758422851563\n",
      "Accuracy:  0.8195\n",
      "Loss: 0.6648195190429688\n",
      "Accuracy:  0.839\n",
      "Loss: 0.6252433471679687\n",
      "Accuracy:  0.845\n",
      "Loss: 0.6892967529296875\n",
      "Accuracy:  0.8275\n",
      "Loss: 0.6405865478515625\n",
      "Accuracy:  0.842\n",
      "Loss: 0.6958038330078125\n",
      "Accuracy:  0.8295\n",
      "Loss: 0.6197286987304688\n",
      "Accuracy:  0.85\n",
      "Loss: 0.6199424438476563\n",
      "Accuracy:  0.849\n",
      "Loss: 0.6544767456054688\n",
      "Accuracy:  0.844\n",
      "Loss: 0.611221923828125\n",
      "Accuracy:  0.854\n",
      "Loss: 0.6881334838867188\n",
      "Accuracy:  0.8365\n",
      "Loss: 0.6811829223632813\n",
      "Accuracy:  0.833\n",
      "Loss: 0.6265020141601563\n",
      "Accuracy:  0.8485\n",
      "Loss: 0.5756753540039062\n",
      "Accuracy:  0.8505\n",
      "Loss: 0.6798303833007813\n",
      "Accuracy:  0.837\n",
      "Loss: 0.6412079467773437\n",
      "Accuracy:  0.8425\n",
      "Loss: 0.67978369140625\n",
      "Accuracy:  0.8335\n",
      "Loss: 0.6706900024414062\n",
      "Accuracy:  0.834\n",
      "Loss: 0.6200053100585937\n",
      "Accuracy:  0.849\n",
      "Loss: 0.6889207153320313\n",
      "Accuracy:  0.8315\n",
      "Loss: 0.6743786010742188\n",
      "Accuracy:  0.8365\n",
      "Loss: 0.7273026123046875\n",
      "Accuracy:  0.824\n",
      "Loss: 0.7108598022460938\n",
      "Accuracy:  0.8255\n",
      "Loss: 0.6231539306640625\n",
      "Accuracy:  0.8535\n",
      "Loss: 0.6113970947265625\n",
      "Accuracy:  0.852\n",
      "Loss: 0.5458061121687104\n",
      "Accuracy:  0.8734177215189873\n",
      "76.42953375847243\n",
      "0.01375661263132972\n",
      "Train: wpb=0, bsz=1967, num_updates=2880\n",
      "Loss: 0.6521171264648438\n",
      "Accuracy:  0.841\n",
      "Loss: 0.70760498046875\n",
      "Accuracy:  0.827\n",
      "Loss: 0.6406182861328125\n",
      "Accuracy:  0.849\n",
      "Loss: 0.6753045043945313\n",
      "Accuracy:  0.8335\n",
      "Loss: 0.6255396118164063\n",
      "Accuracy:  0.8455\n",
      "Loss: 0.6103598022460938\n",
      "Accuracy:  0.85\n",
      "Loss: 0.6861859130859375\n",
      "Accuracy:  0.834\n",
      "Loss: 0.6324400024414063\n",
      "Accuracy:  0.848\n",
      "Loss: 0.708142578125\n",
      "Accuracy:  0.8235\n",
      "Loss: 0.6841480712890625\n",
      "Accuracy:  0.829\n",
      "Loss: 0.659031982421875\n",
      "Accuracy:  0.843\n",
      "Loss: 0.6680054321289063\n",
      "Accuracy:  0.839\n",
      "Loss: 0.6612816772460938\n",
      "Accuracy:  0.834\n",
      "Loss: 0.6384889526367188\n",
      "Accuracy:  0.8415\n",
      "Loss: 0.6581267700195312\n",
      "Accuracy:  0.8425\n",
      "Loss: 0.6330810546875\n",
      "Accuracy:  0.85\n",
      "Loss: 0.6332807006835938\n",
      "Accuracy:  0.848\n",
      "Loss: 0.670244140625\n",
      "Accuracy:  0.838\n",
      "Loss: 0.6615797119140625\n",
      "Accuracy:  0.839\n",
      "Loss: 0.6118890380859375\n",
      "Accuracy:  0.8525\n",
      "Loss: 0.6769635620117187\n",
      "Accuracy:  0.834\n",
      "Loss: 0.6618169555664063\n",
      "Accuracy:  0.837\n",
      "Loss: 0.65315283203125\n",
      "Accuracy:  0.842\n",
      "Loss: 0.6350211181640625\n",
      "Accuracy:  0.85\n",
      "Loss: 0.6614029541015625\n",
      "Accuracy:  0.84\n",
      "Loss: 0.6687680053710937\n",
      "Accuracy:  0.8365\n",
      "Loss: 0.68159130859375\n",
      "Accuracy:  0.8255\n",
      "Loss: 0.622626708984375\n",
      "Accuracy:  0.8415\n",
      "Loss: 0.6675834350585937\n",
      "Accuracy:  0.8375\n",
      "Loss: 0.5996475219726562\n",
      "Accuracy:  0.856\n",
      "Loss: 0.678259765625\n",
      "Accuracy:  0.836\n",
      "Loss: 0.663607421875\n",
      "Accuracy:  0.834\n",
      "Loss: 0.6735282592773437\n",
      "Accuracy:  0.836\n",
      "Loss: 0.6774573974609375\n",
      "Accuracy:  0.836\n",
      "Loss: 0.6894466552734375\n",
      "Accuracy:  0.8355\n",
      "Loss: 0.6625296630859375\n",
      "Accuracy:  0.841\n",
      "Loss: 0.6514180297851563\n",
      "Accuracy:  0.8415\n",
      "Loss: 0.6329059448242188\n",
      "Accuracy:  0.845\n",
      "Loss: 0.704490966796875\n",
      "Accuracy:  0.83\n",
      "Loss: 0.5910803833007813\n",
      "Accuracy:  0.855\n",
      "Loss: 0.642127197265625\n",
      "Accuracy:  0.838\n",
      "Loss: 0.6583416748046875\n",
      "Accuracy:  0.849\n",
      "Loss: 0.6618231811523437\n",
      "Accuracy:  0.8415\n",
      "Loss: 0.6726878051757812\n",
      "Accuracy:  0.839\n",
      "Loss: 0.6431301879882813\n",
      "Accuracy:  0.843\n",
      "Loss: 0.6755712890625\n",
      "Accuracy:  0.8335\n",
      "Loss: 0.6621441040039062\n",
      "Accuracy:  0.8375\n",
      "Loss: 0.6364562377929688\n",
      "Accuracy:  0.846\n",
      "Loss: 0.7017801513671875\n",
      "Accuracy:  0.83\n",
      "Loss: 0.66689208984375\n",
      "Accuracy:  0.8415\n",
      "Loss: 0.6363682861328125\n",
      "Accuracy:  0.843\n",
      "Loss: 0.6741588745117187\n",
      "Accuracy:  0.836\n",
      "Loss: 0.6509736938476562\n",
      "Accuracy:  0.84\n",
      "Loss: 0.6579595336914063\n",
      "Accuracy:  0.843\n",
      "Loss: 0.6516639404296874\n",
      "Accuracy:  0.8405\n",
      "Loss: 0.6555365600585937\n",
      "Accuracy:  0.838\n",
      "Loss: 0.6663466796875\n",
      "Accuracy:  0.841\n",
      "Loss: 0.7008812255859375\n",
      "Accuracy:  0.8265\n",
      "Loss: 0.8083427042900762\n",
      "Accuracy:  0.8227848101265823\n",
      "76.58413054836515\n",
      "0.013411020627706969\n",
      "Train: wpb=0, bsz=1967, num_updates=2940\n",
      "Loss: 0.6177702026367188\n",
      "Accuracy:  0.8555\n",
      "Loss: 0.6030701904296875\n",
      "Accuracy:  0.852\n",
      "Loss: 0.5862798461914063\n",
      "Accuracy:  0.857\n",
      "Loss: 0.684560302734375\n",
      "Accuracy:  0.8335\n",
      "Loss: 0.6456366577148438\n",
      "Accuracy:  0.8465\n",
      "Loss: 0.695390869140625\n",
      "Accuracy:  0.827\n",
      "Loss: 0.655971435546875\n",
      "Accuracy:  0.841\n",
      "Loss: 0.603921142578125\n",
      "Accuracy:  0.852\n",
      "Loss: 0.5886614379882813\n",
      "Accuracy:  0.855\n",
      "Loss: 0.6722451171875\n",
      "Accuracy:  0.8345\n",
      "Loss: 0.6689800415039062\n",
      "Accuracy:  0.833\n",
      "Loss: 0.5989217529296875\n",
      "Accuracy:  0.856\n",
      "Loss: 0.676135986328125\n",
      "Accuracy:  0.832\n",
      "Loss: 0.6262739868164062\n",
      "Accuracy:  0.84\n",
      "Loss: 0.6965626831054688\n",
      "Accuracy:  0.8245\n",
      "Loss: 0.6794893798828125\n",
      "Accuracy:  0.8335\n",
      "Loss: 0.6904866943359375\n",
      "Accuracy:  0.8355\n",
      "Loss: 0.703219970703125\n",
      "Accuracy:  0.835\n",
      "Loss: 0.6599572143554687\n",
      "Accuracy:  0.8405\n",
      "Loss: 0.6226465454101563\n",
      "Accuracy:  0.848\n",
      "Loss: 0.6247725830078125\n",
      "Accuracy:  0.8465\n",
      "Loss: 0.7071680297851562\n",
      "Accuracy:  0.825\n",
      "Loss: 0.6168132934570313\n",
      "Accuracy:  0.854\n",
      "Loss: 0.6738329467773437\n",
      "Accuracy:  0.8385\n",
      "Loss: 0.703886474609375\n",
      "Accuracy:  0.832\n",
      "Loss: 0.6447713012695313\n",
      "Accuracy:  0.8395\n",
      "Loss: 0.6366605224609375\n",
      "Accuracy:  0.8445\n",
      "Loss: 0.6355947875976562\n",
      "Accuracy:  0.844\n",
      "Loss: 0.6387151489257813\n",
      "Accuracy:  0.8455\n",
      "Loss: 0.70309228515625\n",
      "Accuracy:  0.83\n",
      "Loss: 0.6817976684570313\n",
      "Accuracy:  0.836\n",
      "Loss: 0.7106156005859375\n",
      "Accuracy:  0.8265\n",
      "Loss: 0.6751524658203125\n",
      "Accuracy:  0.8275\n",
      "Loss: 0.6943191528320313\n",
      "Accuracy:  0.8315\n",
      "Loss: 0.675708251953125\n",
      "Accuracy:  0.833\n",
      "Loss: 0.7031920776367188\n",
      "Accuracy:  0.8275\n",
      "Loss: 0.6575070190429687\n",
      "Accuracy:  0.8395\n",
      "Loss: 0.60011083984375\n",
      "Accuracy:  0.8545\n",
      "Loss: 0.7030053100585938\n",
      "Accuracy:  0.8335\n",
      "Loss: 0.7012166137695313\n",
      "Accuracy:  0.8275\n",
      "Loss: 0.5671893310546875\n",
      "Accuracy:  0.8625\n",
      "Loss: 0.684581787109375\n",
      "Accuracy:  0.834\n",
      "Loss: 0.66273828125\n",
      "Accuracy:  0.8355\n",
      "Loss: 0.6892973022460938\n",
      "Accuracy:  0.8325\n",
      "Loss: 0.6014774169921875\n",
      "Accuracy:  0.8515\n",
      "Loss: 0.6620150756835937\n",
      "Accuracy:  0.836\n",
      "Loss: 0.651051513671875\n",
      "Accuracy:  0.841\n",
      "Loss: 0.686360107421875\n",
      "Accuracy:  0.834\n",
      "Loss: 0.641823974609375\n",
      "Accuracy:  0.848\n",
      "Loss: 0.6367357177734375\n",
      "Accuracy:  0.8425\n",
      "Loss: 0.6271827392578125\n",
      "Accuracy:  0.848\n",
      "Loss: 0.6515726318359375\n",
      "Accuracy:  0.845\n",
      "Loss: 0.65705029296875\n",
      "Accuracy:  0.8385\n",
      "Loss: 0.6891378784179687\n",
      "Accuracy:  0.831\n",
      "Loss: 0.707896484375\n",
      "Accuracy:  0.829\n",
      "Loss: 0.6482117309570312\n",
      "Accuracy:  0.8375\n",
      "Loss: 0.6036434326171874\n",
      "Accuracy:  0.8525\n",
      "Loss: 0.63307861328125\n",
      "Accuracy:  0.8495\n",
      "Loss: 0.7619123821017109\n",
      "Accuracy:  0.810126582278481\n",
      "76.73242490197241\n",
      "0.01312113637522132\n",
      "Train: wpb=0, bsz=1967, num_updates=3000\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "x_train = torch.from_numpy(X_train).long()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.999), eps=1e-08, weight_decay=4e-5)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "y_train = torch.from_numpy(Y_train).long()\n",
    "dataset_train = TensorDataset(x_train, y_train)\n",
    "#train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "train_loader = DataLoader(dataset_train, batch_size=2000, shuffle=True, num_workers=4, pin_memory=False)\n",
    "model.train()\n",
    "total_loss = 0\n",
    "ncorrect = 0\n",
    "nsentences = 0\n",
    "ntokens = 0\n",
    "niterations = 0\n",
    "for epoch in range(50):\n",
    "    for _i,(inputs, labels) in enumerate(train_loader):\n",
    "        # Get input and target sequences from batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        #labels = torch.tensor([d for d in labels], dtype=torch.long, device='cuda')\n",
    "        preds, _ = model(inputs)\n",
    "        preds = preds.squeeze(1)\n",
    "        loss = criterion(preds, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        '''\n",
    "        for param in model.parameters():\n",
    "            print(param.grad.data.sum())\n",
    "        '''\n",
    "        # Training statistics\n",
    "        total_loss += loss.item()\n",
    "        ncorrect += (torch.max(preds, 1)[1] == labels).sum().item()\n",
    "        nsentences += labels.numel()\n",
    "        niterations += 1\n",
    "        if _i%500:\n",
    "            print(\"Loss:\", loss.item()/labels.numel())\n",
    "            print(\"Accuracy: \",(torch.max(preds, 1)[1] == labels).sum().item()/labels.numel())\n",
    "    total_loss = total_loss / nsentences\n",
    "    accuracy = 100 * ncorrect / nsentences\n",
    "    print(accuracy)\n",
    "    print(total_loss)\n",
    "    print(f'Train: wpb={ntokens//niterations}, bsz={nsentences//niterations}, num_updates={niterations}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
