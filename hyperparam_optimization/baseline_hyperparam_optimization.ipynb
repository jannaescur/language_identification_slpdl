{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['labels.csv', 'x_train.txt', 'y_train.txt', 'x_test.txt']\n",
      "Example:\n",
      "LANG = est\n",
      "TEXT = Klement Gottwaldi surnukeha palsameeriti ning paigutati mausoleumi. Surnukeha oli aga liiga hilja ja oskamatult palsameeritud ning hakkas ilmutama lagunemise tundemärke. 1962. aastal viidi ta surnukeha mausoleumist ära ja kremeeriti. Zlíni linn kandis aastatel 1949–1989 nime Gottwaldov. Ukrainas Harkivi oblastis kandis Zmiivi linn aastatel 1976–1990 nime Gotvald.\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "import random\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch # Deep learning framework\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "#Init random seed to get reproducible results\n",
    "seed = 1111\n",
    "random.seed(seed)\n",
    "np.random.RandomState(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "x_train_full = open(\"../input/x_train.txt\").read().splitlines()\n",
    "y_train_full = open(\"../input/y_train.txt\").read().splitlines()\n",
    "print('Example:')\n",
    "print('LANG =', y_train_full[0])\n",
    "print('TEXT =', x_train_full[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.token2idx = {}\n",
    "        self.idx2token = []\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token not in self.token2idx:\n",
    "            self.idx2token.append(token)\n",
    "            self.token2idx[token] = len(self.idx2token) - 1\n",
    "        return self.token2idx[token]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Dictionary** class is used to map tokens (characters, words, subwords) into consecutive integer indexes.  \n",
    "The index **0** is reserved for padding sequences up to a fixed lenght, and the index **1** for any 'unknown' character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: 10808 UTF characters\n",
      "Labels: 235 languages\n"
     ]
    }
   ],
   "source": [
    "char_vocab = Dictionary()\n",
    "pad_token = '<pad>' # reserve index 0 for padding\n",
    "unk_token = '<unk>' # reserve index 1 for unknown token\n",
    "pad_index = char_vocab.add_token(pad_token)\n",
    "unk_index = char_vocab.add_token(unk_token)\n",
    "\n",
    "# join all the training sentences in a single string\n",
    "# and obtain the list of different characters with set\n",
    "chars = set(''.join(x_train_full))\n",
    "for char in sorted(chars):\n",
    "    char_vocab.add_token(char)\n",
    "print(\"Vocabulary:\", len(char_vocab), \"UTF characters\")\n",
    "\n",
    "lang_vocab = Dictionary()\n",
    "# use python set to obtain the list of languages without repetitions\n",
    "languages = set(y_train_full)\n",
    "for lang in sorted(languages):\n",
    "    lang_vocab.add_token(lang)\n",
    "print(\"Labels:\", len(lang_vocab), \"languages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a -> 67\n",
      "cat -> 28\n",
      "est Klement Go\n",
      "52 [45 78 71 79 71 80 86  2 41 81]\n"
     ]
    }
   ],
   "source": [
    "#From token or label to index\n",
    "print('a ->', char_vocab.token2idx['a'])\n",
    "print('cat ->', lang_vocab.token2idx['cat'])\n",
    "print(y_train_full[0], x_train_full[0][:10])\n",
    "x_train_idx = [np.array([char_vocab.token2idx[c] for c in line]) for line in x_train_full]\n",
    "y_train_idx = np.array([lang_vocab.token2idx[lang] for lang in y_train_full])\n",
    "print(y_train_idx[0], x_train_idx[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Radomly select 15% of the database for validation  \n",
    "Create lists of (input, target) tuples for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99875 training samples\n",
      "17625 validation samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_idx, y_train_idx, test_size=0.15, random_state=seed)\n",
    "train_data = [(x, y) for x, y in zip(x_train, y_train)]\n",
    "val_data = [(x, y) for x, y in zip(x_val, y_val)]\n",
    "print(len(train_data), \"training samples\")\n",
    "print(len(val_data), \"validation samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(data, batch_size, token_size):\n",
    "    \"\"\"Yield elements from data in chunks with a maximum of batch_size sequences and token_size tokens.\"\"\"\n",
    "    minibatch, sequences_so_far, tokens_so_far = [], 0, 0\n",
    "    for ex in data:\n",
    "        minibatch.append(ex)\n",
    "        seq_len = len(ex[0])\n",
    "        if seq_len > token_size:\n",
    "            ex = (ex[0][:token_size], ex[1])\n",
    "            seq_len = token_size\n",
    "        sequences_so_far += 1\n",
    "        tokens_so_far += seq_len\n",
    "        if sequences_so_far == batch_size or tokens_so_far == token_size:\n",
    "            yield minibatch\n",
    "            minibatch, sequences_so_far, tokens_so_far = [], 0, 0\n",
    "        elif sequences_so_far > batch_size or tokens_so_far > token_size:\n",
    "            yield minibatch[:-1]\n",
    "            minibatch, sequences_so_far, tokens_so_far = minibatch[-1:], 1, len(minibatch[-1][0])\n",
    "    if minibatch:\n",
    "        yield minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_generator(data, batch_size, token_size, shuffle=False):\n",
    "    \"\"\"Sort within buckets, then batch, then shuffle batches.\n",
    "    Partitions data into chunks of size 100*token_size, sorts examples within\n",
    "    each chunk, then batch these examples and shuffle the batches.\n",
    "    \"\"\"\n",
    "    for p in batch_generator(data, batch_size * 100, token_size * 100):\n",
    "        p_batch = batch_generator(sorted(p, key=lambda t: len(t[0]), reverse=True), batch_size, token_size)\n",
    "        p_list = list(p_batch)\n",
    "        if shuffle:\n",
    "            for b in random.sample(p_list, len(p_list)):\n",
    "                yield b\n",
    "        else:\n",
    "            for b in p_list:\n",
    "                yield b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DNN Model**  \n",
    "Includes Python comments with the dimension of the input  matrix:  \n",
    "T = Max number of tokens in a sequence  \n",
    "B = Number of sequences (batch size)  \n",
    "E = Embedding size\n",
    "H = Hidden size  \n",
    "O = Output size (number of languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNNClassifier(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, model=\"lstm\", num_layers=1, bidirectional=False, pad_idx=0):\n",
    "        super().__init__()\n",
    "        self.model = model.lower()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed = torch.nn.Embedding(input_size, embedding_size, padding_idx=pad_idx)\n",
    "        if self.model == \"gru\":\n",
    "            self.rnn = torch.nn.GRU(embedding_size, hidden_size, num_layers, bidirectional=bidirectional)\n",
    "        elif self.model == \"lstm\":\n",
    "            self.rnn = torch.nn.LSTM(embedding_size, hidden_size, num_layers, bidirectional=bidirectional)\n",
    "        #self.h2o = torch.nn.Linear(hidden_size, output_size)\n",
    "        self.h2o = torch.nn.Sequential(torch.nn.Linear(hidden_size, 1024),\n",
    "                                      torch.nn.Dropout(),\n",
    "                                      torch.nn.Linear(1024, output_size))\n",
    "        \n",
    "    def forward(self, input, input_lengths):\n",
    "        # T x B\n",
    "        encoded = self.embed(input)\n",
    "        # T x B x E\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(encoded, input_lengths)\n",
    "        # Packed T x B x E\n",
    "        output, _ = self.rnn(packed)\n",
    "        # Packed T x B x H\n",
    "        padded, _ = torch.nn.utils.rnn.pad_packed_sequence(output, padding_value=float('-inf'))\n",
    "        # T x B x H\n",
    "        padded = padded.permute(1,2,0)\n",
    "        # B x H x T\n",
    "        output = F.adaptive_max_pool1d(padded, 1).view(-1, self.hidden_size)\n",
    "        # B x H\n",
    "        output = self.h2o(output)\n",
    "        # B x O\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    print(\"WARNING: CUDA is not available. Select 'GPU On' on kernel settings\")\n",
    "device = torch.device(\"cuda\")\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **nn.CrossEntropyLoss()** criterion combines **nn.LogSoftmax()** and **nn.NLLLoss()** in one single class.  \n",
    "It is useful when training a classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, data, batch_size, token_size, log=False, L2_norm = True):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    ncorrect = 0\n",
    "    nsentences = 0\n",
    "    ntokens = 0\n",
    "    niterations = 0\n",
    "    for batch in pool_generator(data, batch_size, token_size, shuffle=True):\n",
    "        # Get input and target sequences from batch\n",
    "        X = [torch.from_numpy(d[0]) for d in batch]\n",
    "        X_lengths = [x.numel() for x in X]\n",
    "        ntokens += sum(X_lengths)\n",
    "        X_lengths = torch.tensor(X_lengths, dtype=torch.long, device=device)\n",
    "        y = torch.tensor([d[1] for d in batch], dtype=torch.long, device=device)\n",
    "        # Pad the input sequences to create a matrix\n",
    "        X = torch.nn.utils.rnn.pad_sequence(X).to(device)\n",
    "        model.zero_grad()\n",
    "        output = model(X, X_lengths)\n",
    "        loss = criterion(output, y)\n",
    "        if L2_norm:\n",
    "            all_linear1_params = torch.cat([x.view(-1) for x in model.h2o.parameters()])\n",
    "            l2_regularization = 0.01 * torch.norm(all_linear1_params, 2)\n",
    "            loss+=l2_regularization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Training statistics\n",
    "        total_loss += loss.item()\n",
    "        ncorrect += (torch.max(output, 1)[1] == y).sum().item()\n",
    "        nsentences += y.numel()\n",
    "        niterations += 1\n",
    "    \n",
    "    total_loss = total_loss / nsentences\n",
    "    accuracy = 100 * ncorrect / nsentences\n",
    "    if log:\n",
    "        print(f'Train: wpb={ntokens//niterations}, bsz={nsentences//niterations}, num_updates={niterations}')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, data, batch_size, token_size):\n",
    "    model.eval()\n",
    "    # calculate accuracy on validation set\n",
    "    ncorrect = 0\n",
    "    nsentences = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in pool_generator(data, batch_size, token_size):\n",
    "            # Get input and target sequences from batch\n",
    "            X = [torch.from_numpy(d[0]) for d in batch]\n",
    "            X_lengths = torch.tensor([x.numel() for x in X], dtype=torch.long, device=device)\n",
    "            y = torch.tensor([d[1] for d in batch], dtype=torch.long, device=device)\n",
    "            # Pad the input sequences to create a matrix\n",
    "            X = torch.nn.utils.rnn.pad_sequence(X).to(device)\n",
    "            answer = model(X, X_lengths)\n",
    "            ncorrect += (torch.max(answer, 1)[1] == y).sum().item()\n",
    "            nsentences += y.numel()\n",
    "        dev_acc = 100 * ncorrect / nsentences\n",
    "    return dev_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "embedding_size = 32\n",
    "bidirectional = False\n",
    "ntokens = len(char_vocab)\n",
    "nlabels = len(lang_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharRNNClassifier(ntokens, embedding_size, hidden_size, nlabels, bidirectional=bidirectional, pad_idx=pad_index).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training cross-validation model for 20 epochs\n",
      "Train: wpb=90535, bsz=245, num_updates=407\n",
      "| epoch 001 | train accuracy=43.7%\n",
      "| epoch 001 | valid accuracy=76.7%\n",
      "| epoch 002 | train accuracy=80.4%\n",
      "| epoch 002 | valid accuracy=85.3%\n",
      "| epoch 003 | train accuracy=86.3%\n",
      "| epoch 003 | valid accuracy=88.6%\n",
      "| epoch 004 | train accuracy=89.2%\n",
      "| epoch 004 | valid accuracy=89.6%\n",
      "| epoch 005 | train accuracy=90.9%\n",
      "| epoch 005 | valid accuracy=90.6%\n",
      "| epoch 006 | train accuracy=92.0%\n",
      "| epoch 006 | valid accuracy=91.2%\n",
      "| epoch 007 | train accuracy=92.9%\n",
      "| epoch 007 | valid accuracy=91.6%\n",
      "| epoch 008 | train accuracy=93.6%\n",
      "| epoch 008 | valid accuracy=92.2%\n",
      "| epoch 009 | train accuracy=94.2%\n",
      "| epoch 009 | valid accuracy=92.4%\n",
      "| epoch 010 | train accuracy=95.7%\n",
      "| epoch 010 | valid accuracy=93.0%\n",
      "| epoch 011 | train accuracy=96.1%\n",
      "| epoch 011 | valid accuracy=93.0%\n",
      "| epoch 012 | train accuracy=96.5%\n",
      "| epoch 012 | valid accuracy=93.1%\n",
      "| epoch 013 | train accuracy=96.8%\n",
      "| epoch 013 | valid accuracy=92.9%\n",
      "| epoch 014 | train accuracy=97.0%\n",
      "| epoch 014 | valid accuracy=93.2%\n",
      "| epoch 015 | train accuracy=97.3%\n",
      "| epoch 015 | valid accuracy=93.3%\n",
      "| epoch 016 | train accuracy=97.4%\n",
      "| epoch 016 | valid accuracy=93.1%\n",
      "| epoch 017 | train accuracy=97.8%\n",
      "| epoch 017 | valid accuracy=93.3%\n",
      "| epoch 018 | train accuracy=98.7%\n",
      "| epoch 018 | valid accuracy=93.4%\n",
      "| epoch 019 | train accuracy=98.8%\n",
      "| epoch 019 | valid accuracy=93.4%\n",
      "| epoch 020 | train accuracy=99.0%\n",
      "| epoch 020 | valid accuracy=93.4%\n"
     ]
    }
   ],
   "source": [
    "batch_size, token_size = 256, 200000\n",
    "epochs = 20\n",
    "train_accuracy = []\n",
    "valid_accuracy = []\n",
    "scheduler_lr = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 8, gamma = 0.5)\n",
    "print(f'Training cross-validation model for {epochs} epochs')\n",
    "for epoch in range(1, epochs + 1):\n",
    "    acc = train(model, optimizer, train_data, batch_size, token_size, log=epoch==1)\n",
    "    train_accuracy.append(acc)\n",
    "    print(f'| epoch {epoch:03d} | train accuracy={acc:.1f}%')\n",
    "    acc = validate(model, val_data, batch_size, token_size)\n",
    "    valid_accuracy.append(acc)\n",
    "    print(f'| epoch {epoch:03d} | valid accuracy={acc:.1f}%')\n",
    "    scheduler_lr.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(1, len(train_accuracy)+1), train_accuracy)\n",
    "plt.plot(range(1, len(valid_accuracy)+1), valid_accuracy)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final model**  \n",
    "Finally, we create a model using all the training data and we generate the submission with the predicted test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharRNNClassifier(ntokens, embedding_size, hidden_size, nlabels, bidirectional=bidirectional).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training final model for 20 epochs\n",
      "Train: wpb=90505, bsz=245, num_updates=479\n",
      "| epoch 001 | train accuracy=48.368\n",
      "| epoch 002 | train accuracy=82.511\n",
      "| epoch 003 | train accuracy=87.841\n",
      "| epoch 004 | train accuracy=90.393\n",
      "| epoch 005 | train accuracy=91.883\n",
      "| epoch 006 | train accuracy=92.797\n",
      "| epoch 007 | train accuracy=93.519\n",
      "| epoch 008 | train accuracy=94.105\n",
      "| epoch 009 | train accuracy=94.627\n",
      "| epoch 010 | train accuracy=95.045\n",
      "| epoch 011 | train accuracy=95.537\n",
      "| epoch 012 | train accuracy=95.854\n",
      "| epoch 013 | train accuracy=96.066\n",
      "| epoch 014 | train accuracy=96.449\n",
      "| epoch 015 | train accuracy=96.673\n",
      "| epoch 016 | train accuracy=97.037\n",
      "| epoch 017 | train accuracy=97.122\n",
      "| epoch 018 | train accuracy=97.374\n",
      "| epoch 019 | train accuracy=97.609\n",
      "| epoch 020 | train accuracy=97.964\n"
     ]
    }
   ],
   "source": [
    "print(f'Training final model for {epochs} epochs')\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(f'| epoch {epoch:03d} | train accuracy={train(model, optimizer, train_data + val_data, batch_size, token_size, log=epoch==1):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, data, batch_size, token_size):\n",
    "    model.eval()\n",
    "    sindex = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in pool_generator(data, batch_size, token_size):\n",
    "            # Get input sequences from batch\n",
    "            X = [torch.from_numpy(d[0]) for d in batch]\n",
    "            X_lengths = torch.tensor([x.numel() for x in X], dtype=torch.long, device=device)\n",
    "            # Pad the input sequences to create a matrix\n",
    "            X = torch.nn.utils.rnn.pad_sequence(X).to(device)\n",
    "            answer = model(X, X_lengths)\n",
    "            label = torch.max(answer, 1)[1].cpu().numpy()\n",
    "            # Save labels and sentences index\n",
    "            labels.append(label)\n",
    "            sindex += [d[1] for d in batch]\n",
    "    return np.array(sindex), np.concatenate(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the test database we replace the label (language) with a sentence index.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_txt = open(\"../input/x_test.txt\").read().splitlines()\n",
    "x_test_idx = [np.array([char_vocab.token2idx[c] if c in char_vocab.token2idx else unk_index for c in line]) for line in x_test_txt]\n",
    "test_data = [(x, idx) for idx, x in enumerate(x_test_idx)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentence index is used to rearrange the labels in the original sentence order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "index, labels = test(model, test_data, batch_size, token_size)\n",
    "order = np.argsort(index)\n",
    "labels = labels[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,mwl\n",
      "1,nld\n",
      "2,ava\n",
      "3,tcy\n",
      "4,bjn\n",
      "5,mon\n",
      "6,arz\n",
      "7,lez\n",
      "8,bul\n",
      "9,nan\n"
     ]
    }
   ],
   "source": [
    "with open('submission.csv', 'w') as f:\n",
    "    print('Id,Language', file=f)\n",
    "    for sentence_id, lang_id in enumerate(labels):\n",
    "        language = lang_vocab.idx2token[lang_id]\n",
    "        if sentence_id < 10:\n",
    "            print(f'{sentence_id},{language}')\n",
    "        print(f'{sentence_id},{language}', file=f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
